

```{r prob_scores, message = FALSE}
## Probability scores instead of proper scores
scores <- sims %>%
  mutate(next_state = dplyr::lead(state_index), model = as.integer(model)) %>%
  filter(model < 3) %>%
  rowwise() %>%
  mutate(prob =  transitions[[model]][state_index, next_state, action_index] ) %>% 
  group_by(reps, model) %>%
  summarise(score = sum(log(prob))) 

score_table <- scores %>% group_by(model) %>% summarise(mean_score = mean(score))

scores %>% mutate(model = as.factor(model)) %>% 
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 70)
```





There are techniques for approaching model uncertainty which condition on utility alone which are not greedy algorithms. For example, in model-free deep reinforcement learning, a neural network is trained on cumulative utility to select the best action.  


If the problem as posed is not solved by existing approaches, then perhaps the paradox is only a result of 

- This result is driven by the unrealistic simplicity, and would not occur in more realistic examples.  




An example based on a constant-mortality policy would be easier to critique by observing that the policy was not in fact optimal in the first place.  Recall that the purpose of this modeling exercise is not to precisely mimic the decision process of a specific quota in specific fish stock, but rather to illustrate in more general terms how carefully considered theory for approaching decision making under model uncertainty, from rigorous assessment of forecast skill to adaptive management, fail to handle this very simple example.



No measure of forecast skill would prefer predictions drawn from Model 1 over those drawn from Model 2. Any process of updating model probabilities will lead away from Model 1 and towards Model 2.  Simpler methods such as information criteria, $r^2$ or other goodness-of-fit metrics will also all clearly prefer model 2.  The problem does not arise from a lack of data, or from over-fitting, which given sufficient data would be addressed by the use of @Gneiting2007's proper scoring rules.  Nor does the problem resolved by approaches explicitly designed to maximize utility (e.g. economic value) over model uncertainty [@Polasky2011].  

  

- This result is simply an example of how different objectives lead to different strategies. 

<!-- -->
# Discussion take 2

Some readers will find the presentation of forecasting 


addressing potential critiques: 

Making this problem more complicated might better approximate real-world decision making but would
by no means resolve the issue.  For example, increasing the suite of models under consideration to
include all possible parameter values Gordon-Schaefer model, or better, adding
in many other well studied structural forms such as Ricker [@], Beverton-Holt [@], or Shepard equation
to our set of candidate models would give the same result.  Moreover, focusing on expanding the set of
possible models ignores the simple fact that our most stripped down version of the problem already 
contains a model which can obtain nearly optimal performance, just as it already contains a model which
can make pretty accurate forecasts.  If well considered approaches make the wrong choice in this simple
problem, the issue requires more attention.  

Note that this problem is not merely a case of alternative objectives leading to alternate outcomes. 
Unlike model selection based on forecasting performance, the adaptive management approach explicitly
considers the objective of maximizing the utility, and yet still fails to select the correct model.




## Discussion


This paradox in performance of forecasting vs performance in decision making can be easily resolved by considering the context of the decision problem more closely. Comparing plots of the functional form of our two logistic-curve models, compared to the functional form of the "true" model used to drive the simulations (Fig. 3A), it is clear to see that model 2 does indeed lie closer to the true model throughout the state space, agreeing precisely with the true carrying capacity (where both functions cross zero with negative slope).  However, the peak of model 3 very nearly matches the peak of model 1. The optimal decision literature, dating back to the 1950s [@Schaefer1954], demonstrates that the Maximum Sustainable Yield (MSY) is maintained by harvesting a stock down to the size at which it achieves its maximum growth rate, i.e. 50% of the un-fished equilibrium size for a symmetric growth model ($K/2$).  Model 1, while being very wrong about both the growth rate and the un-fished equilibrium, is nevertheless nearly perfect in estimating the stock size at which maximum growth rate is achieved, and this gives nearly optimal decisions (Fig 3B) despite its terrible forecasts.   

Thus, each year our model 1 managers are again chagrined to see the stock size estimates come in far below their rosy predictions, but nevertheless manage to set a nearly optimal quota by comparing the observed stock size to the model's predicted optimal escapement level [@Reed1979].  Meanwhile, model 2 managers could only congratulate themselves that each year's observations fall neatly within their predicted interval, unaware that the they were over-exploiting the fishery by both economic and ecological metrics.  If we had access to model 3, we would no doubt find that it outperformed model 2 in forecast accuracy as well as ecological and economic performance.  But in real ecological decision making, we never know the true model -- we will always be comparing among approximations.  Within fisheries, even in today's parameter-rich age-structured models, recruitment approximations with symmetric growth functions (Logistic, Ricker, Beverton-Holt, etc) still dominate [@ramlegacy2012; @ramlegacy2018].  

This issue is by no means unique to fisheries.  Throughout resource management and conservation, and no doubt other fields, decisions about which model to use are guided by which model best fits available data [@Clark1990]. Increasingly, these are joined by calls to assess _forecast accuracy_ [@Clark2001; @Dietze2018; @White2019] as the ultimate test of a model.  Yet as this example illustrates, such metrics, no matter how rigorously defined, may select entirely the wrong model for the task at hand.  A decision maker has other objectives than prediction accuracy, and approaches which ignore these considerations do so at their peril. This example has also shown that once we are managing with the wrong model, no amount of comparing predictions from that model to actual outcomes will guarantee we discover our mistake.  Despite its consistently good predictions, model 1 is in fact over-fishing to a dangerous level.  

Because we will never know the "true" model, we must never forget that our choice of models must reflect the context for which those models will be used [@Levins1966].  Model 2 would indeed be a better choice than model 1 if our objective was to determine the natural size of our fish stock in the absence of fishing.  Only when we focus on the outcomes we actually care about -- in this case, economic and ecological performance -- can we see which model is best for decision-making.  Model 1, despite its many mistakes, is right about one key feature: the biomass for peak growth -- and that is enough to guarantee nearly optimal performance.   This conclusion should also be reassuring to both modelers and decision makers, for it reminds us that effective models need be perfect or even all that close in every aspect, as long as they capture the key features of the decision context.  Decision theory [e.g. @Clark1973; @Reed1979; @pomdp-intro] and research into the socio-ecological models [e.g. @Kareiva2006] helps us better understand that context.  Adaptive management approaches [@Walters1978] can apply that theory to compare management outcomes between models directly.  It is not true that we need good forecasts to make good decisions.




```{r figure3, fig.width=7, fig.height=4, fig.cap="Panel A: Population recruitment curves for each model, compared to that of the true Model. 3 Model 2 more closely approximates the true Model 3, but note that maximum value Model 1 and Model 3 occur at nearly the same value for the state, x.  Panel B: The computed optimal policy of each model, derived by SDP, expressed in terms of the target escapement (population size remaining after harvest) for each possible stock size. Model 1 over-harvests consistently, while the target escapement under Model 2 is nearly identical to that of the true Model 3."}
fig3a + ggtitle("A") + fig3b + ggtitle("B")
```

# Methods

Stochastic transition matrices are defined for models 1-3 on a discrete grid of
240 possible states spaced uniformly from 0 to 24. A discrete action space 
enumerating possible harvest quotas is set to the same grid. The utility of a harvest quota $H_t$ given a population state $X_t$ is given by $U(X_t, H_t) = \min(X_t, H_t)$ (i.e. a fixed price for realized harvest). A modest discount of
$\gamma = 0.99$ allows comparisons to approaches that ignore [@Schaefer1954] or include [@Clark1973; @Reed1979] discounting; results are not sensitive to this choice. The optimal policy for each model is determined by stochastic dynamic programming [@Marescot2013].  Details of the implementation, including fully reproducible R code, have been included in the appendix.



acknowledgements: |
  This work was supported in part by NSF CAREER (#1942280) and computational resources from NSF's XSEDE Jetstream (DEB160003) and Chameleon cloud platforms.





# A way forward?

note that the model set considered here already contains a model which captures the key feature. 

- Greedy strategy will not work.  Could stick with a model for 50 years then switch...
- A mechanistic understanding of the decision process, not the ecological one.  Analyzing the decision problem, we know that in this situation, the optimal stategy depends only on the location of the peak of the growth rate.  This could suggest a very different way forward
- 



Yet the model that leads to the best decisions is not always the model that makes the most accurate forecasts, as I illustrate here. Surprisingly, this can even happen when a decision is derived directly from a complex optimization routine of a probabilistic predictive models [@Marescot2013].  Reality is complex; even our best models can only ever be approximations of underlying processes.  Here, I use a classic, well-understood example from fisheries management [@Schaefer1954; @Clark1973; @Reed1979] to illustrate both the paradox of how a model with the worst forecast provides the best decision outcomes, as well as show how we can avoid selecting models that are poorly suited for management by considering the management context more explicitly. These results underscore that in choosing the best model for decision-making, it can be more important to capture a single key feature of the process than it is to make the most accurate prediction about future states.  

<!--
Many ecological management problems are sequential decision problems, in which each year (or other interval) a manager must observe the state of the system and choose a course of action to maximize long term objectives. Such problems inherently depend on forecasts: each possible action can result in a different forecast for the future state, typically reflecting some uncertainty as well.  The utility the manager derives may depend on both the choice of action and the state of the system, reflecting the costs and benefits associated with each. Sequential decision-making problems are distinguished by the need to think more than one move ahead. For instance, harvesting as many fish as possible in year one may maximize the market value that year, but if too few fish are left to reproduce then harvest in future years will suffer.  The same calculus of thinking ahead frequently applies to rebuilding species populations as well [e.g. @Lambert;  @Chades2008].  
-->



