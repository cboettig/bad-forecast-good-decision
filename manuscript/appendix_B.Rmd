---
title: 'Appendix for: "Bad Forecast, Good Decision"'
date: "`r Sys.Date()`"
author: Carl Boettiger
journal: Appendix
layout: 3p
bibliography: refs.bib
#output:
#  tufte::tufte_handout:
#    dev: cairo_pdf
#    latex_engine: xelatex
output: 
  hrbrthemes::ipsum_pdf:
    dev: cairo_pdf
    latex_engine: xelatex

---


```{r model_definitions}
states <- seq(0,24, length.out = 240)
actions <- states
obs <- states
```

```{r}
reward_fn <- function(x,h) pmin(x,h)
discount <- 0.99
```



```{r graphics_setup, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"

knitr::opts_chunk$set(cache=FALSE, tidy = "styler", message = FALSE, warning = FALSE, echo = TRUE)

```

```{r setup, message=FALSE}
library(tidyverse)
library(MDPtoolbox)
library(expm)
# remotes::install_github("boettiger-lab/mdplearning")
library(mdplearning)
```




```{r}
# K is at twice max of f3; 8 * K_3 / 5
f1 <- function(x, h = 0, r = 2, K = 10 * 8 / 5){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
}
f2 <- function(x, h = 0, r = 0.5, K = 10){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
}

# max is at 4 * K / 5 
f3  <- function(x, h = 0, r = .002, K = 10){
  s <- pmax(x - h, 0)
  s + s ^ 4 * r * (1 - s / K)
}

n_models <- 32
rs <- seq(0.4, 2, length.out = n_models)
Ks <- seq(8, 10*8/5, length.out = n_models)
sigmas <- seq(0.05, 1.5*0.05, length.out = n_models)
pars <- data.frame(r = rs, K = Ks, sigma = sigmas)

closure <- function(r, K){
  function(x, h = 0){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
  }
}

models <- lapply(1:n_models, function(i) closure(pars[i,"r"], pars[i,"K"]))

## gather models together, indicate true model
sigma_g <- 0.05
models <- c(f3, models)
model_sigmas <- c(sigma_g, pars$sigma)
true_model <- 1
```


On a discrete grid of possible states and actions, we can define the growth rate of a given state $X_t$ subject to harvest $H_t$,
$f(X_t,H_t)$ as set of matrices.  Each matrix $i$ gives the transition probabilities for any current state to any future state, 
given that action $i$ is taken.  



```{r transition_matrices}
transition_matrices <- function(f, states, actions, sigma_g){
  n_s <- length(states)
  n_a <- length(actions)
  transition <- array(0, dim = c(n_s, n_s, n_a))
  for (k in 1:n_s) {
    for (i in 1:n_a) {
      nextpop <- f(states[k], actions[i])
      if(nextpop <= 0){
        transition[k, , i] <- c(1, rep(0, n_s - 1))
      } else if(sigma_g > 0){
        x <- dlnorm(states, log(nextpop), sdlog = sigma_g)
        if(sum(x) == 0){ ## nextpop is computationally zero
          transition[k, , i] <- c(1, rep(0, n_s - 1))
        } else {
          x <- x / sum(x) # normalize evenly
          transition[k, , i] <- x
        }
      }
    }
  }
  transition
}
```

This follows the standard setup for standard stochastic dynamic programming, see @Marescot2013. Having defined a function to compute the transition matrix, we can use it to create matrices corresponding to each of the three models:

```{r}
transitions <- lapply(seq_along(models), 
                      function(i) transition_matrices(models[[i]], 
                                                      states, 
                                                      actions, 
                                                      model_sigmas[[i]]))
names(transitions) <- as.character(seq_along(models))

```


Likewise, a corresponding matrix defining the rewards associated with each state $X$ and each harvest action $H$ can also be defined.

````{r}
## Compute reward matrix (shared across all models)
n_s <- length(states)
n_a <- length(actions)
reward <- array(0, dim = c(n_s, n_a))
for (k in 1:n_s) {
  for (i in 1:n_a) {
    reward[k, i] <- reward_fn(states[k], actions[i])
  }
}
```

# Plot Models 



```{r plot_models}
model_set <- models
names(model_set) <- c("true", as.character(round(Ks, 2)))
d <- 
  map_dfc(model_set, function(f) f(states) - states) %>%
  mutate(state = states)

  d %>% pivot_longer(names(model_set[-1]), "model") %>%
  ggplot(aes(state, value, col=as.numeric(model))) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_point(alpha = .5) + 
  geom_line(aes(state, true), col = "red") +
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x") + viridis::scale_color_viridis()
```


We can easily plot the optimal policy derived from each model, as shown
in Fig3b. 

```{r plot_policies}
fig3b <- policies %>%
  ggplot(aes(states, escapement, col=model, lty=model)) + 
  geom_line(lwd=2) + xlab("state")
```


# Adaptive Management

Passive adaptive management using a Bayesian learning scheme still learns the wrong model. 

```{r}
adaptive_management <- memoise::memoise(mdp_learning, cache = memoise::cache_filesystem("cache/"))
x0 <- which.min(abs(states - 6))

am1 <- adaptive_management(transitions[-1], 
                           reward, 
                           discount, 
                           x0 = x0, 
                           Tmax = 50, 
                           true_transition = transitions[[true_model]], 
                           epsilon = 0.001, 
                           max_iter = 2000)
```


```{r}
names(am1$posterior) <- Ks
belief <- am1$posterior %>%
  mutate(time = 1:dim(am1$posterior)[1]) %>% 
  pivot_longer(as.character(Ks), "K", values_to = "belief") %>%
  mutate(K = as.numeric(K))

belief %>% filter(time %in% seq(1, 11, by = 2)) %>%
  ggplot(aes(K, belief, col=time, group=as.character(time))) +
  geom_line()

belief %>% filter(K < 10) %>%
  ggplot(aes(time, belief, col=K, group=as.character(K))) +
  geom_line()

```


```{r}
am <- am1$df %>% mutate(belief = am1$posterior[,1])
am %>%  ggplot(aes(time, states[state], col = belief)) + 
  geom_line() + 
  geom_point() +
  scale_colour_gradient(limits = c(0,1), low = pal[1], high = pal[4])
```















```{r include = FALSE}
as_tibble(am1$posterior) %>% 
  mutate(time = seq_along(V1)) %>%
  select(time = time, "1" = V1, "2" = V2) %>% 
  pivot_longer(c("1", "2"), values_to="belief", names_to = "model") %>%
  filter(time < 20) %>%
  ggplot(aes(time, belief, col=model)) + geom_line(col = pal[4])
```



# Simulations and step-ahead forecasts

We simulate fishing dynamics under the optimal policy for each model, using a simple helper function from the `mdplearning` package. Because growth dynamics are stochastic, we perform 100 simulations of each model from identical starting condition to ensure results are not the result of chance alone.


```{r simulations}
library(mdplearning)
Tmax <- 100
x0 <- which.min(abs(states - 6))
reps <- 100
set.seed(12345)

## Simulate each policy reps times, with `3` as the true model:
simulate_policy <- function(i, policy){
  mdp_planning(transitions[[true_model]], reward, discount,
           policy = policy, x0 = x0, Tmax = Tmax) %>%
    select(value, state_index = state, time, action_index = action)  %>% 
    mutate(state = states[state_index])
}

sims <- 
  map_dfr(names(transitions), 
          function(m){
            policy <- policies %>% filter(model == m) %>% pull(policy)
            map_dfr(1:reps, simulate_policy, policy = policy, .id = "reps")
          },
          .id = "model"
         )
```



```{r stepahead_unfished}
stepahead_unfished <- sims
stepahead_unfished$state_index <- rep(sims$state_index[sims$model == "1"],3)

stepahead_unfished <- stepahead_unfished  %>% 
  filter(model != "3") %>% 
  mutate(next_state = dplyr::lead(state_index), model = as.integer(model)) %>%
  rowwise() %>%
  mutate(expected = transitions[[model]][state_index, , 1]  %*% states,
         var = transitions[[model]][state_index, , 1]  %*% states ^ 2 - expected ^ 2,
         low = states[max(which(cumsum(transitions[[model]][state_index,,1]) < 0.025)) ],
         high = states[min(which(cumsum(transitions[[model]][state_index,,1]) > 0.975)) ],
         true = states[next_state ]) 
```




```{r stepahead_fished}
stepahead_fished <- sims %>% 
  filter(model != "3") %>%
  mutate(next_state = dplyr::lead(state_index), model = as.integer(model)) %>%
  rowwise() %>%
  mutate(prob =  transitions[[model]][state_index, next_state, action_index],
         expected = transitions[[model]][state_index, , action_index]  %*% states,
         var = transitions[[model]][state_index, , action_index]  %*% states ^ 2 - expected ^ 2,
         low = states[max(which(cumsum(transitions[[model]][state_index,,action_index]) < 0.025)) ],
         high = states[min(which(cumsum(transitions[[model]][state_index,,action_index]) > 0.975)) ],
         true = states[next_state]) %>%
 select(time, model, true, expected, low, high, var, prob, reps)
```


# Proper scores


```{r proper_scores, message = FALSE}
# Gneiting & Raferty (2007), eq27
scoring_fn <- function(x, mu, sigma){ -(mu - x )^2 / sigma^2  - log(sigma)}

stepahead_unfished <- stepahead_unfished %>%
  mutate(sd = sqrt(var),
         score = scoring_fn(expected, true, sd))

stepahead_fished <- stepahead_fished %>%
  mutate(sd = sqrt(var),
         score = scoring_fn(expected, true, sd))
```

```{r}
predictions <- 
  stepahead_unfished %>% 
  select(time, model, reps, expected, low, high, true, score) %>% 
  mutate(scenario = "A_unfished") %>% 
  bind_rows(stepahead_fished  %>% 
          select(time, model, reps, expected, low, high, true, score) %>% 
          mutate(scenario = "B_fished")
  ) %>% 
  mutate(model = as.character(model))
```


# Plotting

Having computed a single data frame of simulation data under both optimally fished (according to each model) and unfished scenarios for each model, along with the proper scores for the step-ahead forecasts a la @Gneiting2007, we have all the results necessary to generate the figures presented in the main paper.    



```{r fig1a}
fig1ab <- predictions %>%
  filter(reps == "2", time < 10) %>%
  ggplot(aes(time, col = model, fill = model)) + 
  geom_point(aes(y = expected)) + 
  geom_errorbar(aes(ymin = low, ymax = high)) +
  geom_point(aes(y = true), pch = "*", size = 12, 
             alpha = 0.6) + 
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "A. Without fishing",
    B_fished = "B. Managed harvest"))) +
  ylab("stock size")

```


Likewise we can plot the data on proper scores associated with each prediction of each model: 

```{r}
fig1cd <- predictions %>%
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 2, show.legend = FALSE) +
  coord_cartesian(xlim = c(-100, 1), ylim = c(0,4000)) +
  xlab("Proper score") +
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "C.",
    B_fished = "D."))) + 
  scale_x_continuous(breaks = c(-100, 0)) +
  theme(axis.text.y = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )

```



## Forecast ecological and economic performance


The ecological performance of the model can easily be visualized by plotting
the results of each simulation. 

```{r plot_sims}
fig2a <- 
  sims %>% 
  group_by(model, time) %>%
  summarise(mean_state = mean(state), sd = sd(state), .groups = "drop") %>%
  filter(time < 25, model != "3") %>%
  ggplot(aes(time, mean_state)) + 
  geom_line(aes(col = model), lwd=1.5, show.legend = FALSE) +
    geom_ribbon(aes(ymin = mean_state - 2*sd, 
                    ymax = mean_state + 2*sd,
                    fill = model),
                alpha = 0.2, show.legend = FALSE) +
  ylab("state") + 
  ggtitle("A") + labs(subtitle = "Ecological")
```

To plot the economic value over time, we must sum up the discounted values
at each time step, and then average over replicate simulations of each model:

```{r plot_npv}
##  Net Present Value accumulates over time
npv_df <- sims %>% 
  group_by(model, reps) %>%
  mutate(npv = cumsum(value * discount ^ time)) %>%
  group_by(time, model)  %>% 
  summarise(mean_npv = mean(npv), .groups="drop") %>% 
  arrange(model, time)

optimal <- select(filter(npv_df, model == "3"), time, mean_npv)

fig2b <- 
npv_df %>%
  filter(model != "3", time %in% seq(1,100, by = 5)) %>%
  ggplot(aes(time, mean_npv)) +
  geom_line(data = optimal, lwd = 1.5, col = "grey20") +
  geom_point(aes(col=model), size = 4, alpha = 0.8) + 
  ylab("Net present value") + xlab("time")  + 
  ggtitle("B.") + labs(subtitle = "Economic")

```


Plotting the model functions themselves are shown in Fig 3a:










# References
