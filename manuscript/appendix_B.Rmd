---
title: 'Appendix for: "Bad Forecast, Good Decision"'
date: "`r Sys.Date()`"
author: Carl Boettiger
journal: Appendix
layout: 3p
bibliography: refs.bib
#output:
#  tufte::tufte_handout:
#    dev: cairo_pdf
#    latex_engine: xelatex
output: 
  hrbrthemes::ipsum_pdf:
    dev: cairo_pdf
    latex_engine: xelatex

---


```{r model_definitions}
states <- seq(0,24, length.out = 240)
actions <- states
obs <- states
```

```{r}
reward_fn <- function(x,h) pmin(x,h)
discount <- 0.99
```



```{r graphics_setup, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"

knitr::opts_chunk$set(cache=FALSE, tidy = "styler", message = FALSE, warning = FALSE, echo = TRUE)

```

```{r setup, message=FALSE}
library(tidyverse)
library(MDPtoolbox)
library(expm)
# remotes::install_github("boettiger-lab/mdplearning")
library(mdplearning)
```




```{r}
# K is at twice max of f3; 8 * K_3 / 5
f1 <- function(x, h = 0, r = 2, K = 10 * 8 / 5){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
}
f2 <- function(x, h = 0, r = 0.5, K = 10){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
}

# max is at 4 * K / 5 
f3  <- function(x, h = 0, r = .002, K = 10){
  s <- pmax(x - h, 0)
  s + s ^ 4 * r * (1 - s / K)
}

n_models <- 32
rs <- seq(0.4, 2, length.out = n_models)
Ks <- seq(8, 10*8/5, length.out = n_models)
sigmas <- seq(0.05, 1.5*0.05, length.out = n_models)
pars <- data.frame(r = rs, K = Ks, sigma = sigmas)

closure <- function(r, K){
  function(x, h = 0){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
  }
}

models <- lapply(1:n_models, function(i) closure(pars[i,"r"], pars[i,"K"]))

## gather models together, indicate true model
sigma_g <- 0.05
models <- c(f3, models)
model_sigmas <- c(sigma_g, pars$sigma)
true_model <- 1
```


On a discrete grid of possible states and actions, we can define the growth rate of a given state $X_t$ subject to harvest $H_t$,
$f(X_t,H_t)$ as set of matrices.  Each matrix $i$ gives the transition probabilities for any current state to any future state, 
given that action $i$ is taken.  



```{r transition_matrices}
transition_matrices <- function(f, states, actions, sigma_g){
  n_s <- length(states)
  n_a <- length(actions)
  transition <- array(0, dim = c(n_s, n_s, n_a))
  for (k in 1:n_s) {
    for (i in 1:n_a) {
      nextpop <- f(states[k], actions[i])
      if(nextpop <= 0){
        transition[k, , i] <- c(1, rep(0, n_s - 1))
      } else if(sigma_g > 0){
        x <- dlnorm(states, log(nextpop), sdlog = sigma_g)
        if(sum(x) == 0){ ## nextpop is computationally zero
          transition[k, , i] <- c(1, rep(0, n_s - 1))
        } else {
          x <- x / sum(x) # normalize evenly
          transition[k, , i] <- x
        }
      }
    }
  }
  transition
}
```

This follows the standard setup for standard stochastic dynamic programming, see @Marescot2013. Having defined a function to compute the transition matrix, we can use it to create matrices corresponding to each of the three models:

```{r}
transitions <- lapply(seq_along(models), 
                      function(i) transition_matrices(models[[i]], 
                                                      states, 
                                                      actions, 
                                                      model_sigmas[[i]]))
names(transitions) <- as.character(seq_along(models))

```


Likewise, a corresponding matrix defining the rewards associated with each state $X$ and each harvest action $H$ can also be defined.

````{r}
## Compute reward matrix (shared across all models)
n_s <- length(states)
n_a <- length(actions)
reward <- array(0, dim = c(n_s, n_a))
for (k in 1:n_s) {
  for (i in 1:n_a) {
    reward[k, i] <- reward_fn(states[k], actions[i])
  }
}
```

# Plot Models 



```{r plot_models}
model_set <- models
names(model_set) <- c("true", as.character(round(Ks, 2)))
d <- 
  map_dfc(model_set, function(f) f(states) - states) %>%
  mutate(state = states)

  d %>% pivot_longer(names(model_set[-1]), "model") %>%
  ggplot(aes(state, value, col=as.numeric(model))) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_point(alpha = .5) + 
  geom_line(aes(state, true), col = "red") +
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x") + viridis::scale_color_viridis()
```


We can easily plot the optimal policy derived from each model, as shown
in Fig3b. 

```{r plot_policies}
fig3b <- policies %>%
  ggplot(aes(states, escapement, col=model, lty=model)) + 
  geom_line(lwd=2) + xlab("state")
```


# Adaptive Management

Passive adaptive management using a Bayesian learning scheme still learns the wrong model. 

```{r}
adaptive_management <- memoise::memoise(mdp_learning, cache = memoise::cache_filesystem("cache/"))
x0 <- which.min(abs(states - 6))

am1 <- adaptive_management(transitions[-1], 
                           reward, 
                           discount, 
                           x0 = x0, 
                           Tmax = 50, 
                           true_transition = transitions[[true_model]], 
                           epsilon = 0.001, 
                           max_iter = 2000)
```




```{r}
am <- am1$df %>% mutate(belief = am1$posterior[,1])
am %>%  ggplot(aes(time, states[state], col = belief)) + 
  geom_line() + 
  geom_point() +
  scale_colour_gradient(limits = c(0,1), low = pal[1], high = pal[4])
```


```{r}



am_npv <- sum(am$value * discount ^ am$time)
am_npv / npv[[1,"npv"]]
```









```{r include = FALSE}
as_tibble(am1$posterior) %>% 
  mutate(time = seq_along(V1)) %>%
  select(time = time, "1" = V1, "2" = V2) %>% 
  pivot_longer(c("1", "2"), values_to="belief", names_to = "model") %>%
  filter(time < 20) %>%
  ggplot(aes(time, belief, col=model)) + geom_line(col = pal[4])
```


