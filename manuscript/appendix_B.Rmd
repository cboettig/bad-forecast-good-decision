---
title: 'Appendix B'
date: "`r Sys.Date()`"
author: Carl Boettiger
journal: Appendix
layout: 3p
bibliography: refs.bib
#output:
#  tufte::tufte_handout:
#    dev: cairo_pdf
#    latex_engine: xelatex
output: 
  hrbrthemes::ipsum_pdf:
    dev: cairo_pdf
    latex_engine: xelatex

---


```{r model_definitions}
states <- seq(0,24, length.out = 240)
actions <- states
obs <- states
```

```{r}
reward_fn <- function(x,h) pmin(x,h)
discount <- 0.99
```



```{r graphics_setup, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"

knitr::opts_chunk$set(cache=FALSE, tidy = "styler", message = FALSE, warning = FALSE, echo = FALSE)

```

```{r setup, message=FALSE}
library(tidyverse)
library(MDPtoolbox)
library(expm)
# remotes::install_github("boettiger-lab/mdplearning")
library(mdplearning)
```


```{r}
# max is at 4 * K / 5 
f3  <- function(x, h = 0, r = .002, K = 10){
  s <- pmax(x - h, 0)
  s + s ^ 4 * r * (1 - s / K)
}
```

```{r}
# Try 6 log-spaced values
rs <- 10 ^ seq(0.01, 2, length=6)
Ks <- 10 ^ seq(-.3, 1.2, length=6)

# compare over fixed set of candidate r and K
rs <- c(0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 2)
Ks <- c(0.5, 1, 1.5, 4, 10, 16)
n_models <- length(rs)*length(Ks)
sigmas <- 0.1
pars <- expand.grid(rs, Ks)
names(pars) <- c("r", "K")
pars$sigma <- sigmas

closure <- function(r, K){
  function(x, h = 0){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
  }
}

models <-  lapply(1:n_models, function(i) closure(pars[i,"r"], pars[i,"K"]))

## gather models together, indicate true model
sigma_g <- 0.05
models <- c(f3, models)
model_sigmas <- c(sigma_g, pars$sigma)
true_model <- 1
```




```{r transition_matrices}
transition_matrices <- function(f, states, actions, sigma_g){
  n_s <- length(states)
  n_a <- length(actions)
  transition <- array(0, dim = c(n_s, n_s, n_a))
  for (k in 1:n_s) {
    for (i in 1:n_a) {
      nextpop <- f(states[k], actions[i])
      if(nextpop <= 0){
        transition[k, , i] <- c(1, rep(0, n_s - 1))
      } else if(sigma_g > 0){
        x <- dlnorm(states, log(nextpop), sdlog = sigma_g)
        if(sum(x) == 0){ ## nextpop is computationally zero
          transition[k, , i] <- c(1, rep(0, n_s - 1))
        } else {
          x <- x / sum(x) # normalize evenly
          transition[k, , i] <- x
        }
      }
    }
  }
  transition
}
```


```{r}
transitions <- lapply(seq_along(models), 
                      function(i) transition_matrices(models[[i]], 
                                                      states, 
                                                      actions, 
                                                      model_sigmas[[i]]))
names(transitions) <- as.character(seq_along(models))

```



````{r}
## Compute reward matrix (shared across all models)
n_s <- length(states)
n_a <- length(actions)
reward <- array(0, dim = c(n_s, n_a))
for (k in 1:n_s) {
  for (i in 1:n_a) {
    reward[k, i] <- reward_fn(states[k], actions[i])
  }
}
```

# Plot Models 



```{r plot_models, fig.width=6, fig.height=4}
model_set <- models
names(model_set) <- c("true", 1:n_models)
d <- 
  map_dfc(model_set, function(f) f(states) - states) %>%
  mutate(state = states)

d %>% pivot_longer(names(model_set[-1]), "model") %>%
  mutate(model = as.numeric(model)) %>%
  ggplot(aes(state, value, col = model)) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_point(alpha = .5) + 
  geom_line(aes(state, true), col = "red") +
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x") + viridis::scale_color_viridis()

```



# Adaptive Management

Passive adaptive management using a Bayesian learning scheme still learns the wrong model. 

```{r}
adaptive_management <- memoise::memoise(mdp_learning, cache = memoise::cache_filesystem("cache/"))
x0 <- which.min(abs(states - 6))
## start with 90% weight on 'model 1'
model_prior <- c(rep(.1, length(models)-2) / (length(models) - 2), .9)

am_many <- adaptive_management(transitions[-1],  
                           reward = reward, 
                           discount = discount,
                           model_prior = model_prior,
                           x0 = x0, 
                           Tmax = 20, 
                           true_transition = transitions[[true_model]], 
                           epsilon = 0.001, 
                           max_iter = 2000)
```


```{r}
Tmax <- max(am_many$df$time)
best_i <- which.max(am_many$posterior[Tmax,])
good_i <- am_many$posterior[Tmax,] > 0.1
pars[good_i,] %>% mutate(final_prob = as.numeric(am_many$posterior[Tmax,good_i])) 
```



```{r}
model1 <- dim(am_many$posterior)[2]

am_multi <- am_many$df %>% 
  mutate(belief = am_many$posterior[,model1],
         stock = states[state],
         quota = actions[action]) %>%
  select(time, belief, stock, quota) 
write_csv(am_multi, "../data/am_multi.csv")
```

```{r}
am_multi %>%
  ggplot(aes(time, stock, col = belief)) + 
  geom_line() + 
  geom_point() +
  geom_point(aes(time, quota), col="darkgreen", shape=2) + 
  scale_colour_gradient(limits = c(0,1), low = pal[4], high = pal[1]) + 
  ylab("fish stock") + labs(caption = "Belief in model 1 falls off rapidly as magagement pushes stocks to lower and lower sizes.")



```



```{r}
posteriors <- data.frame(r = rs, K = Ks, t(am_many$posterior)) %>%
  pivot_longer(starts_with("X"), 
               values_to = "probability",
               names_to = "time") %>% 
  mutate(time = as.integer(as.factor(time))) 

i <- 20
r_marginals <- posteriors %>% group_by(time,r) %>% 
  summarise(prob = sum(probability), .groups = "drop") %>% 
  filter(time %in% i)

r_marginals %>%  ggplot(aes(r, prob, group=time)) + geom_bar(stat="identity") + facet_wrap(~time)
```


## Marginal Posterior Probability evolution over K

```{r}
i = 20
K_marginals <- posteriors %>% group_by(time,K) %>% 
  summarise(prob = sum(probability), .groups = "drop") %>% 
  filter(time %in% i) 
K_marginals %>%
  ggplot(aes(K, prob, group=time)) + geom_bar(stat="identity") + facet_wrap(~time) + scale_x_log10()


```



```{r}
policy <- mdp_compute_policy(transitions[-1],  
                           reward = reward, 
                           discount = discount,
                           model_prior = model_prior,
                           Tmax = 20)
non_adaptive <- memoise::memoise(mdp_planning, cache = memoise::cache_filesystem("cache/"))
x0 <- which.min(abs(states - 6))
## start with 90% weight on 'model 1'
model_prior <- c(rep(.1, length(models)-2) / (length(models) - 2), .9)

non_am <- non_adaptive(transitions[-1],  
                           reward = reward, 
                           discount = discount,
                           model_prior = model_prior,
                           x0 = x0, 
                           policy = policy,
                           Tmax = 20, 
                           true_transition = transitions[[true_model]], 
                           epsilon = 0.001, 
                           max_iter = 2000)
```

```{r}
am_npv <- sum(am_many$df$value * discount ^ am_many$df$time)
am_npv
```




