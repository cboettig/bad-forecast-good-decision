---
title: ''
author:
- name: Carl Boettiger
  email: cboettig@berkeley.edu
  affiliation: a,1
date: "`r Sys.Date()`"

abstract: |

  
  Model-based forecasts have been enabled by recent explosion of data and computational methods and spurred on by decision-makers appetite for forecasts in everything from elections to pandemic response. It is taken for granted that the model which makes the most accurate forecast, accounting for uncertainty, will also be the best model to inform decision-making.  Using a classic example from fisheries management, I demonstrate that selecting the model that produces the most accurate and precise forecast can lead to decidedly worse outcomes. This situation can arise whenever the models are only approximations of the reality, underscoring the risk of evaluating models only by prediction accuracy and not decision objectives using formal tools of decision theory.

corresponding_author:
  code: 1
  text: "To whom correspondence should be addressed. E-mail: cboettig@berkeley.edu"

bibliography: refs.bib
address:
- code: a
  address: Department of Environmental Science, Policy, and Management, University of California, 130 Mulford Hall Berkeley, CA
    94720-3114, USA
    
keywords:
- transients,
- optimal control,
- adaptive management,
- stochasticity,
- uncertainty,
- ecological management

# For footer text
lead_author_surname: Boettiger

significance: |


acknowledgements: |
  This work was supported in part by NSF CAREER (#1942280) and computational resources from NSF's XSEDE Jetstream (DEB160003) and Chameleon cloud platforms.


## must be one of: pnasresearcharticle (usual two-column layout), pnasmathematics (one column layout), or pnasinvited (invited submissions only)
pnas_type: pnasresearcharticle
csl: pnas.csl

## change to true to add optional line numbering
lineno: true

output: rticles::pnas_article
---


Global change issues are complex and outcomes are difficult to predict, making approaches to uncertainty a central part
of effective decision-making [@ @]. While some uncertainty is intrinsic to the underlying processes (stochasticity) or by
limits to what variables we can observe and how accurately we measure them (measurement uncertainty), much uncertainty 
is the result of our imperfect knowledge of the processes involved, as expressed by the structure and
parameters of mathematical models used to approximate those processes (model uncertainty). Unlike the other forms of
uncertainty, model uncertainty can be 
reduced by gathering additional data, which may rule out certain models or parameter values as implausible (). 
When model parameters are estimated directly from available data, there is a risk that the best-fitting models may 
_overfit_ to patterns arising from by chance from the stochasticity or measurement error which do not reflect the
underlying process. Researchers have often favored simpler models which are less prone to over-fitting, sometimes
through explicit penalties such as information criteria [@], though these can be misleading [@]. With the rapid 
expansion of available ecological and environmental data [@], it is increasingly possible to rely instead on comparisons
between model predictions and data purposely excluded from model estimation (cross-validation), or better, 
comparisons between model predictions and data collected in the future (forecasting, @Clark2001; @Dietze2018]). 
Over sufficiently long timescales, comparing model forecasts to future observations will select the models with
greatest predictive accuracy.  Because model-based forecasts frequently play an important role in decision-making,
it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty
will also improve decision-making outcomes. Here, I illustrate that this need not be the case: it is quiet possible
that any approach with improves the forecast accuracy of the model(s) over time can simultaneously lead to worse 
decision outcomes. This example illustrates that knowing when increased forecast accuracy will or will not also improve 
decision outcomes in general remains an open problem, as is the challenge of what to do about it.



<!-- 
It may be easy to dismiss this example on the argument that both models are (or more precisely,
the any logistic growth model, regardless of parameterization would be) extremely naive
and unrealistic, and any solution would surely begin with replacing both models with something
more realistic.  Yet such objections overlook the simple fact that for all its faults in 
prediction, model 1 already provides nearly optimal performance.  That performance arises
because model 1 has very accurately reflected one key aspect of the true model (the position of the
maximum growth rate), even while it is wrong about every other aspect of the biology. 

The open problem is simply to propose an approach to model uncertainty which would select model 1
over model 2 (i.e. choosing among the suite of available models, the one that gives the better 
decisions.)


An important aspect of this example is that model 1 performs nearly optimally. 


-->

I illustrate this problem using an example from fisheries management. 
Fisheries are a significant economic and conservation concern world
wide and their management remains an important debate [e.g. @Worm2006;
@Worm2009; @Costello2016]. Moreover, their management has been a proving 
grounds for theoretical and practical decision-making issues which are widely
applicable in other areas of ecology and conservation [@Ludwig1993; @Lande1994],
and one that has long wrestled with issues of uncertainty in the context
of management decisions [e.g. @Clark1973; @Reed1979; @Walters1981; @Ludwig1982].
While modern fisheries management frequently relies on complex models which may
contain scores of parameters to reflect the specific age or stage structure of a
specific fish stock [@ram], I will rely on simple, well-studied models which
permit greater intuition and generalization [@Levins1966]. Consistent with 
such previous work [@Schaefer1954; @Clark1973; @Reed1979; @Walters1981; @Ludwig1982; @Costello2016],
let us consider the problem of determining the optimal harvest policy given a 
measurement of the current stock size.  

# Ecological Models

The sustainable harvest decision problem can be stated as follows: The fish stock is observed to be in state $X_t$ at time $t$, and is then subjected to some harvest $H_t$ before recruiting new fish, subject to stochastic environmental noise $\xi_t$, to bring the stock to $X_t+1$,

\begin{equation}
X_{t+1} = f(X_t - H_t, \xi_t) 
\end{equation}

A manager seeks each year to select the harveset $H_t$ which will maximize the sum
of the utility derived from such a harvest and such a state, $U(X_t,H_t)$, over all time,
subject to discount rate $\delta$ [@Clark1973]:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t
\end{equation}


Further we imagine that the function $f$ is not known precisely, and so we will rely on an evaluation of forecasting skill across a set of candidate models to determine which one to use to manage the fishery.  Again for simplicity, we will restrict ourselves to two simple candidate models $f_1$ and $f_2$.  Both share the same underlying structure of logistic recruitment (known as the Gordon-Schaefer model in fisheries context owing to groundbreaking work independently by @Schafer1954 and @Gordon1954), differing only in their choice of certain parameters:

\begin{equation}
f_i(Y) = Y + r_i Y \left( 1 - \frac{Y}{K_i} \right) * \xi_t(\sigma)
\end{equation}

Where $\xi_t(\sigma)$ represents log-normal random noise with mean of unity and log-standard-deviation $\sigma$.
Model 1 is given by $r_1 = 2$, $K_1 = 16$, $\sigma_1 = 0.05$, Model 2 by $r_2 = 0.5$,  $K_2 = 10$, $\sigma = 0.075$ (in dimensionless units).  Having both the larger growth rate and the larger carrying capacity, Model 1 is clearly the more optimistic of the two choices. 

Selecting between Model 1 and Model 2 can thus be considered the simplest illustration of the model uncertainty problem. This is a subset of the more general problem of selecting model parameters, assuming a logistic growth, which itself is a subset of estimating the best structural from (e.g. Ricker, Beverton-Holt, etc).  There is no need to consider these more complicated versions of the model uncertainty problem here, since they all inherit the same issue.  Reducing the model selection problem to these two models simplifies the presentation and will aid intuition at no loss of generality.  

The only additional assumption we will need is that the "true" model is not among the suite of models under consideration. 
Mathematical models are, at best, approximations of the underlying processes.  Ecological processes are much too complex to ever be modeled exactly.  For illustrative purposes, we will thus assume the "true" process to be given by a third model, which is unknown to the decision-maker, but similar enough to at least one of the candidate models might be considered a reasonable approximation.

\begin{equation}
f_3(Y) = Y + r_3 Y^4 \left( 1 - \frac{Y}{K_3} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$. 



# Managing Under Model Uncertainty 

A wide range of paradigms are available for approaching the issue of decision-making under uncertainty. These approaches can roughly be divided into two groups: the first group treats the issue of model uncertainty independently from the decision itself, while the second integrates the process of reducing model uncertainty into the process of decision making to maximize the value of some objective. There are a wide range of techniques within each, and it is also possible to blend approaches. The key distinction is that methods in the first group do not involve any direct consideration of the possible actions or the utility that may result from those actions in how they select models [statistical approaches such as information criteria @BurnhamAnderson; and in particular, forecasting evaluation @Clark2001; @Dietze2018], while those in the second group require a more explicit statement of possible actions and the desired objectives [Decision theoretic approaches, for which @Polasky2011 provides an excellent and accessible review]. I illustrate how the most promising techniques from each of these groups would be applied to this simple problem, and demonstrate that it both cases they lead us away from the model that produces the most desirable decisions towards worse outcomes.  In retrospect, it will become obvious that neither these nor any other widely applied methods will select the model that leads to the best outcomes from the set of models considered.  


**Forecasts and Strictly Proper Scoring**

Like many decision problems, the task of setting a sustainable harvest quota appears to hinge on having an accurate forecast: if we can predict to what size the fish stock will increase next year, $X_t+1$, and we know the current stock, $X_t$, then we can safely harvest $X_{t+1} - X_t$.  Overestimating or underestimating such recruitment will result in over-harvesting or under-harvesting, respectively.  Selecting a model based on forecast skill is also justifiable on theoretical grounds, since it reduces the risk of over-fitting by comparing model predictions to later observations that were not used to estimate the model. @Gneiting2007 provides a rigorous proof for "strictly proper" scoring criteria, which have the desirable property which no model predicting the distribution of future outcomes, $Q(x)$ can achieve a better average score than the true model $P(x)$. That is, unlike likelihood or other goodness-of-fit scores, it is impossible to overfit when conditioning on a strictly proper score -- since no model model can out-perform the true model. Not that strictly proper scoring rules score _probabilistic forecasts_ and not just point predictions, favoring models which accurately reflect the uncertainty over those which under-estimate it.  These features have made proper scoring rules for probabilistic forecasts a successful and popular approach for addressing model uncertainty in many other areas [@Gneiting2014; @Raftery2016] and a promising tool for evaluation of ecological forecasts [@Dietze2018]. 

I illustrate the process of model selection by strictly proper scoring rules using two scenarios.  


**Decision-Theoretic Approaches**

Decision-theoretic approaches include scenario analysis, resilience thinking, optimal control and related methods [@Polasky2011], which have a long history in ecology and particularly in fisheries management. One of the most influential of these is _adaptive management_ [sensu @Walters1978], in which a manager seeks to both reduce uncertainty over time while also achieving the best outcomes given current knowledge.  While the term today is frequently used in a looser sense, adaptive management as originally developed can be considered an example of optimal control under model uncertainty

[@Walters1981; @Smith1981]


# A way forward?

note that the model set considered here already contains a model which captures the key feature. 

- Greedy strategy will not work.  Could stick with a model for 50 years then switch...
- A mechanistic understanding of the decision proces, not the ecological one.  Analyzing the decision problem, we know that in this situation, the optimal stategy depends only on the location of the peak of the growth rate.  This could suggest a very different way forward
- 



Yet the model that leads to the best decisions is not always the model that makes the most accurate forecasts, as I illustrate here. Surprisingly, this can even happen when a decision is derived directly from a complex optimization routine of a probabilistic predictive models [@Marescot2013].  Reality is complex; even our best models can only ever be approximations of underlying processes.  Here, I use a classic, well-understood example from fisheries management [@Schaefer1954; @Clark1973; @Reed1979] to illustrate both the paradox of how a model with the worst forecast provides the best decision outcomes, as well as show how we can avoid selecting models that are poorly suited for management by considering the management context more explicitly. These results underscore that in choosing the best model for decision-making, it can be more important to capture a single key feature of the process than it is to make the most accurate prediction about future states.  

<!--
Many ecological management problems are sequential decision problems, in which each year (or other interval) a manager must observe the state of the system and choose a course of action to maximize long term objectives. Such problems inherently depend on forecasts: each possible action can result in a different forecast for the future state, typically reflecting some uncertainty as well.  The utility the manager derives may depend on both the choice of action and the state of the system, reflecting the costs and benefits associated with each. Sequential decision-making problems are distinguished by the need to think more than one move ahead. For instance, harvesting as many fish as possible in year one may maximize the market value that year, but if too few fish are left to reproduce then harvest in future years will suffer.  The same calculus of thinking ahead frequently applies to rebuilding species populations as well [e.g. @Lambert;  @Chades2008].  
-->








```{r, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(6)
txtcolor <- "#586e75"
```


```{r, include=FALSE}
rmarkdown::render("appendix.Rmd")
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE, dev="cairo_pdf")
```



# Results

## Forecast performance

```{r figure1, fig.width=7, fig.height=5, fig.cap = "Forecast performance of each model.  Panels A, B: Step ahead predictions of stock size under unfished (A) and fished (B) scenarios. Error bars indicating the 95% confidence intervals around each prediction, while stars denote the observed value in that year.  Because the models make different decisions each year in the fished scenario, the observed stock size in year 2, 3, etc under the management of model 1 (blue stars) is different from that under model 2 (red stars).  Panels C, D: corresponding distribution of proper scores across all predictions (100 replicates of 100 timesteps). Higher scores are better, confirming that model 2 makes the better forecasts."}
fig1ab + fig1cd + 
  plot_layout(widths =  c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```

The one-step-ahead prediction performance of each model in a simulation of an un-fished environment show consistently better performance of Model 2 (Fig. 1A). Model 1 predictions appear far too optimistic, with the true value falling well below the 95% confidence intervals.  In contrast, all observed values fall easily within the confidence intervals produced by model 2.  

Predictive performance of the un-fished population does not give us the full picture, since it reflects predictive accuracy only in the region of the true carrying capacity, while an actively harvested stock will be at a lower size.  The model that predicts the equilibrium size may not be the one that best forecasts stock recovery. This comparison does not reflect the influence of any decisions that might be made based on the model forecast.  To address these concerns, we consider a second scenario where our fishery is managed according to the optimal harvest predicted by each model in turn.  Each year the model produces both a forecast and a decision about the harvest quota. (The mechanics of determining a harvest quota given the model follow standard methods for Markov Decision Process, which depend on step-ahead predictions [@Marescot2013], see appendix for details.) We then implement that harvest and compare the observed stock size the following year to that which the model has predicted, (Fig. 1B).  Again we observe that the observations under model 1 consistently fall well outside of the 95% confidence intervals it predicts, while under model 2, stock sizes consistently fall within the predicted intervals.  Once again, model 2 shows a higher forecast accuracy while model 1 appears consistently over-optimistic.  

Interest in assessing probabilistic forecasts has led to the development of rigorous methods of scoring [e.g. @Gneiting2014; @Raftery2016]. A scoring rule is "proper" if it has the convenient property that no other prediction can achieve a higher score, on average, than we would if we used the true distribution [@Gneiting2007]. The distribution of proper scores across 100 replicate simulations for both un-fished and actively managed scenarios (Fig. 1C, 1D respectively) show consistently higher scores of model 2, using a proper scoring rule (Eq (27) of Ref. @Gneiting2007).  


## Ecological and economic performance

Given this evidence, model 2 clearly provides the more accurate forecast and we would no doubt conclude that model 2 was thus a better approximation of the true model and thus the better choice to inform decision making about harvest quotas.  Yet if we revisit our experiment of managing the fishery under each model in turn, and focus not on _predictive accuracy_ but on _ecological_ and _economic_ outcomes, it quickly becomes clear that model 1 gives much better results (Fig. 2A).  For comparison, we have also included the results of optimal management given the true model.  Despite its optimistic predictions, model 1 does not result in over-fishing, but holds the stock near the same level as the optimal management strategy.  In contrast, model 2 suppresses the stock to a much lower level. The over-fishing in model 2 is not economically efficient either (Fig. 2B).  The net present value of the fishery, as calculated as the cumulative, discounted value of the harvest (assuming a fixed unit price for fish with negligible cost for harvest, see appendix) under the fishing regime of model 1 falls precisely along that of the optimal solution, while the value derived under model 2 is consistently lower.  

```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast. Harvest quotas derived from Model 1 result in a significantly higher fish stock size than under Model 2 (panel A). Economic returns under Model 1 are also substantially higher (panel B)"}
fig2a + fig2b
```


## Discussion


This paradox in performance of forecasting vs performance in decision making can be easily resolved by considering the context of the decision problem more closely. Comparing plots of the functional form of our two logistic-curve models, compared to the functional form of the "true" model used to drive the simulations (Fig. 3A), it is clear to see that model 2 does indeed lie closer to the true model throughout the state space, agreeing precisely with the true carrying capacity (where both functions cross zero with negative slope).  However, the peak of model 3 very nearly matches the peak of model 1. The optimal decision literature, dating back to the 1950s [@Schaefer1954], demonstrates that the Maximum Sustainable Yield (MSY) is maintained by harvesting a stock down to the size at which it achieves its maximum growth rate, i.e. 50% of the un-fished equilibrium size for a symmetric growth model ($K/2$).  Model 1, while being very wrong about both the growth rate and the un-fished equilibrium, is nevertheless nearly perfect in estimating the stock size at which maximum growth rate is achieved, and this gives nearly optimal decisions (Fig 3B) despite its terrible forecasts.   

Thus, each year our model 1 managers are again chagrined to see the stock size estimates come in far below their rosy predictions, but nevertheless manage to set a nearly optimal quota by comparing the observed stock size to the model's predicted optimal escapement level [@Reed1979].  Meanwhile, model 2 managers could only congratulate themselves that each year's observations fall neatly within their predicted interval, unaware that the they were over-exploiting the fishery by both economic and ecological metrics.  If we had access to model 3, we would no doubt find that it outperformed model 2 in forecast accuracy as well as ecological and economic performance.  But in real ecological decision making, we never know the true model -- we will always be comparing among approximations.  Within fisheries, even in today's parameter-rich age-structured models, recruitment approximations with symmetric growth functions (Logistic, Ricker, Beverton-Holt, etc) still dominate [@ramlegacy2012; @ramlegacy2018].  

This issue is by no means unique to fisheries.  Throughout resource management and conservation, and no doubt other fields, decisions about which model to use are guided by which model best fits available data [@Clark1990]. Increasingly, these are joined by calls to assess _forecast accuracy_ [@Clark2001; @Dietze2018; @White2019] as the ultimate test of a model.  Yet as this example illustrates, such metrics, no matter how rigorously defined, may select entirely the wrong model for the task at hand.  A decision maker has other objectives than prediction accuracy, and approaches which ignore these considerations do so at their peril. This example has also shown that once we are managing with the wrong model, no amount of comparing predictions from that model to actual outcomes will guarantee we discover our mistake.  Despite its consistently good predictions, model 1 is in fact over-fishing to a dangerous level.  

Because we will never know the "true" model, we must never forget that our choice of models must reflect the context for which those models will be used [@Levins1966].  Model 2 would indeed be a better choice than model 1 if our objective was to determine the natural size of our fish stock in the absence of fishing.  Only when we focus on the outcomes we actually care about -- in this case, economic and ecological performance -- can we see which model is best for decision-making.  Model 1, despite its many mistakes, is right about one key feature: the biomass for peak growth -- and that is enough to guarantee nearly optimal performance.   This conclusion should also be reassuring to both modelers and decision makers, for it reminds us that effective models need be perfect or even all that close in every aspect, as long as they capture the key features of the decision context.  Decision theory [e.g. @Clark1973; @Reed1979; @pomdp-intro] and research into the socio-ecological models [e.g. @Kareiva2006] helps us better understand that context.  Adaptive management approaches [@Walters1978] can apply that theory to compare management outcomes between models directly.  It is not true that we need good forecasts to make good decisions.




```{r figure3, fig.width=7, fig.height=4, fig.cap="Panel A: Population recruitment curves for each model, compared to that of the true Model. 3 Model 2 more closely approximates the true Model 3, but note that maximum value Model 1 and Model 3 occur at nearly the same value for the state, x.  Panel B: The computed optimal policy of each model, derived by SDP, expressed in terms of the target escapement (population size remaining after harvest) for each possible stock size. Model 1 over-harvests consistently, while the target escapement under Model 2 is nearly identical to that of the true Model 3."}
fig3a + ggtitle("A") + fig3b + ggtitle("B")
```

# Methods

Stochastic transition matrices are defined for models 1-3 on a discrete grid of
240 possible states spaced uniformly from 0 to 24. A discrete action space 
enumerating possible harvest quotas is set to the same grid. The utility of a harvest quota $H_t$ given a population state $X_t$ is given by $U(X_t, H_t) = \min(X_t, H_t)$ (i.e. a fixed price for realized harvest). A modest discount of
$\gamma = 0.99$ allows comparisons to approaches that ignore [@Schaefer1954] or include [@Clark1973; @Reed1979] discounting; results are not sensitive to this choice. The optimal policy for each model is determined by stochastic dynamic programming [@Marescot2013].  Details of the implementation, including fully reproducible R code, have been included in the appendix.




\pagebreak 


# References