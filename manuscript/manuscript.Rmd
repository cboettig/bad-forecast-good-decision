---
title: 'Are we learning to build better models at the cost of better decisions?'
author:
- name: Carl Boettiger
  email: cboettig@berkeley.edu
  affiliation: a,1
date: "`r Sys.Date()`"

abstract: |
  Model-based forecasts have been enabled by recent explosion of data and computational methods and spurred on by decision-makers appetite for forecasts in everything from elections to pandemic response. It is taken for granted that the model which makes the most accurate forecast, accounting for uncertainty, will also be the best model to inform decision-making.  Using a classic example from fisheries management, I demonstrate that selecting the model that produces the most accurate and precise forecast can lead to decidedly worse outcomes.

corresponding_author:
  code: 1
  text: "To whom correspondence should be addressed. E-mail: cboettig@berkeley.edu"

bibliography: refs.bib
address:
- code: a
  address: Department of Environmental Science, Policy, and Management, University of California, 130 Mulford Hall Berkeley, CA
    94720-3114, USA
    
keywords:
- transients,
- optimal control,
- adaptive management,
- stochasticity,
- uncertainty,
- ecological management



## change to true to add optional line numbering
lineno: true

output: rticles::elsevier_article
layout: 3p
---


```{r, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"
```
```{r, include=FALSE}
rmarkdown::render("appendix.Rmd")
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, dev="cairo_pdf")
```


<!-- I think the introduction could have a clearer storyline and better introduce some of the ideas you bring up later on in the methods/results. A quick outline idea: 

  - Paragraph1: Model-based forecasts have become a cornerstone of applied science in disciplines as diverse as atmospheric science [weather?], economics [stock markets?], and ecology [dietz]. Seeking to inform a variety of decisions, near-term forecasting in these diverse fields has  simultaneously been encouraged by policy-makers [CITE?] and enabled by the rapid and recent increase in data availability and computational methods leading to a rapid expansion of thier use (or something...)
  
   - Paragraph 2: Introduce ecological decision making under uncertainty, rapid global change, etc. (kind of like your current opening bit)

  - Paragraph 3: But do better forecast always lead to better decisions?  Because model-based forecasts frequently play an important role in decision-making,it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty will also improve decision-making outcomes. 
            - While the literature has largely addressed issues of overfittting that might lead forecasts to misguide decisions, proper scoring methods blah blah [...introduce this concept]. 
            - Beyond overfitted models,... [maybe add something about Value of Information literature here?]
  
  - Paragraph 4: Your current paragraph on "I illustrate..." (I'd add into that paragraph something about the fact that you explore adaptive management?)

  
A second idea on framing: this maybe is a to divergent from the main point (or leads you into a post-modernist narrative that you'd prefer to avoid...) but it seems like you could reframe the whole thing as an argument for embracing an inclusive process of resource management that acknowledges pluralism.  
  (1) Policy makers often think "scientists" hold knowledge of "an environmental reality"
  (2) Despite this not being true (scientist disagree on a lot of things), even when "science" points to a more likely reality, that might be off in ways that can fundementally misguide decisions. 
  (3) The problem is deeper than acknowledging a "rashamon set" of models or multiple possible descriptions of of reality. Sometimes what appears closer to reality is actually off in a way that is critical to the decision.
-->

<!-- Open on real-world issues, uncertainty, particularly model uncertainty as key theme.  -->

Global change issues are complex and outcomes are difficult to predict, making approaches to uncertainty a central part
of effective decision-making [@ @]. Reducing that uncertainty by improving models in response to more data
becomes available 



While some uncertainty is intrinsic to the underlying processes (stochasticity) or by
limits to what variables we can observe and how accurately we measure them (measurement uncertainty), much uncertainty 
is the result of our imperfect knowledge of the processes involved, as expressed by the structure and
parameters of mathematical models used to approximate those processes (model uncertainty). Unlike the other forms of
uncertainty, model uncertainty can be 
reduced by gathering additional data, which may rule out certain models or parameter values as implausible[^1]. 
When model parameters are estimated directly from available data, there is a risk that the best-fitting models may 
_overfit_ to patterns arising from by chance from the stochasticity or measurement error which do not reflect the
underlying process [@Ginzburg2004]. Researchers have often favored simpler models which are less prone to over-fitting, sometimes
through explicit penalties such as information criteria [@], though these can be misleading [@]. With the rapid 
expansion of available ecological and environmental data [@], it is increasingly possible to rely instead on comparisons
between model predictions and data purposely excluded from model estimation (cross-validation), or better, 
comparisons between model predictions and data collected in the future (forecasting, @Clark2001; @Dietze2018]). 
Over sufficiently long timescales, comparing model forecasts to future observations will select the models with
greatest predictive accuracy.  Because model-based forecasts frequently play an important role in decision-making,
it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty
will also improve decision-making outcomes. Here, I illustrate that this need not be the case: it is quiet possible
that any approach with improves the forecast accuracy of the model(s) over time can simultaneously lead to worse 
decision outcomes. This example illustrates that knowing when increased forecast accuracy will or will not also improve 
decision outcomes in general remains an open problem, as is the challenge of what to do about it.

[^1]: While some authors distinguish between model uncertainty and parameter uncertainty, I will refer collectively to
any uncertainty that arises from our imperfect knowledge of the system as "model uncertainty". Uncertainty in model structure can often be reflected by appropriate parameterization, just as models with different parameter values can be treated distinctly. Whereas stochasticity is inherent to the process (or rather, to our choice of state variables, @Boettiger2018) and measurement uncertainty can only be reduced by more accurate tools for measurement, model uncertainty is reducible by learning from additional data. 


<!-- 
It may be easy to dismiss this example on the argument that both models are (or more precisely,
the any logistic growth model, regardless of parameterization would be) extremely naive
and unrealistic, and any solution would surely begin with replacing both models with something
more realistic.  Yet such objections overlook the simple fact that for all its faults in 
prediction, model 1 already provides nearly optimal performance.  That performance arises
because model 1 has very accurately reflected one key aspect of the true model (the position of the
maximum growth rate), even while it is wrong about every other aspect of the biology. 

The open problem is simply to propose an approach to model uncertainty which would select model 1
over model 2 (i.e. choosing among the suite of available models, the one that gives the better 
decisions.)


An important aspect of this example is that model 1 performs nearly optimally. 


-->

I illustrate this problem using an example from fisheries management. 
Fisheries are a significant economic and conservation concern world
wide and their management remains an important debate [e.g. @Worm2006;
@Worm2009; @Costello2016]. Moreover, their management has been a proving 
grounds for theoretical and practical decision-making issues which are widely
applicable in other areas of ecology and conservation [@Ludwig1993; @Lande1994],
and one that has long wrestled with issues of uncertainty in the context
of management decisions [e.g. @Clark1973; @Reed1979; @Walters1981; @Ludwig1982].
While modern fisheries management frequently relies on complex models which may
contain scores of parameters to reflect the specific age or stage structure of a
specific fish stock [@ramlegacy2018; @ram], I will rely on simple, well-studied models which
permit greater intuition and generalization [@Levins1966]. Consistent with 
such previous work [@Schaefer1954; @Clark1973; @Reed1979; @Walters1981; @Ludwig1982; @Costello2016],
let us consider the problem of determining the optimal harvest policy given a 
measurement of the current stock size.  

# Ecological Models

The sustainable harvest decision problem can be stated as follows: The fish stock is observed to be in state $X_t$ at time $t$, and is then subjected to some harvest $H_t$ before recruiting new fish, subject to stochastic environmental noise $\xi_t$, to bring the stock to $X_t+1$,

\begin{equation}
X_{t+1} = f(X_t - H_t, \xi_t) 
\end{equation}

A manager seeks each year to select the harvest $H_t$ which will maximize the sum
of the utility derived from such a harvest and such a state, $U(X_t,H_t)$, over all time,
subject to discount rate $\delta$ [@Clark1973]:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t \label{utility}
\end{equation}


Further we imagine that the function $f$ is not known precisely, and so we will rely on an evaluation of forecasting skill across a set of candidate models to determine which one to use to manage the fishery.  Again for simplicity, we will restrict ourselves to two simple candidate models $f_1$ and $f_2$.  Both share the same underlying structure of logistic recruitment (known as the Gordon-Schaefer model in fisheries context owing to groundbreaking work independently by @Schaefer1954 and @Gordon1954), differing only in their choice of certain parameters:

\begin{equation}
f_i(Y) = Y + r_i Y \left( 1 - \frac{Y}{K_i} \right) * \xi_t(\sigma)
\end{equation}

Where $\xi_t(\sigma)$ represents log-normal random noise with mean of unity and log-standard-deviation $\sigma$.
Model 1 is given by $r_1 = 2$, $K_1 = 16$, $\sigma_1 = 0.05$, Model 2 by $r_2 = 0.5$,  $K_2 = 10$, $\sigma = 0.075$ (in dimensionless units).  Having both the larger growth rate and the larger carrying capacity, Model 1 is clearly the more optimistic of the two choices. 

Selecting between Model 1 and Model 2 can thus be considered the simplest illustration of the model uncertainty problem. This is a subset of the more general problem of selecting model parameters, assuming a logistic growth, which itself is a subset of estimating the best structural from (e.g. Ricker, Beverton-Holt, etc).  There is no need to consider these more complicated versions of the model uncertainty problem here, since they all inherit the same issue.  Reducing the model selection problem to these two models simplifies the presentation and will aid intuition at no loss of generality.  

The only additional assumption we will need is that the "true" model is not among the suite of models under consideration. 
Mathematical models are, at best, approximations of the underlying processes.  Ecological processes are much too complex to ever be modeled exactly.  For illustrative purposes, we will thus assume the "true" process to be given by Model 3, which is unknown to the decision-maker, but similar enough to at least one of the candidate models might be considered a reasonable approximation:

\begin{equation}
f_3(Y) = Y + r_3 Y^4 \left( 1 - \frac{Y}{K_3} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$. 


The task of deciding whether Model 1 or Model 2 would be the better choice for decision making is 
thus perhaps the simplest example of the much studied issue of model uncertainty that we can pose. 
As in any real world scenario, neither model is the true model, but nevertheless this model set
contains a good enough approximation of the true model to make good decisions.  However, any of
the well-developed approaches for decision-making under model uncertainty will prefer Model 2 over
Model 1, despite the fact that the optimal policy under Model 2 leads to much worse outcomes
ecologically and economically.  


# Methods for Managing Under Model Uncertainty 

<!-- I feel like parts of the below paragrph should be in the abstract (i.e. "I illustrate how the most promising techniques from statistical approaches and decision theoretic approaches to decision making under uncertainty would be applied to this simple problem, and demonstrate that it both cases they lead us away from the model that produces the most desirable decisions towards worse outcome") -->

A wide range of paradigms are available for approaching the issue of decision-making under uncertainty. These approaches can roughly be divided into two groups: the first group treats the issue of model uncertainty independently from the decision itself, while the second integrates the process of reducing model uncertainty into the process of decision making to maximize the value of some objective. There are a wide range of techniques within each, and it is also possible to blend approaches. The key distinction is that methods in the first group do not involve any direct consideration of the possible actions or the utility that may result from those actions in how they select models [statistical approaches such as information criteria e.g. @Burnham1998]; and in particular, forecasting evaluation @Clark2001; @Dietze2018], while those in the second group require a more explicit statement of possible actions and the desired objectives [Decision theoretic approaches, for which @Polasky2011 provides an excellent and accessible review]. I illustrate how the most promising techniques from each of these groups would be applied to this simple problem, and demonstrate that it both cases they lead us away from the model that produces the most desirable decisions towards worse outcomes.  In retrospect, it will become obvious that neither these nor any other widely applied methods will select the model that leads to the best outcomes from the set of models considered.  

## Forecasts and Proper Scores

<!-- I think this subsection should be renamed. it makes it a bit hard to follow the structure of the larger section. Seems like something like: "Statistical approaches: forecasts and proper scores" idk, but something that more closely aligns with the above paragraph and the second subsection -->

Like many decision problems, the task of setting a sustainable harvest quota appears to hinge on having an accurate forecast: if we can predict to what size the fish stock will increase next year, $X_t+1$, and we know the current stock, $X_t$, then we can safely harvest $X_{t+1} - X_t$.  

<!-- it is unclear what you mean by safely harvest?  as in the stock wont crash? or it won't be worse than it is this year? You could imaging that we could safetly harvest more (or only less) than that.-->

Overestimating or underestimating such recruitment will result in over-harvesting or under-harvesting, respectively. 

Selecting a model based on forecast skill is also justifiable on theoretical grounds, since it reduces the risk of over-fitting by comparing model predictions to later observations that were not used to estimate the model.

@Gneiting2007 provides a rigorous proof for "proper" scoring criteria, which have the desirable property which no model predicting the distribution of future outcomes, $Q(x)$ can achieve a better average score than the true model $P(x)$. That is, unlike likelihood or other goodness-of-fit scores, it is impossible to overfit when conditioning on a strictly proper score -- since no model model can out-perform the true model. Not that strictly proper scoring rules score _probabilistic forecasts_ and not just point predictions, favoring models which accurately reflect the uncertainty over those which under-estimate it.  These features have made proper scoring rules for probabilistic forecasts a successful and popular approach for addressing model uncertainty in many other areas [@Gneiting2014; @Raftery2016] and a promising tool for evaluation of ecological forecasts [@Dietze2018]. 

<!-- This scoring bit comes abruptly (and it feels like could be in the introduction) -->


I illustrate the process of model selection by strictly proper scoring rules using two scenarios. In Scenario A (passive observation) the fish stock is unharvested and allowed to recover towards carrying capacity (as simulated under our "true" model, Model 3) while comparing the observed stock size in each subsequent time step to the distribution predicted under Model 1 and Model 2 respectively [Fig 1].  The mean, $\mu$ and variance, $\sigma$ of the forecast are compared against the true observation $x$ using a Proper scoring rule of @Gneiting2007, 

\begin{equation}
-(\mu - x )^2 / \sigma^2  - \log(\sigma) \label{proper}
\end{equation}

for each prediction over 100 replicate simulations of 100 time steps each [Fig 1].



```{r figure1, fig.width=7, fig.height=5, fig.cap = "Forecast performance of each model.  Panels A, B: Step ahead predictions of stock size under unfished (A) and fished (B) scenarios. Error bars indicating the 95% confidence intervals around each prediction, while stars denote the observed value in that year.  Because the models make different decisions each year in the fished scenario, the observed stock size in year 2, 3, etc under the management of model 1 (blue stars) is different from that under model 2 (red stars).  Panels C, D: corresponding distribution of proper scores across all predictions (100 replicates of 100 timesteps). Higher scores are better, confirming that model 2 makes the better forecasts."}
predictions <- read_csv("../data/predictions.csv") %>% mutate(model = as.character(model))

fig1cd <- predictions %>%
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 2, show.legend = FALSE) +
  coord_cartesian(xlim = c(-100, 1), ylim = c(0,4000)) +
  xlab("Proper score") +
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "C.",
    B_fished = "D."))) + 
  scale_x_continuous(breaks = c(-100, 0)) +
  theme(axis.text.y = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )



 fig1ab <- predictions %>% 
  filter(reps == "2", time < 10) %>%
  ggplot(aes(time, col = model, fill = model)) + 
  geom_point(aes(y = expected)) + 
  geom_errorbar(aes(ymin = low, ymax = high)) +
  geom_point(aes(y = true), pch = "*", size = 12, 
             alpha = 0.6) + 
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "A. Without fishing",
    B_fished = "B. Managed harvest"))) +
  ylab("stock size")
 
 
 
 fig1ab + fig1cd + 
  plot_layout(widths =  c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```


In Scenario B (actively harvest), I have first solved for the optimal management strategy for both Model 1 and Model 2 [Fig 1b]. For small noise and concave functions with linear reward structure this can be done analytically [see proof in @Reed1979], or solved more generally by stochastic dynamic programming [see review by @Marescot2013, details in Appendix].  Under this scenario, replicate simulations of the stock are harvested at each time step using the optimal quota dictated by either Model 1 and Model 2. The resulting stock sizes in the time-step following this harvest are once again compared to the probabilities predicted by each model using Eq \eqref{proper}. Model 2 unequivocally outperforms Model 1 in both scenarios of passive observation and active harvest.

Despite the clearly superior predictive accuracy of Model 2 in both scenarios, the outcomes from management under Model 2 are substantially worse.  We can assess such outcomes in less abstract terms than forecasting skill, such as economic value (in dollars) or the ecological value (unharvested biomass). In our simple formulation of the decision problem, the "utility" the manager seeks to maximize is simply the economic value (net present value: the discounted sum of all profits from future harvests, Eq \eqref{utility}) of harvested fish.  This formulation ignores any utility provided by fish that are not harvested, beyond their contribution to future potential harvests. While it is possible to include such contributions directly in the utility function being optimized [e.g. @Halpern2013], even without doing so, Model 1 maintains both a higher unharvested biomass and also leads to higher economic returns throughout [Fig 2].  


```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast. Harvest quotas derived from Model 1 result in a significantly higher fish stock size than under Model 2 (panel A). Economic returns under Model 1 are also substantially higher (panel B)"}

sims <- read_csv("../data/sims.csv")
npv_df <- read_csv("../data/npv_df.csv")


plot_ecology <- 
  sims %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  group_by(model, time) %>%
  summarise(mean_state = mean(state), sd = sd(state), .groups = "drop") %>%
  filter(time < 25, model != "3") %>%
  ggplot(aes(time, mean_state)) + 
  geom_line(aes(col = model), lwd=1.5, show.legend = FALSE) +
    geom_ribbon(aes(ymin = mean_state - 2*sd, 
                    ymax = mean_state + 2*sd,
                    fill = model),
                alpha = 0.2, show.legend = FALSE) +
  ylab("state") + 
  labs(subtitle = "A. Ecological outcomes")

optimal <- select(filter(npv_df, model == "3"), time, mean_npv)

plot_econ <- 
npv_df %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  filter(model != "3", time %in% seq(1,100, by = 5)) %>%
  ggplot(aes(time, mean_npv)) +
  geom_line(data = optimal, lwd = 1.5, col = "grey20") +
  geom_point(aes(col=model), size = 4, alpha = 0.8) + 
  ylab("Net present value") + xlab("time")  + 
  labs(subtitle = "B. Economic outcomes")


plot_ecology + plot_econ
```

Had we been able to include Model 3 in our forecast comparisons, it would equal or outperform the forecasting skill of both Model 1 and Model 2 (as guaranteed by the theorem of @Gneiting2007), while also matching or out-performing their economic utility (as guaranteed by the theorem of @Reed1979).  In practice, we never have access to the generating model. 

One obvious limitation in this comparison is that scenario B treats each model as fixed over the entire course of the simulation. Research has long emphasized the importance of learning and adaption in the face of new data whenever we are dealing with model uncertainty [@Polasky2011]. A related limitation is that in Scenario B, decisions were based either on assuming Model 1 is correct or assuming Model 2 is correct. A more robust approach [e.g. @Walters1978] incorporates the uncertainty over models directly into the decision calculations by integrating over the probability each model $M$ (or parameter value) is correct when calculating the utility; $U(x_t, a_t) = \int U(x_t, a_t | M) P(M)$. The probability assigned to each model can then be updated after each subsequent action in light of the resulting outcomes [@Ludwig1982, @Smith1981]. While a the wide range of strategies for such iterative updating are known [@Polasky2011]; all will fail to select the higher-performing of the two simple models considered  here. 


<!-- replacing step-ahead with n-step ahead would not matter-->

## Decision-Theoretic Approaches

Decision-theoretic approaches include scenario analysis, resilience thinking, optimal control and related methods [@Polasky2011], which have a long history in ecology and particularly in fisheries management. One of the most influential of these is _adaptive management_ [sensu @Walters1978], in which a manager seeks to both reduce uncertainty over time while also achieving the best outcomes given current knowledge.  While the term today is frequently used in a looser sense, adaptive management as originally developed can be considered an example of optimal control under model uncertainty [@Walters1981; @Smith1981].  In this approach, a manager assigns probabilities to each of the possible models under consideration, i.e. models 1 & 2.  The expected utility of any action reflects both the intrinsic uncertainty over future states under either model (process uncertainty) and the manager's uncertainty over the choice of models (or parameter values). Unlike the forecast comparison, an adaptive management approach explicitly considers the utility function, Eqn \eqref{utility}, which it seeks to maximize by integrating over model uncertainty. After observing the consequences of the action, the manager updates the posterior probabilities that each model is correct, typically by Bayes rule [e.g. @Walters1981; @Smith1981; @Ludwig1982; @Punt2016]. The decision strategy is then re-evaluated in light of this revised uncertainty.  Adaptive management may be either passive or active. Under passive adaptive management, the decision-maker chooses the action which maximizes that expected future utility.  Under active adaptive management, the decision-maker may chose 'exploratory' actions which are not optimal under the current uncertainty but may lead to more rapidly decreasing the uncertainty in the future [@Walters1978]. In practice, many managers are reluctant to employ exploratory strategies, so passive adaptive management approaches such as as Management Strategy Evaluation (MSE) are more common [@Punt2016].


<!--
Any of these adaptive management approaches proves to be maladaptive under the simple example of model uncertainty considered here.  That is, the manager derives lower utility (here, economic yield), and the fish stock is also surpressed to a lower biomass than would be achieved if an identical management approach without any adaptive updating to model uncertainty were employed.  Put another way, the "value of information" gained from future observations of stock in response to any management action is actually negative.     

Owing to the greater complexity involved in many fisheries models, a typical MSE would evaluate a fixed set of possible strategies rather than solve a sequential decision problem [@Marescot2013] for the optimal sequence of actions, but the central premise of iteratively updating model posterior probabilities remains the same.  By limiting ourselves to the simpler models considered here, we are able to perform a passive adaptive management analysis of the full sequential decision problem, analgous to earlier examples [@Walters1981; @Smith1981; @Ludwig1982], which ensures the result is not an artifact of considering too narrow a set of possible policies (e.g. only constant-mortality policies). 

-->

Unfortunately, any adaptive management strategy which updates posterior distributions that characterize model undertainty leads to worse outcomes than an approach which does no updating.  I illustrate the application of a passive adaptive management strategy to this simple example, following classic examples for parameter [@Ludwig1982] or structural [@Smith1981] model uncertainty.  To demonstrate that the behavior is not driven by failure to explore sufficiently, (which might be addressed by an active adaptive management), I will assign initial probability that model 2 is true at 1%.  After a single iteration of learning, these probabilities are completely reversed, with the manager deciding that model 2 is almost certainly correct  [Fig 4]. As before, this results in a management practice with much worse ecological and economic outcomes than would have been realized by a manager who stubbornly clung to model 1 without updating [Fig 2, achieving a net present value over 100 time steps that only 31% that expected under management using Model 1 alone]. If instead of considering only these two models we consider a whole suite of Gordon-Schaefer-type models with varying $r$ and $K$ parameters, learning is slightly slower but no less counter-productive: mean stock size is $46%$ and net present value is $20%$ of what would be achieved under Model 1 (Appendix, Fig S1).  

```{r figure3, fig.cap="Adaptive management under model uncertainty, color indicates the belief that Model 2 is correct (red). Each model is assigned an initial prior belief. To ensure that any issues with passive exploration are not the cause of preferring Model 2, the initial belief in Model 2 is set to 1%.  Within a single iteration of adaptive management, the belief over models is updated to near certainty in Model 2, resulting in higher harvests and lower stock sizes similar to managing under Model 2 alone, Fig 2."}
am <- read_csv("../data/am.csv")
am %>%  filter(time <30) %>%
  ggplot(aes(time, states[state], col = belief)) + 
  geom_line() + 
  geom_point() +
  scale_colour_gradient(limits = c(0,1), low = pal[1], high = pal[4]) +
  ylab("stock size")
```


   


# Discussion
<!--
hit bigger themes first & objections first
- Learning can be counter-productive
- Value of information can be negative in practice

Objections:
- we already know simple models can be better, overfit models can lead us astray
- we already know Knightian uncertainty matters.  (underestimating uncertainty isn't the same thing)

- is this the result of underestimating the uncertainty?  

-->

Given this simplified version of model uncertainty in which one of the two models leads to effectively optimal ecological and economic outcomes, current approaches invariably choose the other. Importantly, a decision maker employing an adaptive management or iterative forecasting assessment such as those considered here would have no way of realizing that the outcomes they experienced were in fact sub-optimal.  The manager quickly concludes that Model 1 is entirely implausible, but also finds Model 2 to be remarkably accurate at predicting future values, and is entirely unaware that the model is leading to such poor outcomes. A more diligent manager may seek to improve Model 2 further by re-estimating parameters, but this in fact leads to a model with slightly lower $K$ and thus even higher level of over-fishing and under-performance than Model 2. The only way to avoid this trap is not to learn with each new observation, not to apply iterative updates over the models. A manager must follow the decisions proposed by model 1 for many iterations to realize that it leads to more desirable outcomes.  This raises several questions.  Do such methods of learning and reducing model uncertainty ever lead us towards worse long-term outcomes in the real world? How would we know? And what should we do about it?


It is tempting to argue that this situation would almost never arise in real world scenarios.  In this argument, Model 1 is a unicorn, a mythical model that somehow gives optimal performance while at the same time making predictions that are wildly wrong.  Could such unicorns really exist?  This is a difficult assertion to prove either way, but it is worth noting that neither of these features by themselves are especially rare. Examples of serious forecasts that widely miss future observations abound wherever forecasts are common, from elections to economics to environmental change [@].  Meanwhile, the premise that model need not perfectly capture reality to be useful is at the very heart of modeling.  Model 1 successfully captures the one key feature driving decisions in optimal harvest control problems: the stock size at which the maximum growth rate occurs [Fig 4a].  Capturing only the essential aspects as simply as possible is the goal of any model building exercise.  Moreover, rigorous updating of models and parameters through approaches such as adaptive management or iterative forecasting are not yet widespread practices in ecological management [@Dietze2018].  If such unicorn models exist, that lack of adoption would make it easier for them to persist at least until now.  

If these unicorns do exist in real world management, then what do we do about it? Our step may be to acknowledging that we do not yet have the answer.  There are at least three common ideas which are not the answer, which might succinctly be termed (1) don't care, (2) don't learn or (3) don't model.  

The first response, don't care, suggests that iterative updating is always desirable.  Previous work has acknowledged that future observations may be relatively uninformative, failing to reduce uncertainty between models by much.  This is the motivation for active over passive adaptive management, [@Walters1978], and has also acknowledge that even significant improvements in predictive accuracy may have negligible improvement on the quality of the decision.  This is the motivation for quantifying the value of information, [@Katz1987].  However, the conclusion from such observations is that additional information is at worst harmless to the quality of the decision, while in this example it is actively detrimental.  The realized 'value of information' contained in future measurements is negative.  

The

Why then do our best available approaches so readily reject the model?  

The reason for this may appear obvious once we compare both curves to that of the underlying model, Model 3.  Plotting the growth rate functions of each model, [Fig 4a], it is hardly surprising that no method which would prefer the closely overlapping Model 2 to the no-where-close Model 1 as the better approximation of Model 3. Nevertheless, decisions based on Model 1 are nearly indistinguishable from those based on the true model [Fig 4b], while Model 2 leads to over-harvesting.  We have no methods of reducing model uncertainty which do not perform significantly worse on this management problem than an approach which does not attempt to learn.  Put another way, the value of information contained in future observations [@] is always negative in this example. 





So why do our best approaches fail such a simple test case? It is tempting to dismiss this example as having little relevance to real-world decision problems on a variety of grounds:   (3) Don't we already know this?



(1) this problem is an artifact of overly-simplified models, (2) this problem is an artifact of an unrealistic optimal control solution, (3) the problem arises from having started with different objectives, (4) the problem is already solved. I address these in turn.



<!-- (3) THIS problem (4) THIS problem. I like that you outline the points you are going to cover! -->


```{r figure4,  fig.cap="Panel A: Population growth curves of each model. The positive equilibrium of each model occurs where the curve crosses the horizontal axis. Note that while Model 2 is a better approximation to the truth (Model 3), Model 1 better approximates the stock size which leads to maximum growth.", fig.width=8}
# Fig 4
model_curves <- read_csv("../data/model_curves.csv") %>% mutate(model = as.character(model))
policies <- read_csv("../data/policies.csv") %>% mutate(model = as.character(model))

plot_models <- 
  model_curves %>% 
  ggplot(aes(state, value, col=model, lty=model)) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_line(lwd=2, show.legend = FALSE) + 
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x")

plot_policies <- policies %>%
  ggplot(aes(states, escapement, col=model, lty=model)) + 
  geom_line(lwd=2) + xlab("state")


plot_models + labs(subtitle="A") +  plot_policies +  labs(subtitle="B")
```



This issue is not unique to sequential decision problems or optimal control solutions.  For example, Management Strategy Evaluation [@Punt2016] in fisheries seeks to evaluate pre-specified strategies rather solve for the best possible strategy through optimal control.  This approach is well justified -- optimal control techniques such as stochastic dynamic programming illustrated here quickly become intractable for more complex models [@Marescot2013] typically used in fisheries stock assessments.  Constraining the search to only strategies that impose a constant mortality (defined as harvest per unit biomass, $F = H/B$) means we only have to evaluate those $N$ strategies each iteration, not solve a sequential decision problem. Doing so, we would find the best constant-mortality solution under Model 1 performs much worse than the best constant-mortality solution under Model 2. Does this mean MSE is not suseptible to this issue?  No, it does not.  It is just as easy to construct an alternative Model 1 with the same properties of leading to nearly optimal decisions while being rejected by any method of iterative learning, such as forecasting or adaptive management.  Just as the optimal control policy for a Gordon-Schaefer (logistic) model depends only on parameter $K$ [@Reed1979], the optimum constant escapement policy depends only on parameter $r$ [@Schaefer1954]. If Model 1 has a value of $r$ such that it happens to match the best possible constant-mortality solution for the (unobserved) true model, then $K$ can be set arbitrarily high, ensuring the any model selection, forecasting, or adaptive management approach would lead away from Model 1 and towards worse-performing models once again. 


 
Is this result an artifact of the simple models considered here?  A more realistic approach would consider uncertainty over model parameters, as well as models with different structural forms, such as Beverton-Holt [@Beverton1957], Ricker [@Ricker1954], or Shepherd [@Shepherd1980].  Such complexity would not be sufficient to resolve the paradox of model uncertainty illustrated here. Simplicity facilitates intuition: Model 1 performs best because it correctly approximates the stock size which achieves maximum growth rate under the true model.  In contrast, each of those widely used recruitment relationships are symmetric, and their best-fitting parameter estimates will always have a maximum growth rate at a lower stock size. It is easy to point out that even this wider array of parameter and model values is too simple, since fisheries models in practice are much more complicated still (indeed, too complicated for the formal integration over model uncertainty considered here).  It is indeed true that either the forecasting or adaptive management example would work as expected if the set of candidate models included the true model, or at least a functional form that could better approximate the peak of the true model while also fitting better than model 2.  But we never have the true model in real systems.   If the candidate models are only caricatures, so too is the "true" model.  The candidate models considered here are no doubt much closer approximations to model 3 (all being single species, unstructured in age, stage, time or space, etc) than any model used in practice is to reality. Morever, a more accurate model is not required to reach optimal outcomes -- Model 1 already provides this by correctly approximating the key feature in this problem, even while it is wrong about much else.  How do proceed when our best methods fail in such a simple case? 

<!-- move to appendix?  too confusing/too long?

Not sure - it feels a little extraneous (I did think this was an interesting point brought up by reviewer 1 but I also don't necessarily think it warrants a long paragraph that might confuse the main point rather than clarify something) Could this be a couple sentences and reference an appendix that walks through what you write out (potentially even with a simple constant mortality example)
-->
Is this result an artifact of optimal control? The analysis has relied on optimal control theory to determine the optimal decision associated with each model or distribution of models.  The resulting policies are mathematically optimal under the assumptions given [see @Reed1979 for stochastic harvest problem in particular and @Marescot2013 for discussion of Markov Decision Problems more generally].  In most realistic decision problems, model complexity currently precludes the calculation of an optimal control solution at all ["the curse of dimensionality", e.g. @Polasky2011; @Marescot2013]. Under the assumptions of any of the models considered here, the optimal control solution leads to what is known as a "constant escapement" policy, which seeks to bring biomass to the target escapement level, $B_{MSY}$ as quickly as possible, $H_t = \min(X_t - B_{MSY}, 0)$.  If the assumptions are not met, a constant-escapement policy may not be optimal, as in the case when observations of the stock $X_t$ are not perfect [@pomdp-intro].  While constant escapement is used as a basis of management in some real world cases such as salmon fisheries, most marine fisheries are based on a target of "constant mortality", ($F_{MSY}$). Notably, a constant mortality policy, implemented as $H_t = F_{MSY} X_t$, would not perform better under Model 1 than Model 2.  Constant mortality is optimal only under deterministic growth at equilibrium (where the policy is the same under constant escapement).  Does that mean the result is an artifact of assuming constant escapement rather than constant mortality as the basis for management?  No, it does not.  It is just as easy to construct a model under which the constant mortality matches the constant mortality of the true model while being a poor predictor.  Moreover, using a non-optimal policy would always beg the question of whether the poor outcomes were merely the result of using a non-optimal policy rather than a consequence of model uncertainty.  Because the space of possible actions is smaller than that of possible models, we should expect most decision problems to have their own "Model 1," a model that would lead to correct actions while being a poor predictor of future outcomes.  The trouble is that we have no theory which would not reject such a model. 


Is this result merely an artifact of differing objectives?  @Gneiting2007's proper scoring rules are designed to select models which make the best forecast, and Model 2 really is the best choice for that task.  However, the adaptive management approach explicitly seeks to maximize expected utility, integrating over the uncertainty in the model, and yet it evolves away from Model 1 which would achieve that goal and settles on Model 2.  Note that this issue is not resolved by updating the learning step to reflect only the immediate reward -- it is in fact easy to see that so-called greedy algorithms would take the largest harvest possible and never reach the sustainable yield achieved by lower harvest rates of Model 1. To select Model 1 over alternatives like Model 2 which make better predictions but lead to worse outcomes requires a more long-term approach that forgoes iterative updating of probabilities as new data becomes available.  Note that this is the opposite recommendation of _adaptive_ management [@Walters1978] or _iterative_ forecasting [@Dietze2018].


<!-- I really like the last paragraph!! -->

Perhaps Model 1 is a unicorn, a mythically simple model that gives optimal results but has no analogue in the real world. This is not an easy assertion to prove either way.  Future observations consistently fall outside the 95% confidence intervals predicted under Model 1.  Unfortunately, such innaccurate predictions are only too common in the real world.  Have any of those led to effective policy despite the innaccuracy?  Have any been replaced with more accurate models that have also driven worse decisions? That is more difficult to answer.  What this example demonstrates is that if such unicorn models exist, our current paradigm will surely filter them out eventually, to be replaced with worse outcomes.


<!-- Omit this?  Does it beg an example? 

There is at least one technique which might address the problem presented here.  Recently developed techniques based on model-free deep reinforcement learning train multi-layer neural networks to approximate the optimal control policy without assuming or attempting to model the underlying dynamic process at all.  Feedback is based on cumulative reward, not short-term reward or predictive accuracy.  
-->

<!--I was thinking about exactly this while I was reading the paper.  I do think this would likely beg an example, which might be a can of worms that you don't want to open with this piece... I also don't think that I think it would address the presented problem. (Kind of repeating my second framing point but) It reminds me of the "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and use Interpretable Models Instead" paper where she talks about how, for a given data set, you can define the Rashomon set as the set of reasonably accurate predictive models-  Because the data are finite, the data could admit many close-to-optimal models that predict differently from each other: a large Rashomon set. I suspect this happens often in practice because sometimes many different ML algorithms perform similarly on the same data set, despite having different functional forms (for example, random forests, neural networks, support vector machines). As long as the Rashomon set contains a large enough set of models with diverse predictions, it probably contains functions that can be approximated well by simpler functions, and so the Rashomon set can also contain these simpler functions. Said another way, uncertainty arising from the data leads to a Rashomon set; a larger Rashomon set probably contains interpretable models, thus interpretable accurate models often exist. If this theory holds, we should expect to see interpretable models exist across domains. These interpretable models may be hard to find through optimization, but at least there is a reason we might expect that such models exist.

The problem is deeper than acknowleding a "rashamon set of models" or a plurality of reality (relevant to that Bioscience paper I sent last week about the Rashamon effect in conservation). Sometimes what appears closer to reality is actually fundementally wrong in some way that is misguiding to a decision maker...more opaque approaches could lessen our capacity to trouble shoot that?) -->




\pagebreak 


# References