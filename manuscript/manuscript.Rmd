---
title: 'When to ignore perfect information: how iterative learning can lead to worse decisions'
author:
- name: Carl Boettiger
  email: cboettig@berkeley.edu
  affiliation: a,1
date: "`r Sys.Date()`"

abstract: |
  Model-based forecasts have been enabled by recent explosion of data and computational methods and spurred on by decision-makers appetite for forecasts in everything from elections to pandemic response. Using a classic example from fisheries management, I demonstrate that selecting the model that produces the most accurate and precise forecast can lead to decidedly worse outcomes.

corresponding_author:
  code: 1
  text: "To whom correspondence should be addressed. E-mail: cboettig@berkeley.edu"

bibliography: refs.bib
address:
- code: a
  address: Department of Environmental Science, Policy, and Management, University of California, 130 Mulford Hall Berkeley, CA
    94720-3114, USA
    
keywords:
- forecasting,
- adaptive management,
- stochasticity,
- uncertainty,
- optimal control


## change to true to add optional line numbering
lineno: true
csl: ecology-letters.csl
output: rticles::elsevier_article
layout: 3p
journal: TBD
---


```{r, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"
```
```{r, include=FALSE}
rmarkdown::render("appendix.Rmd")
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, dev="cairo_pdf")
```


<!-- I think the introduction could have a clearer storyline and better introduce some of the ideas you bring up later on in the methods/results. A quick outline idea: 

  - Paragraph1: Model-based forecasts have become a cornerstone of applied science in disciplines as diverse as atmospheric science [weather?], economics [stock markets?], and ecology [dietz]. Seeking to inform a variety of decisions, near-term forecasting in these diverse fields has  simultaneously been encouraged by policy-makers [CITE?] and enabled by the rapid and recent increase in data availability and computational methods leading to a rapid expansion of thier use (or something...)
  
   - Paragraph 2: Introduce ecological decision making under uncertainty, rapid global change, etc. (kind of like your current opening bit)

  - Paragraph 3: But do better forecast always lead to better decisions?  Because model-based forecasts frequently play an important role in decision-making,it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty will also improve decision-making outcomes. 
            - While the literature has largely addressed issues of overfittting that might lead forecasts to misguide decisions, proper scoring methods blah blah [...introduce this concept]. 
            - Beyond overfitted models,... [maybe add something about Value of Information literature here?]
  
  - Paragraph 4: Your current paragraph on "I illustrate..." (I'd add into that paragraph something about the fact that you explore adaptive management?)

  
A second idea on framing: this maybe is a to divergent from the main point (or leads you into a post-modernist narrative that you'd prefer to avoid...) but it seems like you could reframe the whole thing as an argument for embracing an inclusive process of resource management that acknowledges pluralism.  
  (1) Policy makers often think "scientists" hold knowledge of "an environmental reality"
  (2) Despite this not being true (scientist disagree on a lot of things), even when "science" points to a more likely reality, that might be off in ways that can fundementally misguide decisions. 
  (3) The problem is deeper than acknowledging a "rashamon set" of models or multiple possible descriptions of of reality. Sometimes what appears closer to reality is actually off in a way that is critical to the decision.
-->

<!-- Open on real-world issues, uncertainty, particularly model uncertainty as key theme.  -->
Global change issues are complex and outcomes are difficult to predict  [@Clark2001]. 
To guide decisions in an uncertainty world, researchers and decision makers 
may consider a range of alternative plausible models to better reflect 
what we do and do not know about the processes involved [@Polasky2011].  Forecasts or
predictions from possible models can indicate what outcomes are most likely to result
under what decisions or actions. This has made model-based forecasts a cornerstone 
for scientifically based decision making.  
By comparing outcomes predicted by a model to future observations, 
a decision maker can not only _plan_ for the uncertainty, but also _learn_ which models are most trustworthy.  The value of iterative learning has long been reflected in the theory of 
adaptive management [@Walters1978] as well as in actual adaptive management practices such as 
Management Strategy Evaluation (MSE) [@Punt2016] used in fisheries, and is a central tenant
of a rapidly growing interest in ecological forecasting [@Dietze2018].
But, do iterative learning approaches always lead to better decisions?

In this paper, I will demonstrate that iterative learning or iterative forecasting can also be counter-productive to management outcomes:
that in some cases, a manager would be better to ignore new information which could reduce
uncertainty over available models. Put another way, the value of information (VOI, as
measured by the expected utility given that information minus the utility without it; see @Howard1966; @Katz1987), can actually be negative. This is not a consequence of imperfect information
(e.g. observation error) or the result of model over-fitting.  The issue is an intrisic 
consequence of the fact that many models can lead to the same decisions, and no generic
solution to the problem exists. However, as this example shows, by better understanding
what aspects of model are most and least important to shaping a particular decision, 
we can at least anticipate the circumstances that make this problem more likely.

Iterative learning approaches such as adaptive management or iterative forecasting are
particularly compelling because they address a different reason why seemingly-accurate
models can lead to poor outcomes: the problem of overfitting.  When models are estimated
from the data they seek to describe, the model may over-fit the data, making the data
seem even more likely than it would be under the true process [@Burnham1998].
Over-fit models may underestimate uncertainty and make poor predictions about future
outcomes [@Ginzburg2004]. Because any iterative learning compares models to future outcomes,
overfitting those outcomes is impossible.  This concept was rigorously formalized
by @Gneiting2007's proof of "proper" scoring rules. By definition, under a proper 
scoring rule, no probabilistic prediction $Q(x)$ can score better, on
average, than that of the underlying process, $P(x)$.  @Gneiting2007 further shows which
techniques for evaluating forecast predictions do an do not satisfy this condition, i.e. which
cannot be over-fit. Adaptive management is based on equally secure footing because
model predictions are again compared to future outcomes not used to fit the model.
Instead of scoring and selecting among alternative models, adaptive management 
considers probabilities over all models [@Walters1978].  These two approaches are
particularly compelling because they can both reduce uncertainty and avoid over-fitting.
My example is not meant to undercut the importance of these approaches.  Previous
work has long acknowledged the panelope of ways in which iterative learning or other
model-based decision making can go astray due to conflicting incentives,
implementation errors, or lack of resources for monitoring and updating [e.g. @Ludwig1993].
Here, the problem is more fundamental. 




<!-- Haven't really queued up the idea that I will present two distinct approaches that both fail.  need to more explicitly introduce and justify the two approaches-->


<!--

A wide range of paradigms are available for approaching the issue of decision-making under uncertainty. These approaches can roughly be divided into two groups: statistical approaches and adaptive management approaches.  The first group, "statistical approaches," seeks to maximize some statistical metric (such as goodness-of-fit or predictive accuracy) in determining which model(s) will drive decisions. In contrast, adaptive management approaches seek to select actions which maximize the decision-maker's expected net utility over 


In the first group, I focus on evaluation using "proper" scoring rules as defined by @Gneiting2007, rather than potentially more familiar model comparisons such as $r^2$, likelihood, or information criteria [@Burnham1998] because proper scoring rules are more robust  (see @Gneiting2007 for comparison and how the approaches relate under appropriate assumptions). @Gneiting2007 provides a rigorous proof of scoring rules that have the desirable property of being "proper", that is, rules for which no model predicting the distribution of future outcomes, $Q(x)$ can achieve a better average score than the true model $P(x)$. Unlike likelihood or other goodness-of-fit scores alone, it is impossible (in expectation) to overfit the proper score -- no model model can out-perform the true model. Note that strictly proper scoring rules score _probabilistic forecasts_ and not just point predictions, favoring models which accurately reflect the uncertainty over those which under-estimate it.  These features have made proper scoring rules for probabilistic forecasts a successful and popular approach for addressing model uncertainty in many other areas [@Gneiting2014; @Raftery2016] and a promising tool for evaluation of ecological forecasts [@Dietze2018]. 

For the second group, decision-theoretic approaches, I focus on the example of adaptive management
Decision-theoretic approaches include scenario analysis, resilience thinking, optimal control and related methods [@Polasky2011], which have a long history in ecology and particularly in fisheries management. One of the most influential of these is _adaptive management_ [sensu @Walters1978], in which a manager seeks to both reduce uncertainty over time while also achieving the best outcomes given current knowledge.  While the term today is frequently used in a looser sense, adaptive management as originally developed can be considered an example of optimal control under model uncertainty [@Walters1981; @Smith1981].  In this approach, a manager assigns probabilities to each of the possible models under consideration, i.e. models 1 & 2.  The expected utility of any action reflects both the intrinsic uncertainty over future states under either model (process uncertainty) and the manager's uncertainty over the choice of models (or parameter values). Unlike the forecast comparison, an adaptive management approach explicitly considers the utility function, Eqn \eqref{utility}, which it seeks to maximize by integrating over model uncertainty. After observing the consequences of the action, the manager updates the posterior probabilities that each model is correct, typically by Bayes rule [e.g. @Walters1981; @Smith1981; @Ludwig1982; @Punt2016]. The decision strategy is then re-evaluated in light of this revised uncertainty.  Adaptive management may be either passive or active. Under passive adaptive management, the decision-maker chooses the action which maximizes that expected future utility.  Under active adaptive management, the decision-maker may chose 'exploratory' actions which are not optimal under the current uncertainty but may lead to more rapidly decreasing the uncertainty in the future [@Walters1978]. In practice, many managers are reluctant to employ exploratory strategies, so passive adaptive management approaches such as as Management Strategy Evaluation (MSE) are more common [@Punt2016].

-->


I illustrate this problem using an example from fisheries management. 
Fisheries are a significant economic and conservation concern world
wide and their management remains an important debate [e.g. @Worm2006;
@Worm2009; @Costello2016]. Moreover, their management has been a proving 
grounds for theoretical and practical decision-making issues which are widely
applicable in other areas of ecology and conservation [@Ludwig1993; @Lande1994],
and one that has long wrestled with issues of uncertainty in the context
of management decisions [e.g. @Clark1973; @Reed1979; @Walters1981; @Ludwig1982].
While modern fisheries management frequently relies on complex models which may
contain scores of parameters to reflect the specific age or stage structure of a
specific fish stock [@ramlegacy2018; @ram], I will rely on simple, well-studied models which
permit greater intuition and generalization [@Levins1966]. Consistent with 
such previous work [@Schaefer1954; @Clark1973; @Reed1979; @Walters1981; @Ludwig1982; @Costello2016],
let us consider the problem of determining the optimal harvest policy given a 
measurement of the current stock size. 

For illustrative purposes, we will focus on the simplest example of model uncertainty,
considering two alternative models of equal complexity but differing in the estimated
value of certain parameters. While some authors distinguish between model uncertainty and parameter uncertainty, I will refer collectively to any uncertainty that arises from our imperfect knowledge of the system as "model uncertainty," because the distinction is largely a consequence of notation rather something more intrinsic.  (For example, two structurally different models, $f(x) = a x$ and $f(x) = a x^2$ can be considered merely different parameterizations of $f(x) = a x^b$, or vice versa.)  


# Ecological Models

The sustainable harvest decision problem can be stated as follows: The fish stock is observed to be in state $X_t$ at time $t$, and is then subjected to some harvest $H_t$ before recruiting new fish, subject to stochastic environmental noise $\xi_t$, to bring the stock to $X_t+1$,

\begin{equation}
X_{t+1} = f(X_t - H_t, \xi_t) 
\end{equation}

A manager seeks each year to select the harvest quota $H_t$ which will maximize the sum
of the utility derived from such a harvest and such a state, $U(X_t,H_t)$, over all time,
subject to discount rate $\delta$ [@Clark1973]:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t \label{utility}
\end{equation}

For simplicity and comparison with prior work, we will assume a fixed price of fish $p$ with no additional cost on additional harvest, $U(X_t, H_t) = p \min(H_t, X_t)$ (noting that realized harvest cannot exceed the stock size). Without loss of generality we will set the price $p = 1$ and modest discount $\delta = 0.99$.  

We further imagine that the function $f$ is not known precisely.  Again for simplicity, we restrict ourselves to two simple candidate models $f_1$ and $f_2$.  Both share the same underlying structure of logistic recruitment (known as the Gordon-Schaefer model in fisheries context owing to groundbreaking work independently by @Schaefer1954 and @Gordon1954), differing only in their choice of certain parameters:

\begin{equation}
f_i(Y) = Y + r_i Y \left( 1 - \frac{Y}{K_i} \right) * \xi_t(\sigma)
\end{equation}

For simplicity, we will assume $\xi_t(\sigma)$ represents log-normal random noise with mean of unity and log-standard-deviation $\sigma_i$ for each model. Model 1 is given by $r_1 = 2$, $K_1 = 16$, $\sigma_1 = 0.05$, Model 2 by $r_2 = 0.5$,  $K_2 = 10$, $\sigma = 0.075$ (in dimensionless units).  Having both the larger growth rate and the larger carrying capacity, Model 1 is clearly the more optimistic of the two choices. 

Selecting between Model 1 and Model 2 can thus be considered the simplest illustration of the model uncertainty problem. This is a subset of the more general problem of selecting model parameters, assuming a logistic growth, which itself is a subset of estimating the best structural from (e.g. Ricker, Beverton-Holt, etc).  There is no need to consider these more complicated versions of the model uncertainty problem here, since they all inherit the same issue.  Reducing the model selection problem to these two models simplifies the presentation and will aid intuition at no loss of generality.  

The only additional assumption we will need is that the "true" model is not among the suite of models under consideration.
Mathematical models are, at best, approximations of the underlying processes.  Ecological processes are much too complex to ever be modeled exactly.  For illustrative purposes, we will thus assume the "true" process to be given by Model 3, which is unknown to the decision-maker, but similar enough to at least one of the candidate models might be considered a reasonable approximation:

\begin{equation}
f_3(Y) = Y + r_3 Y^4 \left( 1 - \frac{Y}{K_3} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$. 


The task of deciding whether Model 1 or Model 2 would be the better choice for decision making is 
thus perhaps the simplest example of the much studied issue of model uncertainty that we can pose. 
As in any real world scenario, neither model is the true model, but nevertheless this model set
contains a good enough approximation of the true model to make good decisions.  However, any of
the well-developed approaches for decision-making under model uncertainty will prefer Model 2 over
Model 1, despite the fact that the optimal policy under Model 2 leads to much worse outcomes
ecologically and economically.  


# Methods for Managing Under Model Uncertainty 

We will use this example to illustrate two alternative approaches for iterative learning over model uncertainty: iterative forecasting and adaptive management.  The central difference in the approaches
is that iterative forecasting is premised on the ability to score the predictions of alternative models.
Iterative forecasting is silent on the issue of what to do with those scores, this is left up to the 
decision-maker.  Adaptive management approaches, by contrast, explicitly seek to integrate probabilities over all candidate models to reach a decision.  I consider each in turn.

## Statistical approaches: Forecasting under "Proper" Scoring Rules

Like many decision problems, the task of setting a sustainable harvest quota appears to hinge on having an accurate forecast: if we can predict to what size the fish stock will increase next year, $X_t+1$, and we know the current stock, $X_t$, then we can sustainably harvest $X_{t+1} - X_t$ without decreasing the biomass over the long term.  Selecting a model based on forecast skill is also justifiable on theoretical grounds, since it reduces the risk of over-fitting by comparing model predictions to later observations that were not used to estimate the model [@Gneiting2014].

I illustrate the process of model selection by strictly proper scoring rules using two scenarios. In Scenario A (passive observation) the fish stock is unharvested and allowed to recover towards carrying capacity (as simulated under our "true" model, Model 3) while comparing the observed stock size in each subsequent time step to the distribution predicted under Model 1 and Model 2 respectively [Fig 1].  The mean, $\mu_t$ and variance, $\sigma_t$ of the forecast are compared against the true observation $x_t$ using a Proper scoring rule of @Gneiting2007, 

\begin{equation}
S(x_t|\mu_t,\sigma_t) = -(\mu_t - x_t )^2 / \sigma_t^2  - \log(\sigma_t) \label{proper}
\end{equation}

for each prediction over 100 replicate simulations of 100 time steps each [Fig 1].



```{r figure1, fig.width=7, fig.height=5, fig.cap = "Forecast performance of each model.  Panels A, B: Step ahead predictions of stock size under unfished (A) and fished (B) scenarios. Error bars indicating the 95% confidence intervals around each prediction, while stars denote the observed value in that year.  Because the models make different decisions each year in the fished scenario, the observed stock size in year 2, 3, etc under the management of model 1 (blue stars) is different from that under model 2 (red stars).  Panels C, D: corresponding distribution of proper scores across all predictions (100 replicates of 100 timesteps). Higher scores are better, confirming that model 2 makes the better forecasts."}
predictions <- read_csv("../data/predictions.csv") %>% mutate(model = as.character(model))

fig1cd <- predictions %>%
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 2, show.legend = FALSE) +
  coord_cartesian(xlim = c(-100, 1), ylim = c(0,4000)) +
  xlab("Proper score") +
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "C.",
    B_fished = "D."))) + 
  scale_x_continuous(breaks = c(-100, 0)) +
  theme(axis.text.y = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )



 fig1ab <- predictions %>% 
  filter(reps == "2", time < 10) %>%
  ggplot(aes(time, col = model, fill = model)) + 
  geom_point(aes(y = expected)) + 
  geom_errorbar(aes(ymin = low, ymax = high)) +
  geom_point(aes(y = true), pch = "*", size = 12, 
             alpha = 0.6) + 
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "A. Without fishing",
    B_fished = "B. Managed harvest"))) +
  ylab("stock size")
 
 
 
 fig1ab + fig1cd + 
  plot_layout(widths =  c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```


In Scenario B (actively harvest), I have first solved for the optimal management strategy for both Model 1 and Model 2 [Fig 1b]. For small noise and concave functions with linear reward structure this can be done analytically [see proof in @Reed1979], or solved more generally by stochastic dynamic programming [see review by @Marescot2013, details in Appendix].  Under this scenario, replicate simulations of the stock are harvested at each time step using the optimal quota dictated by either Model 1 and Model 2. The resulting stock sizes in the time-step following this harvest are once again compared to the probabilities predicted by each model using Eq \eqref{proper}. Model 2 unequivocally outperforms Model 1 in both scenarios of passive observation and active harvest.

Despite the clearly superior predictive accuracy of Model 2 in both scenarios, the outcomes from management under Model 2 are substantially worse.  We can assess such outcomes in less abstract terms than forecasting skill, such as economic value (in dollars) or the ecological value (unharvested biomass). In our simple formulation of the decision problem, the "utility" the manager seeks to maximize is simply the economic value (net present value: the discounted sum of all profits from future harvests, Eq \eqref{utility}) of harvested fish.  This formulation ignores any utility provided by fish that are not harvested, beyond their contribution to future potential harvests. While it is possible to include such contributions directly in the utility function being optimized [e.g. @Halpern2013], even without doing so, Model 1 maintains both a higher unharvested biomass and also leads to higher economic returns throughout [Fig 2].  


```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast. Harvest quotas derived from Model 1 result in a significantly higher fish stock size than under Model 2 (panel A). Economic returns under Model 1 are also substantially higher (panel B)"}

sims <- read_csv("../data/sims.csv")
npv_df <- read_csv("../data/npv_df.csv")


plot_ecology <- 
  sims %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  group_by(model, time) %>%
  summarise(mean_state = mean(state), sd = sd(state), .groups = "drop") %>%
  filter(time < 25, model != "3") %>%
  ggplot(aes(time, mean_state)) + 
  geom_line(aes(col = model), lwd=1.5, show.legend = FALSE) +
    geom_ribbon(aes(ymin = mean_state - 2*sd, 
                    ymax = mean_state + 2*sd,
                    fill = model),
                alpha = 0.2, show.legend = FALSE) +
  ylab("state") + 
  labs(subtitle = "A. Ecological outcomes")

optimal <- select(filter(npv_df, model == "3"), time, mean_npv)

plot_econ <- 
npv_df %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  filter(model != "3", time %in% seq(1,100, by = 5)) %>%
  ggplot(aes(time, mean_npv)) +
  geom_line(data = optimal, lwd = 1.5, col = "grey20") +
  geom_point(aes(col=model), size = 4, alpha = 0.8) + 
  ylab("Net present value") + xlab("time")  + 
  labs(subtitle = "B. Economic outcomes")


plot_ecology + plot_econ
```

Had we been able to include Model 3 in our forecast comparisons, it would equal or outperform the forecasting skill of both Model 1 and Model 2 (as guaranteed by the theorem of @Gneiting2007), while also matching or out-performing their economic utility (as guaranteed by the theorem of @Reed1979).  In practice, we never have access to the generating model. 

One obvious limitation in this comparison is that scenario B treats each model as fixed over the entire course of the simulation. It would be possible to generate forecasts for the next $n$ steps, but these forecasts would also be conditional on the management action (i.e. harvest quota) selected at each step.  To evaluate two-step-ahead predictions we must consider each possible action under each possible state predicted by the step-ahead forecast, weighted by probability of the state given the model and the probability of the model.  This rapidly expanding set of possibilities is addressed by adaptive management for sequential decision problems [e.g. @Smith1981], which I employ in the next section.  

<!-- replacing step-ahead with n-step ahead would not matter-->

## Decision-Theoretic Approaches

<!--
Any of these adaptive management approaches proves to be maladaptive under the simple example of model uncertainty considered here.  That is, the manager derives lower utility (here, economic yield), and the fish stock is also surpressed to a lower biomass than would be achieved if an identical management approach without any adaptive updating to model uncertainty were employed.  Put another way, the "value of information" gained from future observations of stock in response to any management action is actually negative.     

Owing to the greater complexity involved in many fisheries models, a typical MSE would evaluate a fixed set of possible strategies rather than solve a sequential decision problem [@Marescot2013] for the optimal sequence of actions, but the central premise of iteratively updating model posterior probabilities remains the same.  By limiting ourselves to the simpler models considered here, we are able to perform a passive adaptive management analysis of the full sequential decision problem, analgous to earlier examples [@Walters1981; @Smith1981; @Ludwig1982], which ensures the result is not an artifact of considering too narrow a set of possible policies (e.g. only constant-mortality policies). 

-->

Any adaptive management strategy updates posterior distributions over model uncertainty [@Punt2016; @Ludwig1982]. Unfortunately, any such adaptive updating leads to worse outcomes than the equivalent non-adaptive strategy, in which model uncertainty is held fixed.  I illustrate the application of a passive adaptive management strategy to this simple example, following classic examples for parameter [@Ludwig1982] or structural [@Smith1981] model uncertainty. Passive adaptive management for a simple sequential decision problem is straight forward to implement over a discrete set of states and actions using dynamic programming with iterative updates [@Smith1981, example code in Appendix]. To demonstrate that the behavior is not driven by failure to explore sufficiently, (which might be addressed by an active adaptive management), I will assign initial probability that model 2 is true at 1%.  After a single iteration of learning, these probabilities are completely reversed, with the manager deciding that model 2 is almost certainly correct  [Fig 3]. As before, this results in a management practice with much worse ecological and economic outcomes than would have been realized by a manager who stubbornly clung to model 1 without updating, which achieves a net present value over 100 time steps that only 31% that expected under management using Model 1 alone [Fig 2]. 

```{r figure3, fig.cap="Adaptive management under model uncertainty, color indicating the belief that Model 1 is correct (blue), lines indicating stock biomass over time, green triangles indicating harvest quotas set by adaptive management. Panel A: Each model is assigned an initial prior belief. To ensure that any issues with passive exploration are not the cause of preferring Model 2, the initial belief in Model 2 is set to 1%.  Within a single iteration of adaptive management, the belief over models is updated to near certainty in Model 2, resulting in higher harvests and lower stock sizes similar to managing under Model 2 alone, Fig 2A. Panel B: given 36 candidate models over a broad range of parameters, adaptive management leads to models that overharvest even more, leading to even lower stock sizes and lower harvest quotas."}
am <- read_csv("../data/am.csv")
fig3a <- am %>%  filter(time <=20) %>%
  ggplot(aes(time, stock, col = belief)) + 
  geom_line(show.legend = FALSE) + 
  geom_point(show.legend = FALSE) +
  geom_point(aes(time, quota), col="darkgreen", shape=2) + 
  scale_colour_gradient(limits = c(0,1), low = pal[4], high = pal[1]) +
  ylab("stock size") + labs(subtitle="A. Two models") + ylim(c(0,11))

am_multi <- read_csv("../data/am_multi.csv")
fig3b <- am_multi %>%
  ggplot(aes(time, stock, col = belief)) + 
  geom_line() + 
  geom_point() +  
  geom_point(aes(time, quota), col="darkgreen", shape=2) + 
  scale_colour_gradient(limits = c(0,1), low = pal[4], high = pal[1]) + 
  ylab("fish stock") + labs(subtitle="B. 36 models")+ ylim(c(0,11))

fig3a + fig3b
```


Increasing the space of possible models to cover a whole plausible range of parameters $r$ and $K$ does nothing to resolve this problem.  Learning is somewhat slower over the larger range of possibilities, but nevertheless converges towards parameter values with lower growth rates and lower carrying capacity, for which the target biomass is far lower [Fig 3B; details in appendix B]. The manager becomes trapped in vicious cycle as biomass declines further and further, subsequent observations seem more and more consistent with models which most favor over-fishing in the first place.  As biomass declines, annual harvests and profits also plunge, and yet our manager becomes only more confident in their decisions. 



# Discussion

<!-- 
First: Intuition for the result.
  - See that model 1 captures the key feature, while model 2 is just 'closer to the curve'
  - See that this is a generic truth about decision-making: there's always a Model 1
  - This is not about over-fitting.
-->

<!-- quick summary of what was demonstrated -->
Given this simple decision problem in which one of the two models leads to effectively optimal ecological and economic outcomes, current approaches invariably choose the other. Moreover, a decision maker employing an adaptive management or iterative forecasting assessment such as those considered here would have no way of realizing that the outcomes they experienced were in fact sub-optimal.  In both approaches, the manager quickly concludes that Model 1 is entirely implausible, while finding that Model 2 is remarkably accurate at predicting future values. This problem is not addressed by re-estimating parameters (equivalently, considering a larger suite of models with all possible $K$ and $r$ values): in fact leads to a model with slightly lower $K$ and even higher level of over-fishing and under-performance than Model 2 (Fig 3B). 

<!-- Intuition, Figure 4 -->
The reason for Model 1's seemingly contradictory ability to make good decisions but bad forecasts becomes obvious once we compare both curves to that of the underlying model, Model 3.  Plotting the growth rate functions of each model, [Fig 4A], it is hardly surprising that no method exists which would not prefer the closely overlapping Model 2 to the no-where-close Model 1 as the better approximation of Model 3. However, Nevertheless, decisions based on Model 1 are nearly indistinguishable from those based on the true model [Fig 4B], while Model 2 leads to over-harvesting.  The explanation comes from noticing that the stock size corresponding to the maximum growth rate under Model 1 (the peak of the curve) falls at almost exactly the same stock size as that of the peak growth rate for Model 3.  Meanwhile, the peak of Model 2 occurs at a substantially lower stock size.  While the optimal control solution appears to depend only on a step-ahead forecast accuracy (indeed, the SDP solution method used here takes only step-ahead forecast probabilities as input, [@Marescot2013]), mathematical analysis showed long ago [@Reed1979] that the optimal solution for this problem depends only on keeping the biomass at the value responsible for the maximum growth rate.  This realization is quite general: for most decision problems, simple models will exist under which the optimal decision is the same as it would be for the true model, even when that simple model is wrong in most other ways.  

<!-- Intuition for it's not just this example -->
This phenomenon is not unique to sequential decision problems or optimal control solutions.  For example, Management Strategy Evaluation [@Punt2016] in fisheries seeks to evaluate pre-specified strategies rather solve for the best possible strategy through optimal control.  This approach is well justified -- optimal control techniques such as stochastic dynamic programming illustrated here quickly become intractable for more complex models [@Marescot2013] typically used in fisheries stock assessments.  Constraining the search to only strategies that impose a constant mortality (defined as harvest per unit biomass, $F = H/B$) means we only have to evaluate those $N$ strategies each iteration, not solve a sequential decision problem. Doing so, we would find the best constant-mortality solution under Model 1 performs much worse than the best constant-mortality solution under Model 2. Does this mean MSE is not susceptible to this issue?  No, it does not.  It is just as easy to construct an alternative Model 1 with the same properties of leading to nearly optimal decisions while being rejected by any method of iterative learning, such as forecasting or adaptive management.  Just as the optimal control policy for a Gordon-Schaefer (logistic) model depends only on parameter $K$ [@Reed1979], the optimum constant escapement policy depends only on parameter $r$ [@Schaefer1954]. If Model 1 has a value of $r$ such that it happens to match the best possible constant-mortality solution for the (unobserved) true model, then $K$ can be set arbitrarily high, ensuring the any model selection, forecasting, or adaptive management approach would lead away from Model 1 and towards worse-performing models once again. Many optimization problems share this feature in which the optimal policy depends only on a subset or ratio of model parameters, such that it is usually easy to find a similar Model 1. This example also illustrates the importance of decision constraints -- a model that gives very good outcomes when we are free to vary the havest quota each year (the optimal control problem here) may give very poor results under the constraint of constant mortality, and vice versa. 



```{r figure5,  fig.cap="Panel A: Population growth curves of each model. The positive equilibrium of each model occurs where the curve crosses the horizontal axis. Note that while Model 2 is a better approximation to the truth (Model 3), Model 1 better approximates the stock size which leads to maximum growth.  Panel B: The optimal control policy under Model 1 is nearly identical to that under the true Model 3, while the optimal policy under Model 2 supresses stock to a much lower escapement level.", fig.width=8}
# Fig 5
model_curves <- read_csv("../data/model_curves.csv") %>% mutate(model = as.character(model))
policies <- read_csv("../data/policies.csv") %>% mutate(model = as.character(model))

plot_models <- 
  model_curves %>% 
  ggplot(aes(state, value, col=model, lty=model)) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_line(lwd=2, show.legend = FALSE) + 
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x")

plot_policies <- policies %>%
  ggplot(aes(states, escapement, col=model, lty=model)) + 
  geom_line(lwd=2) + xlab("state")


plot_models + labs(subtitle="A") +  plot_policies +  labs(subtitle="B")
```


<!-- Not just overfitting -->
Note that the mechanism shown here has nothing to do with the much more familiar issue of over-fitting, in which a better-fitting model will also lead to worse outcomes [@Burnham1998].  In fact, an model which has been over-fit will gradually be rejected by either the iterative forecasting or adaptive management approaches shown here, as these approaches continually confront the models with new data to which they had not been previously fit.  The ability to avoid over-fitting is one of the greatest appeals of any iterative management strategy. In the example here, both Model 1 and Model 2 have the same structural complexity.  The better decision performance of Model 1 is a consequence of capturing the key attribute needed for a good decision, which is perhaps a surprisingly an all-together different criteria than fit. Note that other methods of model selection not considered here, including goodness-of-fit metrics such as $r^2$  or information criteria [@Burnham1998], will all prefer Model 2 over Model 1 for the same reason.  

<!-- Not just "get more models" -->
This case cannot be dismissed merely as being dealt a bad hand in having to pick only between Model 1 and Model 2. Model 1 approximates the key feature, giving nearly optimal outcomes.  Previous literature has underscored the importance of Knightian "unknown unknowns" two alternative models examined here fail to fully reflect our uncertainty in the underlying dynamics, and the consequences of underestimating model uncertainty are well understood [@Polasky2011; @Wintle2010]. Had we included the true model in the set of possibilities, the techniques illustrated would have had little difficulty in distinguishing it from Models 1 and 2 after sufficient iterations. In practice, we never have the true model, and even a much more thorough suite of candidate models, such as considering any possible values of $r$, $K$ [Fig 3B], or even among common alternative models with different structural forms, such as Beverton-Holt [@Beverton1957], Ricker [@Ricker1954], or Shepherd [@Shepherd1980], would fail to do any better, since all these models still have symmetric growth rates with a peak growth rate at half the steady-state population size).  Models will always be simpler than reality.  To insist that this issue can and will be avoided by always including more and better models of the process misses the point. 


<!--
So what?
- This is different than previous work: e.g. VOI is not zero, it's negative here.
- Model 1 is not a unicorn: we cannot rule out these situations in real world (particularly ecology where we don't actually do much iterative updating in practice)
- 
-->

Previous work has acknowledged that iterative learning may not always be feasible and that it may not always be beneficial, but has failed to recognize the possibility that it can potentially be detrimental.  For example, active adaptive management is premised on the observation that future observations may be too uninformative to distinguish between alternatives [@Walters1978].  Others have also noted that even big improvements in predictive accuracy could have negligible improvement on the decision outcomes, which motivates quantifying the Value of Information (VOI) [@Katz1987]. But in both cases, the worse outcome is wasted effort.  In the scenario considered here, the VOI is negative: any learning over the proposed model uncertainty leads to lower expected net utility than not learning.  


The only way to avoid this trap is to _forego learning_ with each new observation, to avoid iterative updates.  It may seem that the adaptive management approach fails because posterior probabilities are updated according to Bayes rule. Like iterative forecasting, this favors models which better predict the data, despite the fact that under the adaptive management approach, the overall optimization is conditioned on actual utility and not model fit.  However, this issue is not easily avoided.  For example, so-called greedy optimization techniques that favor actions with higher immediate reward do even worse in this context, since such an algorithm would obviously maximize the immediate harvest and therefore collapse the stock.  Only by comparing the net utility derived from management under model 1 for many iterations to the net utility derived under model 2 after many iterations can we deterime that model 1 leads to better outcomes.  This raises several questions.  Do such methods of learning and reducing model uncertainty ever lead us towards worse long-term outcomes in the real world? If so, How would we know, and what should we do about it?

<!-- **Is it just a unicorn?** -->
Do models which give nearly optimal performance while at the same time making wildly wrong predictions really exist?  It is tempting to argue that Model 1 is a kind of unicorn, a mythical creature existing only in theory. But that is a difficult assertion to prove.  Examples of serious forecasts that widely fail to predict future observations abound wherever forecasts are common, from elections to economics to environmental change [@].  If there is no shortage of bad forecasts, then could any of them be useful?  The premise that model need not perfectly capture reality to be useful is at the very heart of modeling.  Model 1 successfully captures the one key feature driving decisions in optimal harvest control problems: the stock size at which the maximum growth rate occurs [Fig 5A].  Capturing only the essential aspects as simply as possible is the goal of any model building exercise.  Thus it should be no surprise that a model can drive good decisions while making poor predictions.  It is also important to note that in many cases, any models being used in decision-making are not necessarily being subjected to the rigorous evaluation and updating steps proposed by adaptive management [@Walters1978] or iterative forecasting [@Dietze2018].  This makes it more likely that such models could persist in practice until now. While iterations that revise this models will no doubt improve decision outcomes in many case, it is worth bearing in mind from the example here that such a connection is not guaranteed.  

<!-- So what? -->
If these unicorns do exist in real world management, then what do we do about them? I believe this is an open question, but that our first step must be to recognize it as such.  We have seen that the problem cannot be resolved by more data, and is not the result of overfitting. Nor is "creating more models" the answer: when we have a model that is good enough to get optimal results, we cannot always insist on more models. We have also seen how not updating our uncertainty estimates, or doing so less frequently, can reveal a unicorn model before it is discounted by further iterations. Approaches such as iterative forecasting or adaptive management that can reduce model uncertainty over time remain promising and important techniques, but because value of information can be negative, we must revise model uncertainty only cautiously.  More importantly, we have seen that  the intuition offered by decision theory [Fig 5] can help us better understand what features of a model are essential to decision outcomes and what are not.  Conversely, models which appear to lead to accurate predictions (like Model 2) can result in outcomes that are far from optimal. Building on such understanding, it may possible to identify strategies for learning that are more agnostic to the details of the model that are not important, or perhaps not reliant on model-based predictions at all. How that is done is an open question.  

<!-- Model free management: what would it look like? -->
<!-- For example, a simple approach could gradually increase harvest from 0 as long as the observed recruitment increases relative to previous values.  This corresponds to a local search for the peak growth-rate from above. This strategy is optimal for the model considered here, at least under small noise, but is not robust to growth rates with local optima, nor is it optimal during a possibly-very-long transient period of the search.   -->


<!-- Correlary: Neither accurate forecasting nor effective outcomes are sufficient grounds to assert model validity. (meh, that's just 'all models are wrong, some are useful' for certain things)   -->


## Acknowledgements

The author acknowledges support from NSF CAREER Award #1942280 and helpful discussions with Melissa Chapman and Jeremy Fox.

 
<!--
Is this result merely an artifact of differing objectives?  @Gneiting2007's proper scoring rules are designed to select models which make the best forecast, and Model 2 really is the best choice for that task.  However, the adaptive management approach explicitly seeks to maximize expected utility, integrating over the uncertainty in the model, and yet it evolves away from Model 1 which would achieve that goal and settles on Model 2.  Note that this issue is not resolved by updating the learning step to reflect only the immediate reward -- it is in fact easy to see that so-called greedy algorithms would take the largest harvest possible and never reach the sustainable yield achieved by lower harvest rates of Model 1. To select Model 1 over alternatives like Model 2 which make better predictions but lead to worse outcomes requires a more long-term approach that forgoes iterative updating of probabilities as new data becomes available.  Note that this is the opposite recommendation of _adaptive_ management [@Walters1978] or _iterative_ forecasting [@Dietze2018].
-->


<!-- 

The open problem is simply to propose an approach to model uncertainty which would select model 1
over model 2 (i.e. choosing among the suite of available models, the one that gives the better 
decisions.)


-->



\pagebreak 


# References