---
title: 'Learning to be wrong:'
author:
- name: Carl Boettiger
  email: cboettig@berkeley.edu
  affiliation: a,1
date: "`r Sys.Date()`"

abstract: |
  Model-based forecasts have been enabled by recent explosion of data and computational methods and spurred on by decision-makers appetite for forecasts in everything from elections to pandemic response. It is taken for granted that the model which makes the most accurate forecast, accounting for uncertainty, will also be the best model to inform decision-making.  Using a classic example from fisheries management, I demonstrate that selecting the model that produces the most accurate and precise forecast can lead to decidedly worse outcomes.

corresponding_author:
  code: 1
  text: "To whom correspondence should be addressed. E-mail: cboettig@berkeley.edu"

bibliography: refs.bib
address:
- code: a
  address: Department of Environmental Science, Policy, and Management, University of California, 130 Mulford Hall Berkeley, CA
    94720-3114, USA
    
keywords:
- transients,
- optimal control,
- adaptive management,
- stochasticity,
- uncertainty,
- ecological management

# For footer text
lead_author_surname: Boettiger

significance: |


## change to true to add optional line numbering
lineno: true

output: rticles::pnas_article
---




```{r, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"
```
```{r, include=FALSE}
rmarkdown::render("appendix.Rmd")
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE, dev="cairo_pdf")
```


Global change issues are complex and outcomes are difficult to predict, making approaches to uncertainty a central part
of effective decision-making [@ @]. While some uncertainty is intrinsic to the underlying processes (stochasticity) or by
limits to what variables we can observe and how accurately we measure them (measurement uncertainty), much uncertainty 
is the result of our imperfect knowledge of the processes involved, as expressed by the structure and
parameters of mathematical models used to approximate those processes (model uncertainty). Unlike the other forms of
uncertainty, model uncertainty can be 
reduced by gathering additional data, which may rule out certain models or parameter values as implausible[^1]. 
When model parameters are estimated directly from available data, there is a risk that the best-fitting models may 
_overfit_ to patterns arising from by chance from the stochasticity or measurement error which do not reflect the
underlying process. Researchers have often favored simpler models which are less prone to over-fitting, sometimes
through explicit penalties such as information criteria [@], though these can be misleading [@]. With the rapid 
expansion of available ecological and environmental data [@], it is increasingly possible to rely instead on comparisons
between model predictions and data purposely excluded from model estimation (cross-validation), or better, 
comparisons between model predictions and data collected in the future (forecasting, @Clark2001; @Dietze2018]). 
Over sufficiently long timescales, comparing model forecasts to future observations will select the models with
greatest predictive accuracy.  Because model-based forecasts frequently play an important role in decision-making,
it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty
will also improve decision-making outcomes. Here, I illustrate that this need not be the case: it is quiet possible
that any approach with improves the forecast accuracy of the model(s) over time can simultaneously lead to worse 
decision outcomes. This example illustrates that knowing when increased forecast accuracy will or will not also improve 
decision outcomes in general remains an open problem, as is the challenge of what to do about it.

[^1]: While some authors distinguish between model uncertainty and parameter uncertainty, I will refer collectively to
any uncertainty that arises from our imperfect knowledge of the system as "model uncertainty". Uncertainty in model structure can often be reflected by appropriate parameterization, just as models with different parameter values can be treated distinctly. Whereas stochasticity is inherent to the process (or rather, to our choice of state variables, @Boettiger2018) and measurement uncertainty can only be reduced by more accurate tools for measurement, model uncertainty is reducible by learning from additional data. 


<!-- 
It may be easy to dismiss this example on the argument that both models are (or more precisely,
the any logistic growth model, regardless of parameterization would be) extremely naive
and unrealistic, and any solution would surely begin with replacing both models with something
more realistic.  Yet such objections overlook the simple fact that for all its faults in 
prediction, model 1 already provides nearly optimal performance.  That performance arises
because model 1 has very accurately reflected one key aspect of the true model (the position of the
maximum growth rate), even while it is wrong about every other aspect of the biology. 

The open problem is simply to propose an approach to model uncertainty which would select model 1
over model 2 (i.e. choosing among the suite of available models, the one that gives the better 
decisions.)


An important aspect of this example is that model 1 performs nearly optimally. 


-->

I illustrate this problem using an example from fisheries management. 
Fisheries are a significant economic and conservation concern world
wide and their management remains an important debate [e.g. @Worm2006;
@Worm2009; @Costello2016]. Moreover, their management has been a proving 
grounds for theoretical and practical decision-making issues which are widely
applicable in other areas of ecology and conservation [@Ludwig1993; @Lande1994],
and one that has long wrestled with issues of uncertainty in the context
of management decisions [e.g. @Clark1973; @Reed1979; @Walters1981; @Ludwig1982].
While modern fisheries management frequently relies on complex models which may
contain scores of parameters to reflect the specific age or stage structure of a
specific fish stock [@ram], I will rely on simple, well-studied models which
permit greater intuition and generalization [@Levins1966]. Consistent with 
such previous work [@Schaefer1954; @Clark1973; @Reed1979; @Walters1981; @Ludwig1982; @Costello2016],
let us consider the problem of determining the optimal harvest policy given a 
measurement of the current stock size.  

# Ecological Models

The sustainable harvest decision problem can be stated as follows: The fish stock is observed to be in state $X_t$ at time $t$, and is then subjected to some harvest $H_t$ before recruiting new fish, subject to stochastic environmental noise $\xi_t$, to bring the stock to $X_t+1$,

\begin{equation}
X_{t+1} = f(X_t - H_t, \xi_t) 
\end{equation}

A manager seeks each year to select the harvest $H_t$ which will maximize the sum
of the utility derived from such a harvest and such a state, $U(X_t,H_t)$, over all time,
subject to discount rate $\delta$ [@Clark1973]:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t \label{utility}
\end{equation}


Further we imagine that the function $f$ is not known precisely, and so we will rely on an evaluation of forecasting skill across a set of candidate models to determine which one to use to manage the fishery.  Again for simplicity, we will restrict ourselves to two simple candidate models $f_1$ and $f_2$.  Both share the same underlying structure of logistic recruitment (known as the Gordon-Schaefer model in fisheries context owing to groundbreaking work independently by @Schafer1954 and @Gordon1954), differing only in their choice of certain parameters:

\begin{equation}
f_i(Y) = Y + r_i Y \left( 1 - \frac{Y}{K_i} \right) * \xi_t(\sigma)
\end{equation}

Where $\xi_t(\sigma)$ represents log-normal random noise with mean of unity and log-standard-deviation $\sigma$.
Model 1 is given by $r_1 = 2$, $K_1 = 16$, $\sigma_1 = 0.05$, Model 2 by $r_2 = 0.5$,  $K_2 = 10$, $\sigma = 0.075$ (in dimensionless units).  Having both the larger growth rate and the larger carrying capacity, Model 1 is clearly the more optimistic of the two choices. 

Selecting between Model 1 and Model 2 can thus be considered the simplest illustration of the model uncertainty problem. This is a subset of the more general problem of selecting model parameters, assuming a logistic growth, which itself is a subset of estimating the best structural from (e.g. Ricker, Beverton-Holt, etc).  There is no need to consider these more complicated versions of the model uncertainty problem here, since they all inherit the same issue.  Reducing the model selection problem to these two models simplifies the presentation and will aid intuition at no loss of generality.  

The only additional assumption we will need is that the "true" model is not among the suite of models under consideration. 
Mathematical models are, at best, approximations of the underlying processes.  Ecological processes are much too complex to ever be modeled exactly.  For illustrative purposes, we will thus assume the "true" process to be given by Model 3, which is unknown to the decision-maker, but similar enough to at least one of the candidate models might be considered a reasonable approximation:

\begin{equation}
f_3(Y) = Y + r_3 Y^4 \left( 1 - \frac{Y}{K_3} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$. 


The task of deciding whether Model 1 or Model 2 would be the better choice for decision making is 
thus perhaps the simplest example of the much studied issue of model uncertainty that we can pose. 
As in any real world scenario, neither model is the true model, but nevertheless this model set
contains a good enough approximation of the true model to make good decisions.  However, any of
the well-developed approaches for decision-making under model uncertainty will prefer Model 2 over
Model 1, despite the fact that the optimal policy under Model 2 leads to much worse outcomes
ecologically and economically.  


# Methods for Managing Under Model Uncertainty 

A wide range of paradigms are available for approaching the issue of decision-making under uncertainty. These approaches can roughly be divided into two groups: the first group treats the issue of model uncertainty independently from the decision itself, while the second integrates the process of reducing model uncertainty into the process of decision making to maximize the value of some objective. There are a wide range of techniques within each, and it is also possible to blend approaches. The key distinction is that methods in the first group do not involve any direct consideration of the possible actions or the utility that may result from those actions in how they select models [statistical approaches such as information criteria @BurnhamAnderson; and in particular, forecasting evaluation @Clark2001; @Dietze2018], while those in the second group require a more explicit statement of possible actions and the desired objectives [Decision theoretic approaches, for which @Polasky2011 provides an excellent and accessible review]. I illustrate how the most promising techniques from each of these groups would be applied to this simple problem, and demonstrate that it both cases they lead us away from the model that produces the most desirable decisions towards worse outcomes.  In retrospect, it will become obvious that neither these nor any other widely applied methods will select the model that leads to the best outcomes from the set of models considered.  


## Forecasts and Proper Scores

Like many decision problems, the task of setting a sustainable harvest quota appears to hinge on having an accurate forecast: if we can predict to what size the fish stock will increase next year, $X_t+1$, and we know the current stock, $X_t$, then we can safely harvest $X_{t+1} - X_t$.  Overestimating or underestimating such recruitment will result in over-harvesting or under-harvesting, respectively.  Selecting a model based on forecast skill is also justifiable on theoretical grounds, since it reduces the risk of over-fitting by comparing model predictions to later observations that were not used to estimate the model. @Gneiting2007 provides a rigorous proof for "proper" scoring criteria, which have the desirable property which no model predicting the distribution of future outcomes, $Q(x)$ can achieve a better average score than the true model $P(x)$. That is, unlike likelihood or other goodness-of-fit scores, it is impossible to overfit when conditioning on a strictly proper score -- since no model model can out-perform the true model. Not that strictly proper scoring rules score _probabilistic forecasts_ and not just point predictions, favoring models which accurately reflect the uncertainty over those which under-estimate it.  These features have made proper scoring rules for probabilistic forecasts a successful and popular approach for addressing model uncertainty in many other areas [@Gneiting2014; @Raftery2016] and a promising tool for evaluation of ecological forecasts [@Dietze2018]. 

I illustrate the process of model selection by strictly proper scoring rules using two scenarios. In Scenario A (passive observation) the fish stock is unharvested and allowed to recover towards carrying capacity (as simulated under our "true" model, Model 3) while comparing the observed stock size in each subsequent time step to the distribution predicted under Model 1 and Model 2 respectively [Fig].  The mean, $\mu$ and variance, $\sigma$ of the forecast are compared against the true observation $x$ using a Proper scoring rule of @Geiting2007, 

\bein{equation}
-(\mu - x )^2 / \sigma^2  - \log(\sigma) \label{proper}
\end{equation}

for each prediction over 100 replicate simulations of 100 time steps each [Fig].



```{r figure1, fig.width=7, fig.height=5, fig.cap = "Forecast performance of each model.  Panels A, B: Step ahead predictions of stock size under unfished (A) and fished (B) scenarios. Error bars indicating the 95% confidence intervals around each prediction, while stars denote the observed value in that year.  Because the models make different decisions each year in the fished scenario, the observed stock size in year 2, 3, etc under the management of model 1 (blue stars) is different from that under model 2 (red stars).  Panels C, D: corresponding distribution of proper scores across all predictions (100 replicates of 100 timesteps). Higher scores are better, confirming that model 2 makes the better forecasts."}
fig1ab + fig1cd + 
  plot_layout(widths =  c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```


In Scenario B (actively harvest), I have first solved for the optimal management strategy for both Model 1 and Model 2 [Fig], which for small noise and concave functions with linear reward structure can be done analytically [see proof in @Reed1979], or can be solved more generally by stochastic dynamic programming [see review by @Marescot2013, details in Appendix].  Under this scenario, replicate simulations of the stock are harvested at each time step using the optimal quota dictated by either Model 1 and Model 2. The resulting stock sizes in the time-step following this harvest are once again compared to the probabilities predicted by each model using Eq \eqref{proper}. Model 2 unequivocally outperforms Model 1 in both scenarios of passive observation and active harvest.

Despite the clearly superior predictive accuracy of Model 2 in both scenarios, the outcomes from management under Model 2 are substantially worse.  We can assess such outcomes in less abstract terms than forecasting skill, such as economic value (in dollars) or the ecological value (unharvested biomass). In our simple formulation of the decision problem, the "utility" the manager seeks to maximize is simply the economic value (net present value: the discounted sum of all profits from future harvests, Eq \eqref{utility}) of harvested fish.  This formulation ignores any utility provided by fish that are not harvested, beyond their contribution to future potential harvests. While it is possible to include such contributions directly in the utility function being optimized [e.g. @Halpern2013], even without doing so, Model 1 maintains both a higher unharvested biomass and also leads to higher economic returns throughout [Fig ].  


```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast. Harvest quotas derived from Model 1 result in a significantly higher fish stock size than under Model 2 (panel A). Economic returns under Model 1 are also substantially higher (panel B)"}
fig2a + fig2b
```

Had we been able to include Model 3 in our forecast comparisons, it would equal or outperform the forecasting skill of both Model 1 and Model 2 (as guaranteed by the theorem of @Gneiting2007), while also matching or out-performing their economic utility (as guaranteed by the theorem of @Reed1979).  In practice, we never have access to the generating model. 

One obvious limitation in this comparison is that scenario B treats each model as fixed over the entire course of the simulation. Research has long emphasized the importance of learning and adaption in the face of new data whenever we are dealing with model uncertainty [@Polasky2011]. A related limitation is that in Scenario B, decisions were based either on assuming Model 1 is correct or assuming Model 2 is correct. A more robust approach [e.g. @Walters1978] incorporates the uncertainty over models directly into the decision calculations by integrating over the probability each model $M$ (or parameter value) is correct when calculating the utility; $U(x_t, a_t) = \int U(x_t, a_t | M) P(M)$. The probability assigned to each model can then be updated after each subsequent action in light of the resulting outcomes [@Ludgwig1982, @Smith1981]. While a the wide range of strategies for such iterative updating are known [@Polasky2011]; all will fail to select the higher-performing of the two simple models considered  here. 


<!-- replacing step-ahead with n-step ahead would not matter-->

## Decision-Theoretic Approaches

Decision-theoretic approaches include scenario analysis, resilience thinking, optimal control and related methods [@Polasky2011], which have a long history in ecology and particularly in fisheries management. One of the most influential of these is _adaptive management_ [sensu @Walters1978], in which a manager seeks to both reduce uncertainty over time while also achieving the best outcomes given current knowledge.  While the term today is frequently used in a looser sense, adaptive management as originally developed can be considered an example of optimal control under model uncertainty [@Walters1981; @Smith1981].  In this approach, a manager assigns probabilities to each of the possible models under consideration, i.e. models 1 & 2.  The expected utility of any action reflects both the intrinsic uncertainty over future states under either model (process uncertainty) and the manager's uncertainty over the choice of models (or parameter values). Unlike the forecast comparison, an adaptive management approach explicitly considers the utility function, Eqn \eqref{utility}, which it seeks to maximize by intergrating over model uncertainty.  

In passive adaptive management, the manager chooses the action which maximizes that expected future utility. After observing the consequences of the action, the manager updates the probabilities each model is correct, typically by Bayes rule [e.g. @Walters1981; @Smith1981; @Ludwig1982]. In active adaptive management, the key difference is that the manager may chose an action which does not maximize the utility _given the current model uncertainty_, but instead, takes an action whose consequences will more quickly discriminate between the models and thus more rapidly reduce the uncertainty. After observing the outcome of this exploratory step, model probabilities are updated as before.  Under some scenarios the cost of this 'exploration' step can be made up later by actions based on the more precise knowledge gained as a result ('exploitation'). Alternative approaches such as scenario analysis can be considered approximations of this approach.  A scenario analysis does not rely on quantitative estimates of model probabilities or expected utility, but still consider possible outcomes over a suite of models (scenarios), select actions to minimize the risk or or other suitable objective, and update or eliminate scenarios deemed implausible over time [@Polasky2011]. 

Each of these approaches swiftly decides that model 1 in our example is implausible, and settles for making decisions driven almost exclusively by model 2.  I illustrate this using a passive adaptive management strategy following classic examples for parameter [@Ludwig1982] or structural [@Smith1981] model uncertainty. To demonstrate that the behavior is not driven by failure to explore sufficiently, (which might be addressed by an active adaptive management), I will assign initial probability that model 2 is true at 1%.  After a single iteration of learning, these probabilities are completely reversed, with the manager deciding that model 2 is almost certainly correct  [Fig]. As before, this results in a management practice with much worse ecological and economic outcomes than would have been realized by a manager who stubbornly clung to model 1 without updating [Fig , achieving a net present value over 100 time steps that only 31% that expected under management using Model 1 alone]. If instead of considering only these two models we consider a whole suite of Gordon-Schaefer-type models with varying $r$ and $K$ parameters, learning is slightly slower but no less counter-productive: mean stock size is $46%$ and net present value is $20%$ of what would be achieved under Model 1   (Appendix, Fig S1).  

```{r}
am <- am1$df %>% mutate(belief = am1$posterior$V2)
am %>%  ggplot(aes(time, states[state], col = belief)) + 
  geom_line() + 
  geom_point() +
  scale_colour_gradient(limits = c(0,1), low = pal[1], high = pal[4]) +
  ylab("stock size")
```


# Discussion

Given this simplified version of model uncertainty in which one of the two models leads to effectively optimal ecological and economic outcomes, current approaches invariable choose the other.  The reason for this may appear obvious once we compare both curves to that of the underlying model, Model 3.  Plotting the growth rate functions of each model, [Fig 4a], it is hardly surprising that no method which would prefer the closely overlapping Model 2 to the no-where-close Model 1 as the better approximation of Model 3. Seeing this example, it is natural to pose any of four objections: (1) this problem is an artifact of overly-simplified models, (2) this problem is an artifact of the optimal control solution (3) the problem arises from different objectives, (4) the problem is already solved. I address these in turn.


```{r fig.cap="Panel A: Population growth curves of each model. The positive equilibrium of each model occurs where the curve crosses the horizontal axis. Note that while Model 2 is a better approximation to the truth (Model 3), Model 1 better approximates the stock size which leads to maximum growth."}
# Fig 4
plot_models + plot_policies
```

 

Is this result an artifact of the simple models considered here?  A more realistic approach would consider uncertainty over model parameters, as well as models with different structural forms, such as Beverton Holt [@], Ricker [@], Shepard [@].  Such complexity would not be sufficient to resolve the paradox of model uncertainty illustrated here. Simplicity facilitates intuition: Model 1 performs best because it correctly approximates the stock size which achieves maximum growth rate under the true model.  In contrast, each of those widely used recruitment relationships are symmetric, and their best-fitting parameter estimates will always have a maximum growth rate at a lower stock size. It is easy to point out that even this wider array of parameter and model values is too simple, since fisheries models in practice are much more complicated still (indeed, too complicated for the formal integration over model uncertainty considered here).  It is indeed true that either the forecasting or adaptive management example would work as expected if the set of candidate models included the true model, or at least a functional form that could better approximate the peak of the true model while also fitting better than model 2.  But we never have the true model in real systems.   If the candidate models are only caricatures, so too is the "true" model.  The candidate models considered here are no doubt much closer approximations to model 3 (all being single species, unstructured in age, stage, time or space, etc) than any model used in practice is to reality. Morever, a more accurate model is not required to reach optimal outcomes -- Model 1 already provides this by correctly approximating the key feature in this problem, even while it is wrong about much else.  How do proceed when our best methods fail in such a simple case? 


<!-- move to appendix?  too confusing/too long?-->
Is this result an artifact of the optimal control? The analysis has relied on optimal control theory to determine the optimal decision associated with each model or distribution of models.  The resulting policies are mathematically optimal under the assumptions given [see @Reed1979 for stochastic harvest problem in particular and @Marescot2013 for discussion of Markov Decision Problems more generally].  In most realistic decision problems, model complexity currently precludes the calculation of an optimal control solution at all ["the curse of dimensionality", e.g. @Polasky2011; @Marescot2013]. Under the assumptions of any of the models considered here, the optimal control solution leads to what is known as a "constant escapement" policy, which seeks to bring biomass to the target escapement level, $B_{MSY}$ as quickly as possible, $H_t = \min(X_t - B_{MSY}, 0)$.  If the assumptions are not met, a constant-escapement policy may not be optimal, as in the case when observations of the stock $X_t$ are not perfect [@pomdp-intro].  While constant escapement is used as a basis of management in some real world cases such as salmon fisheries, most marine fisheries are based on a target of "constant mortality", ($F_{MSY}$). Notably, a constant mortality policy, implemented as $H_t = F_{MSY} X_t$, would not perform better under Model 1 than Model 2.  Constant mortality is optimal only under deterministic growth at equilibrium (where the policy is the same under constant escapement).  Does that mean the result is an artifact of assuming constant escapement rather than constant mortality as the basis for management?  No, it does not.  It is just as easy to construct a model under which the constant mortality matches the constant mortality of the true model while being a poor predictor.  Moreover, using a non-optimal policy would always beg the question of whether the poor outcomes were merely the result of using a non-optimal policy rather than a consequence of model uncertainty.  

The optimal policy does not depend equally on all aspects of the underlying model.  In this simple case, the policy depends only on the biomass which provides the maximum growth rate, and it is precisely this feature which allows Model 1 to match the policy of the true model despite over-estimating both the per-capita growth rate and unharvested equilbrium stock size so dramatically. The ability to capture certain key features rather than replicate every detail is the objective of any modeling effort, and the reason why scientific decision making is possible in the first place.  Many optimal control solutions are similarly insensitive to getting all the details right.  If we seek to improve decisions in face of model uncertainty, we must be able to exploit this aspect to find models fit to purpose, and be cautious about employing methods that would reject such models out of hand.  


Does this problem merely reflect the fact that different objective functions lead to different solutions? In selecting models based on forecast accuracy



 Of course no statistic favors Model 1 over Model 2.  Nevertheless, the optimal policy derived under Model 1 is virtually indistinguishable from the optimal policy for the true model, [Fig 4b].  It is considerably less obvious that the adaptive management solution would also fail to ever select Model 1.  After all, that approach integrates over the uncertainty in the models, and explicitly seeks to maximize utility (here assumed to be economic net present value), where Model 1 does so much better.  Nevertheless, the "adaptive" part of the approach is actually its downfall. If the manager never learned, never adapted by updating the probabilities that each model was correct, the overall economic and ecological benefits would have been much higher.  What if the learning step was not based on comparing model expectations to observed outcomes, but was instead conditioned only on utility derived by that action?  Clearly such a strategy would take the largest harvest possible, crashing the stock and forfeiting all future harvest value. Such optimization techniques are known as greedy algorithms, and clearly fail in fisheries management.  

There are techniques for approaching model uncertainty which condition on utility alone which are not greedy algorithms. For example, in model-free deep reinforcement learning, a neural network is trained on cumulative utility to select the best action.  


If the problem as posed is not solved by existing approaches, then perhaps the paradox is only a result of 

- This result is driven by the unrealistic simplicity, and would not occur in more realistic examples.  




An example based on a constant-mortality policy would be easier to critique by observing that the policy was not in fact optimal in the first place.  Recall that the purpose of this modeling exercise is not to precisely mimic the decision process of a specific quota in specific fish stock, but rather to illustrate in more general terms how carefully considered theory for approaching decision making under model uncertainty, from rigorous assessment of forecast skill to adaptive management, fail to handle this very simple example.



No measure of forecast skill would prefer predictions drawn from Model 1 over those drawn from Model 2. Any process of updating model probabilities will lead away from Model 1 and towards Model 2.  Simpler methods such as information criteria, $r^2$ or other goodness-of-fit metrics will also all clearly prefer model 2.  The problem does not arise from a lack of data, or from over-fitting, which given sufficient data would be addressed by the use of @Gneiting2007's proper scoring rules.  Nor does the problem resolved by approaches explicitly designed to maximize utility (e.g. economic value) over model uncertainty [@Polasky2011].  

  

- This result is simply an example of how different objectives lead to different strategies. 

<!-- -->
# Discussion take 2

Some readers will find the presentation of forecasting 


addressing potential critiques: 

Making this problem more complicated might better approximate real-world decision making but would
by no means resolve the issue.  For example, increasing the suite of models under consideration to
include all possible parameter values Gordon-Schaefer model, or better, adding
in many other well studied structural forms such as Ricker [@], Beverton-Holt [@], or Shepard equation
to our set of candidate models would give the same result.  Moreover, focusing on expanding the set of
possible models ignores the simple fact that our most stripped down version of the problem already 
contains a model which can obtain nearly optimal performance, just as it already contains a model which
can make pretty accurate forecasts.  If well considered approaches make the wrong choice in this simple
problem, the issue requires more attention.  

Note that this problem is not merely a case of alternative objectives leading to alternate outcomes. 
Unlike model selection based on forecasting performance, the adaptive management approach explicitly
considers the objective of maximizing the utility, and yet still fails to select the correct model.




## Discussion


This paradox in performance of forecasting vs performance in decision making can be easily resolved by considering the context of the decision problem more closely. Comparing plots of the functional form of our two logistic-curve models, compared to the functional form of the "true" model used to drive the simulations (Fig. 3A), it is clear to see that model 2 does indeed lie closer to the true model throughout the state space, agreeing precisely with the true carrying capacity (where both functions cross zero with negative slope).  However, the peak of model 3 very nearly matches the peak of model 1. The optimal decision literature, dating back to the 1950s [@Schaefer1954], demonstrates that the Maximum Sustainable Yield (MSY) is maintained by harvesting a stock down to the size at which it achieves its maximum growth rate, i.e. 50% of the un-fished equilibrium size for a symmetric growth model ($K/2$).  Model 1, while being very wrong about both the growth rate and the un-fished equilibrium, is nevertheless nearly perfect in estimating the stock size at which maximum growth rate is achieved, and this gives nearly optimal decisions (Fig 3B) despite its terrible forecasts.   

Thus, each year our model 1 managers are again chagrined to see the stock size estimates come in far below their rosy predictions, but nevertheless manage to set a nearly optimal quota by comparing the observed stock size to the model's predicted optimal escapement level [@Reed1979].  Meanwhile, model 2 managers could only congratulate themselves that each year's observations fall neatly within their predicted interval, unaware that the they were over-exploiting the fishery by both economic and ecological metrics.  If we had access to model 3, we would no doubt find that it outperformed model 2 in forecast accuracy as well as ecological and economic performance.  But in real ecological decision making, we never know the true model -- we will always be comparing among approximations.  Within fisheries, even in today's parameter-rich age-structured models, recruitment approximations with symmetric growth functions (Logistic, Ricker, Beverton-Holt, etc) still dominate [@ramlegacy2012; @ramlegacy2018].  

This issue is by no means unique to fisheries.  Throughout resource management and conservation, and no doubt other fields, decisions about which model to use are guided by which model best fits available data [@Clark1990]. Increasingly, these are joined by calls to assess _forecast accuracy_ [@Clark2001; @Dietze2018; @White2019] as the ultimate test of a model.  Yet as this example illustrates, such metrics, no matter how rigorously defined, may select entirely the wrong model for the task at hand.  A decision maker has other objectives than prediction accuracy, and approaches which ignore these considerations do so at their peril. This example has also shown that once we are managing with the wrong model, no amount of comparing predictions from that model to actual outcomes will guarantee we discover our mistake.  Despite its consistently good predictions, model 1 is in fact over-fishing to a dangerous level.  

Because we will never know the "true" model, we must never forget that our choice of models must reflect the context for which those models will be used [@Levins1966].  Model 2 would indeed be a better choice than model 1 if our objective was to determine the natural size of our fish stock in the absence of fishing.  Only when we focus on the outcomes we actually care about -- in this case, economic and ecological performance -- can we see which model is best for decision-making.  Model 1, despite its many mistakes, is right about one key feature: the biomass for peak growth -- and that is enough to guarantee nearly optimal performance.   This conclusion should also be reassuring to both modelers and decision makers, for it reminds us that effective models need be perfect or even all that close in every aspect, as long as they capture the key features of the decision context.  Decision theory [e.g. @Clark1973; @Reed1979; @pomdp-intro] and research into the socio-ecological models [e.g. @Kareiva2006] helps us better understand that context.  Adaptive management approaches [@Walters1978] can apply that theory to compare management outcomes between models directly.  It is not true that we need good forecasts to make good decisions.




```{r figure3, fig.width=7, fig.height=4, fig.cap="Panel A: Population recruitment curves for each model, compared to that of the true Model. 3 Model 2 more closely approximates the true Model 3, but note that maximum value Model 1 and Model 3 occur at nearly the same value for the state, x.  Panel B: The computed optimal policy of each model, derived by SDP, expressed in terms of the target escapement (population size remaining after harvest) for each possible stock size. Model 1 over-harvests consistently, while the target escapement under Model 2 is nearly identical to that of the true Model 3."}
fig3a + ggtitle("A") + fig3b + ggtitle("B")
```

# Methods

Stochastic transition matrices are defined for models 1-3 on a discrete grid of
240 possible states spaced uniformly from 0 to 24. A discrete action space 
enumerating possible harvest quotas is set to the same grid. The utility of a harvest quota $H_t$ given a population state $X_t$ is given by $U(X_t, H_t) = \min(X_t, H_t)$ (i.e. a fixed price for realized harvest). A modest discount of
$\gamma = 0.99$ allows comparisons to approaches that ignore [@Schaefer1954] or include [@Clark1973; @Reed1979] discounting; results are not sensitive to this choice. The optimal policy for each model is determined by stochastic dynamic programming [@Marescot2013].  Details of the implementation, including fully reproducible R code, have been included in the appendix.



acknowledgements: |
  This work was supported in part by NSF CAREER (#1942280) and computational resources from NSF's XSEDE Jetstream (DEB160003) and Chameleon cloud platforms.





# A way forward?

note that the model set considered here already contains a model which captures the key feature. 

- Greedy strategy will not work.  Could stick with a model for 50 years then switch...
- A mechanistic understanding of the decision process, not the ecological one.  Analyzing the decision problem, we know that in this situation, the optimal stategy depends only on the location of the peak of the growth rate.  This could suggest a very different way forward
- 



Yet the model that leads to the best decisions is not always the model that makes the most accurate forecasts, as I illustrate here. Surprisingly, this can even happen when a decision is derived directly from a complex optimization routine of a probabilistic predictive models [@Marescot2013].  Reality is complex; even our best models can only ever be approximations of underlying processes.  Here, I use a classic, well-understood example from fisheries management [@Schaefer1954; @Clark1973; @Reed1979] to illustrate both the paradox of how a model with the worst forecast provides the best decision outcomes, as well as show how we can avoid selecting models that are poorly suited for management by considering the management context more explicitly. These results underscore that in choosing the best model for decision-making, it can be more important to capture a single key feature of the process than it is to make the most accurate prediction about future states.  

<!--
Many ecological management problems are sequential decision problems, in which each year (or other interval) a manager must observe the state of the system and choose a course of action to maximize long term objectives. Such problems inherently depend on forecasts: each possible action can result in a different forecast for the future state, typically reflecting some uncertainty as well.  The utility the manager derives may depend on both the choice of action and the state of the system, reflecting the costs and benefits associated with each. Sequential decision-making problems are distinguished by the need to think more than one move ahead. For instance, harvesting as many fish as possible in year one may maximize the market value that year, but if too few fish are left to reproduce then harvest in future years will suffer.  The same calculus of thinking ahead frequently applies to rebuilding species populations as well [e.g. @Lambert;  @Chades2008].  
-->





\pagebreak 


# References