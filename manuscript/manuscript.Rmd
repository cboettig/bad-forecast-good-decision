---
title: 'Are we learning to build better models at the cost of better decisions?'
author:
- name: Carl Boettiger
  email: cboettig@berkeley.edu
  affiliation: a,1
date: "`r Sys.Date()`"

abstract: |
  Model-based forecasts have been enabled by recent explosion of data and computational methods and spurred on by decision-makers appetite for forecasts in everything from elections to pandemic response. Using a classic example from fisheries management, I demonstrate that selecting the model that produces the most accurate and precise forecast can lead to decidedly worse outcomes.

corresponding_author:
  code: 1
  text: "To whom correspondence should be addressed. E-mail: cboettig@berkeley.edu"

bibliography: refs.bib
address:
- code: a
  address: Department of Environmental Science, Policy, and Management, University of California, 130 Mulford Hall Berkeley, CA
    94720-3114, USA
    
keywords:
- forecasting,
- adaptive management,
- stochasticity,
- uncertainty,
- optimal control


## change to true to add optional line numbering
lineno: true

output: rticles::elsevier_article
layout: 3p
---


```{r, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"
```
```{r, include=FALSE}
rmarkdown::render("appendix.Rmd")
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, dev="cairo_pdf")
```


<!-- I think the introduction could have a clearer storyline and better introduce some of the ideas you bring up later on in the methods/results. A quick outline idea: 

  - Paragraph1: Model-based forecasts have become a cornerstone of applied science in disciplines as diverse as atmospheric science [weather?], economics [stock markets?], and ecology [dietz]. Seeking to inform a variety of decisions, near-term forecasting in these diverse fields has  simultaneously been encouraged by policy-makers [CITE?] and enabled by the rapid and recent increase in data availability and computational methods leading to a rapid expansion of thier use (or something...)
  
   - Paragraph 2: Introduce ecological decision making under uncertainty, rapid global change, etc. (kind of like your current opening bit)

  - Paragraph 3: But do better forecast always lead to better decisions?  Because model-based forecasts frequently play an important role in decision-making,it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty will also improve decision-making outcomes. 
            - While the literature has largely addressed issues of overfittting that might lead forecasts to misguide decisions, proper scoring methods blah blah [...introduce this concept]. 
            - Beyond overfitted models,... [maybe add something about Value of Information literature here?]
  
  - Paragraph 4: Your current paragraph on "I illustrate..." (I'd add into that paragraph something about the fact that you explore adaptive management?)

  
A second idea on framing: this maybe is a to divergent from the main point (or leads you into a post-modernist narrative that you'd prefer to avoid...) but it seems like you could reframe the whole thing as an argument for embracing an inclusive process of resource management that acknowledges pluralism.  
  (1) Policy makers often think "scientists" hold knowledge of "an environmental reality"
  (2) Despite this not being true (scientist disagree on a lot of things), even when "science" points to a more likely reality, that might be off in ways that can fundementally misguide decisions. 
  (3) The problem is deeper than acknowledging a "rashamon set" of models or multiple possible descriptions of of reality. Sometimes what appears closer to reality is actually off in a way that is critical to the decision.
-->

<!-- Open on real-world issues, uncertainty, particularly model uncertainty as key theme.  -->

Global change issues are complex and outcomes are difficult to predict, making approaches to uncertainty a central part
of effective decision-making [@ @]. Reducing that uncertainty by improving models in response to more data
becomes available 



While some uncertainty is intrinsic to the underlying processes (stochasticity) or by
limits to what variables we can observe and how accurately we measure them (measurement uncertainty), much uncertainty 
is the result of our imperfect knowledge of the processes involved, as expressed by the structure and
parameters of mathematical models used to approximate those processes (model uncertainty). Unlike the other forms of
uncertainty, model uncertainty can be 
reduced by gathering additional data, which may rule out certain models or parameter values as implausible[^1]. 
When model parameters are estimated directly from available data, there is a risk that the best-fitting models may 
_overfit_ to patterns arising from by chance from the stochasticity or measurement error which do not reflect the
underlying process [@Ginzburg2004]. Researchers have often favored simpler models which are less prone to over-fitting, sometimes
through explicit penalties such as information criteria [@], though these can be misleading [@]. With the rapid 
expansion of available ecological and environmental data [@], it is increasingly possible to rely instead on comparisons
between model predictions and data purposely excluded from model estimation (cross-validation), or better, 
comparisons between model predictions and data collected in the future (forecasting, @Clark2001; @Dietze2018]). 
Over sufficiently long timescales, comparing model forecasts to future observations will select the models with
greatest predictive accuracy.  Because model-based forecasts frequently play an important role in decision-making,
it is commonly assumed [e.g. @Walters1981; @Clark2001; @Dietze2018] that this approach for addressing model uncertainty
will also improve decision-making outcomes. Here, I illustrate that this need not be the case: it is quiet possible
that any approach with improves the forecast accuracy of the model(s) over time can simultaneously lead to worse 
decision outcomes. This example illustrates that knowing when increased forecast accuracy will or will not also improve 
decision outcomes in general remains an open problem, as is the challenge of what to do about it.

[^1]: While some authors distinguish between model uncertainty and parameter uncertainty, I will refer collectively to
any uncertainty that arises from our imperfect knowledge of the system as "model uncertainty". Uncertainty in model structure can often be reflected by appropriate parameterization, just as models with different parameter values can be treated distinctly. Whereas stochasticity is inherent to the process (or rather, to our choice of state variables, @Boettiger2018) and measurement uncertainty can only be reduced by more accurate tools for measurement, model uncertainty is reducible by learning from additional data. 


<!-- 
It may be easy to dismiss this example on the argument that both models are (or more precisely,
the any logistic growth model, regardless of parameterization would be) extremely naive
and unrealistic, and any solution would surely begin with replacing both models with something
more realistic.  Yet such objections overlook the simple fact that for all its faults in 
prediction, model 1 already provides nearly optimal performance.  That performance arises
because model 1 has very accurately reflected one key aspect of the true model (the position of the
maximum growth rate), even while it is wrong about every other aspect of the biology. 

The open problem is simply to propose an approach to model uncertainty which would select model 1
over model 2 (i.e. choosing among the suite of available models, the one that gives the better 
decisions.)


An important aspect of this example is that model 1 performs nearly optimally. 


-->

I illustrate this problem using an example from fisheries management. 
Fisheries are a significant economic and conservation concern world
wide and their management remains an important debate [e.g. @Worm2006;
@Worm2009; @Costello2016]. Moreover, their management has been a proving 
grounds for theoretical and practical decision-making issues which are widely
applicable in other areas of ecology and conservation [@Ludwig1993; @Lande1994],
and one that has long wrestled with issues of uncertainty in the context
of management decisions [e.g. @Clark1973; @Reed1979; @Walters1981; @Ludwig1982].
While modern fisheries management frequently relies on complex models which may
contain scores of parameters to reflect the specific age or stage structure of a
specific fish stock [@ramlegacy2018; @ram], I will rely on simple, well-studied models which
permit greater intuition and generalization [@Levins1966]. Consistent with 
such previous work [@Schaefer1954; @Clark1973; @Reed1979; @Walters1981; @Ludwig1982; @Costello2016],
let us consider the problem of determining the optimal harvest policy given a 
measurement of the current stock size.  

# Ecological Models

The sustainable harvest decision problem can be stated as follows: The fish stock is observed to be in state $X_t$ at time $t$, and is then subjected to some harvest $H_t$ before recruiting new fish, subject to stochastic environmental noise $\xi_t$, to bring the stock to $X_t+1$,

\begin{equation}
X_{t+1} = f(X_t - H_t, \xi_t) 
\end{equation}

A manager seeks each year to select the harvest $H_t$ which will maximize the sum
of the utility derived from such a harvest and such a state, $U(X_t,H_t)$, over all time,
subject to discount rate $\delta$ [@Clark1973]:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t \label{utility}
\end{equation}


Further we imagine that the function $f$ is not known precisely, and so we will rely on an evaluation of forecasting skill across a set of candidate models to determine which one to use to manage the fishery.  Again for simplicity, we will restrict ourselves to two simple candidate models $f_1$ and $f_2$.  Both share the same underlying structure of logistic recruitment (known as the Gordon-Schaefer model in fisheries context owing to groundbreaking work independently by @Schaefer1954 and @Gordon1954), differing only in their choice of certain parameters:

\begin{equation}
f_i(Y) = Y + r_i Y \left( 1 - \frac{Y}{K_i} \right) * \xi_t(\sigma)
\end{equation}

Where $\xi_t(\sigma)$ represents log-normal random noise with mean of unity and log-standard-deviation $\sigma$.
Model 1 is given by $r_1 = 2$, $K_1 = 16$, $\sigma_1 = 0.05$, Model 2 by $r_2 = 0.5$,  $K_2 = 10$, $\sigma = 0.075$ (in dimensionless units).  Having both the larger growth rate and the larger carrying capacity, Model 1 is clearly the more optimistic of the two choices. 

Selecting between Model 1 and Model 2 can thus be considered the simplest illustration of the model uncertainty problem. This is a subset of the more general problem of selecting model parameters, assuming a logistic growth, which itself is a subset of estimating the best structural from (e.g. Ricker, Beverton-Holt, etc).  There is no need to consider these more complicated versions of the model uncertainty problem here, since they all inherit the same issue.  Reducing the model selection problem to these two models simplifies the presentation and will aid intuition at no loss of generality.  

The only additional assumption we will need is that the "true" model is not among the suite of models under consideration. 
Mathematical models are, at best, approximations of the underlying processes.  Ecological processes are much too complex to ever be modeled exactly.  For illustrative purposes, we will thus assume the "true" process to be given by Model 3, which is unknown to the decision-maker, but similar enough to at least one of the candidate models might be considered a reasonable approximation:

\begin{equation}
f_3(Y) = Y + r_3 Y^4 \left( 1 - \frac{Y}{K_3} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$. 


The task of deciding whether Model 1 or Model 2 would be the better choice for decision making is 
thus perhaps the simplest example of the much studied issue of model uncertainty that we can pose. 
As in any real world scenario, neither model is the true model, but nevertheless this model set
contains a good enough approximation of the true model to make good decisions.  However, any of
the well-developed approaches for decision-making under model uncertainty will prefer Model 2 over
Model 1, despite the fact that the optimal policy under Model 2 leads to much worse outcomes
ecologically and economically.  


# Methods for Managing Under Model Uncertainty 

<!-- I feel like parts of the below paragrph should be in the abstract (i.e. "I illustrate how the most promising techniques from statistical approaches and decision theoretic approaches to decision making under uncertainty would be applied to this simple problem, and demonstrate that it both cases they lead us away from the model that produces the most desirable decisions towards worse outcome") -->

A wide range of paradigms are available for approaching the issue of decision-making under uncertainty. These approaches can roughly be divided into two groups: the first group treats the issue of model uncertainty independently from the decision itself, while the second integrates the process of reducing model uncertainty into the process of decision making to maximize the value of some objective. There are a wide range of techniques within each, and it is also possible to blend approaches. The key distinction is that methods in the first group do not involve any direct consideration of the possible actions or the utility that may result from those actions in how they select models [statistical approaches such as information criteria e.g. @Burnham1998]; and in particular, forecasting evaluation @Clark2001; @Dietze2018], while those in the second group require a more explicit statement of possible actions and the desired objectives [Decision theoretic approaches, for which @Polasky2011 provides an excellent and accessible review]. I illustrate how the most promising techniques from each of these groups would be applied to this simple problem, and demonstrate that it both cases they lead us away from the model that produces the most desirable decisions towards worse outcomes.  In retrospect, it will become obvious that neither these nor any other widely applied methods will select the model that leads to the best outcomes from the set of models considered.  

## Statistical approaches: Forecasting under "Proper" Scoring Rules

Like many decision problems, the task of setting a sustainable harvest quota appears to hinge on having an accurate forecast: if we can predict to what size the fish stock will increase next year, $X_t+1$, and we know the current stock, $X_t$, then we can sustainably harvest $X_{t+1} - X_t$ without decreasing the biomass over the long term.  Selecting a model based on forecast skill is also justifiable on theoretical grounds, since it reduces the risk of over-fitting by comparing model predictions to later observations that were not used to estimate the model [@Gneiting2014].

@Gneiting2007 provides a rigorous proof for such "proper" scoring criteria, which have the desirable property which no model predicting the distribution of future outcomes, $Q(x)$ can achieve a better average score than the true model $P(x)$. That is, unlike likelihood or other goodness-of-fit scores, it is impossible to overfit when conditioning on a strictly proper score -- no model model can out-perform the true model. Not that strictly proper scoring rules score _probabilistic forecasts_ and not just point predictions, favoring models which accurately reflect the uncertainty over those which under-estimate it.  These features have made proper scoring rules for probabilistic forecasts a successful and popular approach for addressing model uncertainty in many other areas [@Gneiting2014; @Raftery2016] and a promising tool for evaluation of ecological forecasts [@Dietze2018]. 

<!-- This scoring bit comes abruptly (and it feels like could be in the introduction) -->


I illustrate the process of model selection by strictly proper scoring rules using two scenarios. In Scenario A (passive observation) the fish stock is unharvested and allowed to recover towards carrying capacity (as simulated under our "true" model, Model 3) while comparing the observed stock size in each subsequent time step to the distribution predicted under Model 1 and Model 2 respectively [Fig 1].  The mean, $\mu$ and variance, $\sigma$ of the forecast are compared against the true observation $x$ using a Proper scoring rule of @Gneiting2007, 

\begin{equation}
-(\mu - x )^2 / \sigma^2  - \log(\sigma) \label{proper}
\end{equation}

for each prediction over 100 replicate simulations of 100 time steps each [Fig 1].



```{r figure1, fig.width=7, fig.height=5, fig.cap = "Forecast performance of each model.  Panels A, B: Step ahead predictions of stock size under unfished (A) and fished (B) scenarios. Error bars indicating the 95% confidence intervals around each prediction, while stars denote the observed value in that year.  Because the models make different decisions each year in the fished scenario, the observed stock size in year 2, 3, etc under the management of model 1 (blue stars) is different from that under model 2 (red stars).  Panels C, D: corresponding distribution of proper scores across all predictions (100 replicates of 100 timesteps). Higher scores are better, confirming that model 2 makes the better forecasts."}
predictions <- read_csv("../data/predictions.csv") %>% mutate(model = as.character(model))

fig1cd <- predictions %>%
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 2, show.legend = FALSE) +
  coord_cartesian(xlim = c(-100, 1), ylim = c(0,4000)) +
  xlab("Proper score") +
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "C.",
    B_fished = "D."))) + 
  scale_x_continuous(breaks = c(-100, 0)) +
  theme(axis.text.y = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )



 fig1ab <- predictions %>% 
  filter(reps == "2", time < 10) %>%
  ggplot(aes(time, col = model, fill = model)) + 
  geom_point(aes(y = expected)) + 
  geom_errorbar(aes(ymin = low, ymax = high)) +
  geom_point(aes(y = true), pch = "*", size = 12, 
             alpha = 0.6) + 
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "A. Without fishing",
    B_fished = "B. Managed harvest"))) +
  ylab("stock size")
 
 
 
 fig1ab + fig1cd + 
  plot_layout(widths =  c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```


In Scenario B (actively harvest), I have first solved for the optimal management strategy for both Model 1 and Model 2 [Fig 1b]. For small noise and concave functions with linear reward structure this can be done analytically [see proof in @Reed1979], or solved more generally by stochastic dynamic programming [see review by @Marescot2013, details in Appendix].  Under this scenario, replicate simulations of the stock are harvested at each time step using the optimal quota dictated by either Model 1 and Model 2. The resulting stock sizes in the time-step following this harvest are once again compared to the probabilities predicted by each model using Eq \eqref{proper}. Model 2 unequivocally outperforms Model 1 in both scenarios of passive observation and active harvest.

Despite the clearly superior predictive accuracy of Model 2 in both scenarios, the outcomes from management under Model 2 are substantially worse.  We can assess such outcomes in less abstract terms than forecasting skill, such as economic value (in dollars) or the ecological value (unharvested biomass). In our simple formulation of the decision problem, the "utility" the manager seeks to maximize is simply the economic value (net present value: the discounted sum of all profits from future harvests, Eq \eqref{utility}) of harvested fish.  This formulation ignores any utility provided by fish that are not harvested, beyond their contribution to future potential harvests. While it is possible to include such contributions directly in the utility function being optimized [e.g. @Halpern2013], even without doing so, Model 1 maintains both a higher unharvested biomass and also leads to higher economic returns throughout [Fig 2].  


```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast. Harvest quotas derived from Model 1 result in a significantly higher fish stock size than under Model 2 (panel A). Economic returns under Model 1 are also substantially higher (panel B)"}

sims <- read_csv("../data/sims.csv")
npv_df <- read_csv("../data/npv_df.csv")


plot_ecology <- 
  sims %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  group_by(model, time) %>%
  summarise(mean_state = mean(state), sd = sd(state), .groups = "drop") %>%
  filter(time < 25, model != "3") %>%
  ggplot(aes(time, mean_state)) + 
  geom_line(aes(col = model), lwd=1.5, show.legend = FALSE) +
    geom_ribbon(aes(ymin = mean_state - 2*sd, 
                    ymax = mean_state + 2*sd,
                    fill = model),
                alpha = 0.2, show.legend = FALSE) +
  ylab("state") + 
  labs(subtitle = "A. Ecological outcomes")

optimal <- select(filter(npv_df, model == "3"), time, mean_npv)

plot_econ <- 
npv_df %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  filter(model != "3", time %in% seq(1,100, by = 5)) %>%
  ggplot(aes(time, mean_npv)) +
  geom_line(data = optimal, lwd = 1.5, col = "grey20") +
  geom_point(aes(col=model), size = 4, alpha = 0.8) + 
  ylab("Net present value") + xlab("time")  + 
  labs(subtitle = "B. Economic outcomes")


plot_ecology + plot_econ
```

Had we been able to include Model 3 in our forecast comparisons, it would equal or outperform the forecasting skill of both Model 1 and Model 2 (as guaranteed by the theorem of @Gneiting2007), while also matching or out-performing their economic utility (as guaranteed by the theorem of @Reed1979).  In practice, we never have access to the generating model. 

One obvious limitation in this comparison is that scenario B treats each model as fixed over the entire course of the simulation. Research has long emphasized the importance of learning and adaption in the face of new data whenever we are dealing with model uncertainty [@Polasky2011]. A related limitation is that in Scenario B, decisions were based either on assuming Model 1 is correct or assuming Model 2 is correct. A more robust approach [e.g. @Walters1978] incorporates the uncertainty over models directly into the decision calculations by integrating over the probability each model $M$ (or parameter value) is correct when calculating the utility; $U(x_t, a_t) = \int U(x_t, a_t | M) P(M)$. The probability assigned to each model can then be updated after each subsequent action in light of the resulting outcomes [@Ludwig1982, @Smith1981]. While a the wide range of strategies for such iterative updating are known [@Polasky2011]; all will fail to select the higher-performing of the two simple models considered  here. 


<!-- replacing step-ahead with n-step ahead would not matter-->

## Decision-Theoretic Approaches

Decision-theoretic approaches include scenario analysis, resilience thinking, optimal control and related methods [@Polasky2011], which have a long history in ecology and particularly in fisheries management. One of the most influential of these is _adaptive management_ [sensu @Walters1978], in which a manager seeks to both reduce uncertainty over time while also achieving the best outcomes given current knowledge.  While the term today is frequently used in a looser sense, adaptive management as originally developed can be considered an example of optimal control under model uncertainty [@Walters1981; @Smith1981].  In this approach, a manager assigns probabilities to each of the possible models under consideration, i.e. models 1 & 2.  The expected utility of any action reflects both the intrinsic uncertainty over future states under either model (process uncertainty) and the manager's uncertainty over the choice of models (or parameter values). Unlike the forecast comparison, an adaptive management approach explicitly considers the utility function, Eqn \eqref{utility}, which it seeks to maximize by integrating over model uncertainty. After observing the consequences of the action, the manager updates the posterior probabilities that each model is correct, typically by Bayes rule [e.g. @Walters1981; @Smith1981; @Ludwig1982; @Punt2016]. The decision strategy is then re-evaluated in light of this revised uncertainty.  Adaptive management may be either passive or active. Under passive adaptive management, the decision-maker chooses the action which maximizes that expected future utility.  Under active adaptive management, the decision-maker may chose 'exploratory' actions which are not optimal under the current uncertainty but may lead to more rapidly decreasing the uncertainty in the future [@Walters1978]. In practice, many managers are reluctant to employ exploratory strategies, so passive adaptive management approaches such as as Management Strategy Evaluation (MSE) are more common [@Punt2016].


<!--
Any of these adaptive management approaches proves to be maladaptive under the simple example of model uncertainty considered here.  That is, the manager derives lower utility (here, economic yield), and the fish stock is also surpressed to a lower biomass than would be achieved if an identical management approach without any adaptive updating to model uncertainty were employed.  Put another way, the "value of information" gained from future observations of stock in response to any management action is actually negative.     

Owing to the greater complexity involved in many fisheries models, a typical MSE would evaluate a fixed set of possible strategies rather than solve a sequential decision problem [@Marescot2013] for the optimal sequence of actions, but the central premise of iteratively updating model posterior probabilities remains the same.  By limiting ourselves to the simpler models considered here, we are able to perform a passive adaptive management analysis of the full sequential decision problem, analgous to earlier examples [@Walters1981; @Smith1981; @Ludwig1982], which ensures the result is not an artifact of considering too narrow a set of possible policies (e.g. only constant-mortality policies). 

-->

Unfortunately, any adaptive management strategy which updates posterior distributions that characterize model undertainty leads to worse outcomes than an approach which does no updating.  I illustrate the application of a passive adaptive management strategy to this simple example, following classic examples for parameter [@Ludwig1982] or structural [@Smith1981] model uncertainty.  To demonstrate that the behavior is not driven by failure to explore sufficiently, (which might be addressed by an active adaptive management), I will assign initial probability that model 2 is true at 1%.  After a single iteration of learning, these probabilities are completely reversed, with the manager deciding that model 2 is almost certainly correct  [Fig 4]. As before, this results in a management practice with much worse ecological and economic outcomes than would have been realized by a manager who stubbornly clung to model 1 without updating [Fig 2, achieving a net present value over 100 time steps that only 31% that expected under management using Model 1 alone]. If instead of considering only these two models we consider a whole suite of Gordon-Schaefer-type models with varying $r$ and $K$ parameters, learning is slightly slower but no less counter-productive: mean stock size is $46%$ and net present value is $20%$ of what would be achieved under Model 1 (Appendix, Fig S1).  

```{r figure3, fig.cap="Adaptive management under model uncertainty, color indicates the belief that Model 2 is correct (red). Each model is assigned an initial prior belief. To ensure that any issues with passive exploration are not the cause of preferring Model 2, the initial belief in Model 2 is set to 1%.  Within a single iteration of adaptive management, the belief over models is updated to near certainty in Model 2, resulting in higher harvests and lower stock sizes similar to managing under Model 2 alone, Fig 2."}
am <- read_csv("../data/am.csv")
am %>%  filter(time <30) %>%
  ggplot(aes(time, states[state], col = belief)) + 
  geom_line() + 
  geom_point() +
  scale_colour_gradient(limits = c(0,1), low = pal[1], high = pal[4]) +
  ylab("stock size")
```


   


# Discussion

<!-- 
First: Intuition for the result.
  - See that model 1 captures the key feature, while model 2 is just 'closer to the curve'
  - See that this is a generic truth about decision-making: there's always a Model 1
  - This is not about over-fitting.
-->

<!-- quick summary of what was demonstrated -->
Given this simple decision problem in which one of the two models leads to effectively optimal ecological and economic outcomes, current approaches invariably choose the other. Moreover, a decision maker employing an adaptive management or iterative forecasting assessment such as those considered here would have no way of realizing that the outcomes they experienced were in fact sub-optimal.  In both approaches, the manager quickly concludes that Model 1 is entirely implausible, while finding that Model 2 is remarkably accurate at predicting future values. This problem is not addressed by re-estimating parameters (equivalently, considering a larger suite of models with all possible $K$ and $r$ values): in fact leads to a model with slightly lower $K$ and even higher level of over-fishing and under-performance than Model 2 (Appendix B, Fig S1). 

<!-- Intuition, Figure 4 -->
The reason for Model 1's seemingly contradictory ability to make good decisions but bad forecasts becomes obvious once we compare both curves to that of the underlying model, Model 3.  Plotting the growth rate functions of each model, [Fig 4a], it is hardly surprising that no method exists which would not prefer the closely overlapping Model 2 to the no-where-close Model 1 as the better approximation of Model 3. However, Nevertheless, decisions based on Model 1 are nearly indistinguishable from those based on the true model [Fig 4b], while Model 2 leads to over-harvesting.  The explanation comes from noticing that the stock size corresponding to the maximum growth rate under Model 1 (the peak of the curve) falls at almost exactly the same stock size as that of the peak growth rate for Model 3.  Meanwhile, the peak of Model 2 occurs at a substantially lower stock size.  While the optimal control solution appears to depend only on a step-ahead forecast accuracy (indeed, the SDP solution method used here takes only step-ahead forecast probabilities as input, [@Marescot2013]), mathematical analysis showed long ago [@Reed1979] that the optimal solution for this problem depends only on keeping the biomass at the value responsible for the maximum growth rate.  This realization is quite general: for most decision problems, simple models will exist under which the optimal decision is the same as it would be for the true model, even when that simple model is wrong in most other ways.  

<!-- Inttuition for it's not just this example -->
This phenomenon is not unique to sequential decision problems or optimal control solutions.  For example, Management Strategy Evaluation [@Punt2016] in fisheries seeks to evaluate pre-specified strategies rather solve for the best possible strategy through optimal control.  This approach is well justified -- optimal control techniques such as stochastic dynamic programming illustrated here quickly become intractable for more complex models [@Marescot2013] typically used in fisheries stock assessments.  Constraining the search to only strategies that impose a constant mortality (defined as harvest per unit biomass, $F = H/B$) means we only have to evaluate those $N$ strategies each iteration, not solve a sequential decision problem. Doing so, we would find the best constant-mortality solution under Model 1 performs much worse than the best constant-mortality solution under Model 2. Does this mean MSE is not susceptible to this issue?  No, it does not.  It is just as easy to construct an alternative Model 1 with the same properties of leading to nearly optimal decisions while being rejected by any method of iterative learning, such as forecasting or adaptive management.  Just as the optimal control policy for a Gordon-Schaefer (logistic) model depends only on parameter $K$ [@Reed1979], the optimum constant escapement policy depends only on parameter $r$ [@Schaefer1954]. If Model 1 has a value of $r$ such that it happens to match the best possible constant-mortality solution for the (unobserved) true model, then $K$ can be set arbitrarily high, ensuring the any model selection, forecasting, or adaptive management approach would lead away from Model 1 and towards worse-performing models once again. Many optimization problems share this feature in which the optimal policy depends only on a subset or ratio of model parameters, such that it is usually easy to find a similar Model 1. This example also illustrates the importance of decision constraints -- a model that gives very good outcomes when we are free to vary the havest quota each year (the optimal control problem here) may give very poor results under the constraint of constant mortality, and vice versa. 



```{r figure4,  fig.cap="Panel A: Population growth curves of each model. The positive equilibrium of each model occurs where the curve crosses the horizontal axis. Note that while Model 2 is a better approximation to the truth (Model 3), Model 1 better approximates the stock size which leads to maximum growth.  Panel B: The optimal control policy under Model 1 is nearly identical to that under the true Model 3, while the optimal policy under Model 2 supresses stock to a much lower escapement level.", fig.width=8}
# Fig 4
model_curves <- read_csv("../data/model_curves.csv") %>% mutate(model = as.character(model))
policies <- read_csv("../data/policies.csv") %>% mutate(model = as.character(model))

plot_models <- 
  model_curves %>% 
  ggplot(aes(state, value, col=model, lty=model)) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_line(lwd=2, show.legend = FALSE) + 
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x")

plot_policies <- policies %>%
  ggplot(aes(states, escapement, col=model, lty=model)) + 
  geom_line(lwd=2) + xlab("state")


plot_models + labs(subtitle="A") +  plot_policies +  labs(subtitle="B")
```


<!-- Not just overfitting -->
Note that the mechanism shown here has nothing to do with the much more familiar issue of over-fitting, in which a better-fitting model will also lead to worse outcomes [@overfit].  In fact, an model which has been over-fit will gradually be rejected by either the iterative forecasting or adaptive management approaches shown here, as these approaches continually confront the models with new data to which they had not been previously fit.  The ability to avoid over-fitting is one of the greatest appeals of any iterative management strategy. But in the example here, both Model 1 and Model 2 have the same structural complexity.  The better decision performance of Model 1 is a consequence of capturing the key attribute needed for a good decision, which is perhaps a surprisingly an all-together different criteria than fit. 

<!-- Not just "get more models" -->
This case cannot be dismissed merely as being dealt a bad hand in having to pick only between Model 1 and Model 2. Model 1 approximates the key feature, giving nearly optimal outcomes.  Previous literature has underscored the importance of Knightian "unknown unknowns" two alternative models examined here fail to fully reflect our uncertainty in the underlying dynamics, and the consequences of underestimating model uncertainty are well understood []. Had we included the true model in the set of possibilities, the techniques illustrated would have had little difficulty in distinguishing it from Models 1 and 2 after sufficient iterations. In practice, we never have the true model, and even a much more thorough suite of candidate models, such as considering any possible values of $r$, $K$ (see Appendix B, Fig S1), or even among common alternative models with different structural forms, such as Beverton-Holt [@Beverton1957], Ricker [@Ricker1954], or Shepherd [@Shepherd1980], would fail to do any better, since all these models still have symmetric growth rates with a peak growth rate at half the steady-state population size).  Models will always be simpler than reality.  To insist that this issue can and will be avoided by always including more and better models of the process misses the point. 


<!--
So what?
- This is different than previous work: e.g. VOI is not zero, it's negative here.
- Model 1 is not a unicorn: we cannot rule out these situations in real world (particularly ecology where we don't actually do much iterative updating in practice)
- 
-->

Previous work has acknowledged that iterative learning may not always be feasible and that it may not always be beneficial, but has failed to recognize the possibility that it can potentially be detrimental.  For example, active adaptive management is premised on the observation that future observations may be too uninformative to distinguish between alternatives [@Walters1978].  Others have also noted that even big improvements in predictive accuracy could have negligible improvement on the decision outcomes, which motivates quantifying the Value of Information (VOI) [@Katz1987]. But in both cases, the worse outcome is wasted effort.  In the scenario considered here, the VOI is negative: any learning over the proposed model uncertainty leads to lower expected net utility than not learning.  


The only way to avoid this trap is to _forego learning_ with each new observation, to avoid iterative updates.  It may seem that the adaptive management approach fails because posterior probabilities are updated according to Bayes rule. Like iterative forecasting, this favors models which better predict the data, despite the fact that under the adaptive management approach, the overall optimization is conditioned on actual utility and not model fit.  However, this issue is not easily avoided.  For example, so-called greedy optimization techniques that favor actions with higher immediate reward do even worse in this context, since such an algorithm would obviously maximize the immediate harvest and therefore collapse the stock.  Only by comparing the net utility derived from management under model 1 for many iterations to the net utility derived under model 2 after many iterations can we deterime that model 1 leads to better outcomes.  This raises several questions.  Do such methods of learning and reducing model uncertainty ever lead us towards worse long-term outcomes in the real world? If so, How would we know, and what should we do about it?

<!-- **Is it just a unicorn?** -->
Do models which give nearly optimal performance while at the same time making wildly wrong predictions really exist?  It is tempting to argue that Model 1 is a kind of unicorn, a mythical creature existing only in theory. But that is a difficult assertion to prove.  Examples of serious forecasts that widely fail to predict future observations abound wherever forecasts are common, from elections to economics to environmental change [@].  If there is no shortage of bad forecasts, then could any of them be useful?  The premise that model need not perfectly capture reality to be useful is at the very heart of modeling.  Model 1 successfully captures the one key feature driving decisions in optimal harvest control problems: the stock size at which the maximum growth rate occurs [Fig 4a].  Capturing only the essential aspects as simply as possible is the goal of any model building exercise.  Thus it should be no surprise that a model can drive good decisions while making poor predictions.  It is also important to note that in many cases, any models being used in decision-making are not necessarily being subjected to the rigorous evaluation and updating steps proposed by adaptive management [@Walters1978] or iterative forecasting [@Dietze2018].  This makes it more likely that such models could persist in practice until now. While iterations that revise this models will no doubt improve decision outcomes in many case, it is worth bearing in mind from the example here that such a connection is not guaranteed.  

<!-- So what? -->
If these unicorns do exist in real world management, then what do we do about them? I believe this is an open question, but that our first step must be to recognize it as such.  We have seen that the problem cannot be resolved by more data, and is not the result of overfitting. Nor is "creating more models" the answer: when we have a model that is good enough to get optimal results, we cannot always insist on more models. We have also seen how not updating our uncertainty estimates, or doing so less frequently, can reveal a unicorn model before it is discounted by further iterations. Approaches such as iterative forecasting or adaptive management that can reduce model uncertainty over time remain promising and important techniques, but because value of information can be negative, we must revise model uncertainty only cautiously.  More importantly, we have seen that decision theory, such as the intuition offered by Fig 4, can help us better understand what features of a model are essential to decision outcomes and what are not.  Conversely, models which appear to lead to accurate predictions (like Model 2) can result in outcomes that are far from optimal. Building on such understanding, it may possible to identify strategies for learning that are more agnostic to the details of the model that are not important, or perhaps not reliant on model-based predictions at all. How that is done is an open question.  

<!-- Model free management -->
<!-- For example, a simple approach could gradually increase harvest from 0 as long as the observed recruitment increases relative to previous values.  This corresponds to a local search for the peak growth-rate from above. This strategy is optimal for the model considered here, at least under small noise, but is not robust to growth rates with local optima, nor is it optimal during a possibly-very-long transient period of the search.   -->



## Acknowledgements

The author acknowledges support from NSF CAREER Award #1942280 and helpful discussions with Melissa Chapman and Jeremy Fox.

 
<!--
Is this result merely an artifact of differing objectives?  @Gneiting2007's proper scoring rules are designed to select models which make the best forecast, and Model 2 really is the best choice for that task.  However, the adaptive management approach explicitly seeks to maximize expected utility, integrating over the uncertainty in the model, and yet it evolves away from Model 1 which would achieve that goal and settles on Model 2.  Note that this issue is not resolved by updating the learning step to reflect only the immediate reward -- it is in fact easy to see that so-called greedy algorithms would take the largest harvest possible and never reach the sustainable yield achieved by lower harvest rates of Model 1. To select Model 1 over alternatives like Model 2 which make better predictions but lead to worse outcomes requires a more long-term approach that forgoes iterative updating of probabilities as new data becomes available.  Note that this is the opposite recommendation of _adaptive_ management [@Walters1978] or _iterative_ forecasting [@Dietze2018].
-->





\pagebreak 


# References