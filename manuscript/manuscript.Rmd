---
title: 'Iterative forecasting and the trap of self-fulfilling prophecies'
author:
- name: Carl Boettiger
  email: cboettig@berkeley.edu
  affiliation: a,1
date: "`r Sys.Date()`"

abstract: |
  Encouraged by decision makers appetite for future information on topics ranging from elections to pandemics, and enabled by the explosion of data and computational methods, model based forecasts have garnered increasing influence on a breadth of decisions in modern society. Using a classic example from fisheries management, I demonstrate that selecting the model or models that produce the most accurate and precise forecast can lead to decidedly worse outcomes.  This creates a cycle of self-fulfilling prophecy, in which the manager not only over-exploits the fishery leading to declining biomass and declining yields, but also becomes increasingly convinced that these actions are not over-exploitation but in fact consistent with the best models and data for sustainable management. 

corresponding_author:
  code: 1
  text: "To whom correspondence should be addressed. E-mail: cboettig@berkeley.edu"

bibliography: refs.bib
address:
- code: a
  address: Department of Environmental Science, Policy, and Management, University of California, 130 Mulford Hall Berkeley, CA
    94720-3114, USA

keywords:
- forecasting,
- adaptive management,
- stochasticity,
- uncertainty,
- optimal control


## change to true to add optional line numbering
lineno: true
csl: ecology-letters.csl
output: rticles::elsevier_article
layout: 3p
journal: TBD
---


```{r, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"
```

```{r, include=FALSE}
rmarkdown::render("appendix.Rmd")
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, dev="cairo_pdf", fig.height=5, fig.width=7.5)
```

Global change issues are complex and outcomes are difficult to predict  [@Clark2001].
To guide decisions in an uncertainty world, researchers and decision makers may consider a range of alternative plausible models to better reflect what we do and do not know about the processes involved [@Polasky2011].
Forecasts or predictions from possible models can indicate what outcomes are most likely to result under what decisions or actions.
This has made model-based forecasts a cornerstone for scientifically based decision making.
By comparing outcomes predicted by a model to future observations, a decision maker can not only _plan_ for the uncertainty, but also _learn_ which models are most trustworthy. 
The value of iterative learning has long been reflected in the theory of adaptive management [@Walters1978] as well as in actual adaptive management practices such as Management Strategy Evaluation (MSE) [@Punt2016] used in fisheries, and is a central tenant of a rapidly growing interest in ecological forecasting [@Dietze2018].
But, do iterative learning approaches always lead to better decisions?

In this paper, I will demonstrate that iterative learning or iterative forecasting can also be counter-productive to management outcomes:
that in some cases, a manager would be better to ignore new information which could reduce uncertainty over available models.
Put another way, the value of information (VOI, as measured by the expected utility given that information minus the utility without it; see @Howard1966; @Katz1987), can actually be negative.
This is not a consequence of imperfect information (e.g. observation error) or the result of model over-fitting.
The issue is an intrinsic consequence of the fact that many models can lead to the same decisions, and no generic solution to the problem exists. 
Worse, whenever this situation arises, the decision-maker becomes trapped by a form of self-fulfilling prophecy: 
the model(s) which the decision-makers have learned to favor lead to decisions and outcomes that closely match model expectations.
Consequently, the decision-maker can become ever more firmly entrenched in the belief that their new models are accurate and the mediocre outcomes they experience are as good as it gets, while confidently rejecting earlier models which would have led to much better outcomes.
As this example will show, by better understanding what aspects of model are most and least important to shaping a particular decision, we can at least anticipate the circumstances that make this problem more likely.

Iterative learning approaches such as adaptive management or iterative forecasting [@Dietze2018] are particularly compelling because they address a different reason why seemingly-accurate models can lead to poor outcomes: the problem of over-fitting.
When models are estimated from the data they seek to describe, the model may over-fit the data, making the data seem even more likely than it would be under the true process [@Burnham1998].
Over-fit models may underestimate uncertainty and make poor predictions about future outcomes [@Ginzburg2004].
Because any iterative learning compares models to future outcomes, overfitting those outcomes is impossible.
This concept was rigorously formalized by @Gneiting2007's proof of "proper" scoring rules.
By definition, under a proper scoring rule, no probabilistic prediction $Q(x)$ can score better, on average, than that of the underlying process, $P(x)$.
@Gneiting2007 further shows which techniques for evaluating forecast predictions do an do not satisfy this condition, i.e. which cannot be over-fit.
Adaptive management is based on equally secure footing because model predictions are again compared to future outcomes not used to fit the model.
Instead of scoring and selecting among alternative models, adaptive management considers probabilities over all models [@Walters1978].
These two approaches are particularly compelling because they can both reduce uncertainty and avoid over-fitting.
My example is not meant to undercut the importance of these approaches.
Previous work has long acknowledged the panelope of ways in which iterative learning or other model-based decision making can go astray due to conflicting incentives implementation errors, or lack of resources for monitoring and updating [e.g. @Ludwig1993].
Here, the problem is more fundamental. 

I illustrate this problem using an example from fisheries management. 
Fisheries are a significant economic and conservation concern world wide and their management remains an important debate [e.g. @Worm2006; @Worm2009; @Costello2016].
Moreover, their management has been a proving grounds for theoretical and practical decision-making issues which are widely applicable in other areas of ecology and conservation [@Ludwig1993; @Lande1994], and one that has long wrestled with issues of uncertainty in the context of management decisions [e.g. @Clark1973; @Reed1979; @Walters1981; @Ludwig1982].
While modern fisheries management frequently relies on complex models which may contain scores of parameters to reflect the specific age or stage structure of a specific fish stock [@ramlegacy2018; @ram], I will rely on simple, well-studied models which permit greater intuition and generalization [@Levins1966].
Consistent with such previous work [@Schaefer1954; @Clark1973; @Reed1979; @Walters1981; @Ludwig1982; @Costello2016], let us consider the problem of determining the optimal harvest policy given a measurement of the current stock size. 

For illustrative purposes, we will focus on the simplest example of model uncertainty: two alternative models of equal complexity but differing in the estimated value of certain parameters.
While some authors distinguish between model uncertainty and parameter uncertainty, I will refer collectively to any uncertainty that arises from our imperfect knowledge of the system as "model uncertainty," because the distinction is largely a consequence of notation rather something more intrinsic.
For example, two structurally different models, $f(x) = a x$ and $f(x) = a x^2$ can be considered merely different parameterizations of $f(x) = a x^b$, or vice versa.


# Ecological Models

The sustainable harvest decision problem can be stated as follows: The fish stock is observed to be in state $X_t$ at time $t$, and is then subjected to some harvest $H_t$ before recruiting new fish, subject to stochastic environmental noise $\xi_t$, to bring the stock to $X_t+1$,

\begin{equation}
X_{t+1} = f(X_t - H_t, \xi_t) 
\end{equation}

A manager seeks each year to select the harvest quota $H_t$ which will maximize the sum
of the utility derived from such a harvest and such a state, $U(X_t,H_t)$, over all time,
subject to discount rate $\delta$ [@Clark1973]:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t \label{utility}
\end{equation}

For simplicity and comparison with prior work, we will assume a fixed price of fish $p$ with no additional cost on additional harvest, $U(X_t, H_t) = p \min(H_t, X_t)$ (noting that realized harvest cannot exceed the stock size). Without loss of generality we will set the price $p = 1$ and modest discount $\delta = 0.99$.

We further imagine that the function $f$ is not known precisely.
We restrict ourselves to two simple candidate models $f_1$ and $f_2$.
Both share the same underlying structure of logistic recruitment (known as the Gordon-Schaefer model in fisheries context owing to groundbreaking work independently by @Schaefer1954 and @Gordon1954), differing only in their choice of certain parameters:

\begin{equation}
f_i(Y) = Y + r_i Y \left( 1 - \frac{Y}{K_i} \right) * \xi_t(\sigma)
\end{equation}

Let us assume $\xi_t(\sigma)$ represents log-normal random noise with mean of unity and log-standard-deviation $\sigma_i$ for each model.
Model 1 is given by $r_1 = 2$, $K_1 = 16$, $\sigma_1 = 0.05$, Model 2 by $r_2 = 0.5$, $K_2 = 10$, $\sigma = 0.075$ (in dimensionless units).
Having both the larger growth rate and the larger carrying capacity, model 1 is clearly the more optimistic of the two choices. 

Selecting between model 1 and model 2 can thus be considered the simplest illustration of the model uncertainty problem.
This is a subset of the more general problem of selecting model parameters, assuming a logistic growth, which itself is a subset of estimating the best structural from (e.g. Ricker, Beverton-Holt, etc).
There is no need to consider these more complicated versions of the model uncertainty problem here, since they all inherit the same issue.
Reducing the model selection problem to these two models simplifies the presentation and will aid intuition at no loss of generality.

The only additional assumption we will need is that the "true" model is not among the suite of models under consideration.
Mathematical models are, at best, approximations of the underlying processes.
Ecological processes are much too complex to ever be modeled exactly.
For illustrative purposes, we will thus assume the "true" process to be given by Model 3, which is unknown to the decision-maker, but similar enough to at least one of the candidate models might be considered a reasonable approximation:

\begin{equation}
f_3(Y) = Y + r_3 Y^4 \left( 1 - \frac{Y}{K_3} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$. 


The task of deciding whether Model 1 or model 2 would be the better choice for decision making is thus perhaps the simplest example of the much studied issue of model uncertainty that we can pose. 
As in any real world scenario, neither model is the true model, but nevertheless this model set contains a good enough approximation of the true model to make good decisions.
However, any of the well-developed approaches for decision-making under model uncertainty will prefer model 2 over model 1, despite the fact that the optimal policy under model 2 leads to much worse outcomes ecologically and economically.


# Methods for Managing Under Model Uncertainty 

I will use this example to illustrate two alternative approaches for iterative learning over model uncertainty: iterative forecasting and adaptive management.
The central difference in the approaches is that iterative forecasting is premised on the ability to score the predictions of alternative models.
Iterative forecasting is silent on the issue of what to do with those scores, this is left up to the decision-maker.
Adaptive management approaches, by contrast, explicitly seek to integrate probabilities over all candidate models to reach a decision.
I consider each in turn.

## Statistical approaches: Forecasting under "Proper" Scoring Rules

Like many decision problems, the task of setting a sustainable harvest quota appears to hinge on having an accurate forecast: if we can predict to what size the fish stock will increase next year, $X_t+1$, and we know the current stock, $X_t$, then we can sustainably harvest $X_{t+1} - X_t$ without decreasing the biomass over the long term.
Selecting a model based on forecast skill is also justifiable on theoretical grounds, since it reduces the risk of over-fitting by comparing model predictions to later observations that were not used to estimate the model [@Gneiting2014].

I illustrate the process of model selection by strictly proper scoring rules using two scenarios.
In Scenario A (passive observation) the fish stock is unharvested and allowed to recover towards carrying capacity (as simulated under our "true" model, Model 3) while comparing the observed stock size in each subsequent time step to the distribution predicted under model 1 and model 2 respectively [Fig 1].
The mean, $\mu_t$ and variance, $\sigma_t$ of the forecast are compared against the true observation $x_t$ using a proper scoring rule given by @Gneiting2007, 

\begin{equation}
S(x_t|\mu_t,\sigma_t) = -(\mu_t - x_t )^2 / \sigma_t^2  - \log(\sigma_t) \label{proper}
\end{equation}

for each prediction over 100 replicate simulations of 100 time steps each [Fig 1].



```{r figure1, fig.cap = "Forecast performance of each model.  Panels A, B: Step ahead predictions of stock size under unfished (A) and fished (B) scenarios. Error bars indicating the 95% confidence intervals around each prediction, while stars denote the observed value in that year.  Because the models make different decisions each year in the fished scenario, the observed stock size in year 2, 3, etc under the management of model 1 (blue stars) is different from that under model 2 (red stars).  Panels C, D: corresponding distribution of proper scores across all predictions (100 replicates of 100 timesteps). Higher scores are better, confirming that model 2 makes the better forecasts."}
predictions <- read_csv("../data/predictions.csv") %>% mutate(model = as.character(model))

fig1cd <- predictions %>%
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 2, show.legend = FALSE) +
  coord_cartesian(xlim = c(-100, 1), ylim = c(0,4000)) +
  xlab("Proper score") +
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "C.",
    B_fished = "D."))) + 
  scale_x_continuous(breaks = c(-100, 0)) +
  theme(axis.text.y = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )



 fig1ab <- predictions %>% 
  filter(reps == "2", time < 10) %>%
  ggplot(aes(time, col = model, fill = model)) + 
  geom_point(aes(y = expected)) + 
  geom_errorbar(aes(ymin = low, ymax = high)) +
  geom_point(aes(y = true), pch = "*", size = 12, 
             alpha = 0.6) + 
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "A. Without fishing",
    B_fished = "B. Managed harvest"))) +
  ylab("stock size")
 
 
 
 fig1ab + fig1cd + 
  plot_layout(widths =  c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```


In Scenario B (actively harvest), I have first solved for the optimal management strategy for both model 1 and model 2 [Fig 1b].
For small noise and concave functions with linear reward structure this can be done analytically [see proof in @Reed1979], or solved more generally by stochastic dynamic programming [see review by @Marescot2013, details in Appendix].
Under this scenario, replicate simulations of the stock are harvested at each time step using the optimal quota dictated by either model 1 and model 2.
The resulting stock sizes in the time-step following this harvest are once again compared to the probabilities predicted by each model using Eq \eqref{proper}.
Model 2 unequivocally outperforms model 1 in both scenarios of passive observation and active harvest.

Despite the clearly superior predictive accuracy of model 2 in both scenarios, the outcomes from management under model 2 are substantially worse.
We can assess such outcomes in less abstract terms than forecasting skill, such as economic value (in dollars) or the ecological value (unharvested biomass).
In our simple formulation of the decision problem, the "utility" the manager seeks to maximize is simply the economic value (net present value: the discounted sum of all profits from future harvests, Eq \eqref{utility}) of harvested fish.
This formulation ignores any utility provided by fish that are not harvested, beyond their contribution to future potential harvests.
While it is possible to include such contributions directly in the utility function being optimized [e.g. @Halpern2013], even without doing so, model 1 maintains both a higher unharvested biomass and also leads to higher economic returns throughout [Fig 2].


```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast. Harvest quotas derived from model 1 result in a significantly higher fish stock size than under Model 2 (panel A). Economic returns under model 1 are also substantially higher (panel B)"}

sims <- read_csv("../data/sims.csv")
npv_df <- read_csv("../data/npv_df.csv")


plot_ecology <- 
  sims %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  group_by(model, time) %>%
  summarise(mean_state = mean(state), sd = sd(state), .groups = "drop") %>%
  filter(time < 25, model != "3") %>%
  ggplot(aes(time, mean_state)) + 
  geom_line(aes(col = model), lwd=1.5, show.legend = FALSE) +
    geom_ribbon(aes(ymin = mean_state - 2*sd, 
                    ymax = mean_state + 2*sd,
                    fill = model),
                alpha = 0.2, show.legend = FALSE) +
  ylab("state") + 
  labs(subtitle = "A. Ecological outcomes")

optimal <- select(filter(npv_df, model == "3"), time, mean_npv)

plot_econ <- 
npv_df %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  filter(model != "3", time %in% seq(1,100, by = 5)) %>%
  ggplot(aes(time, mean_npv)) +
  geom_line(data = optimal, lwd = 1.5, col = "grey20") +
  geom_point(aes(col=model), size = 4, alpha = 0.8) + 
  ylab("Net present value") + xlab("time")  + 
  labs(subtitle = "B. Economic outcomes")


plot_ecology + plot_econ
```

In both scenarios, the careful comparison through proper scoring rules has led us to select the worse-performing model.
Crucially, a manager operating under this selection would have little indication that their model was flawed: both future stock sizes and expected harvest yields consistently match model predictions.
Had we been able to include Model 3 in our forecast comparisons, it would equal or outperform the forecasting skill of both model 1 and model 2 (as guaranteed by the theorem of @Gneiting2007), while also matching or out-performing their economic utility (as guaranteed by the theorem of @Reed1979).
In practice, we never have access to the generating model, so it is reasonable to expect model selection to determine the best approximation.
As we see here, the best approximation for forecasting future states does not in fact lead to better outcomes.




One obvious limitation in this comparison is that scenario B treats each model as fixed over the entire course of the simulation.
It would be possible to generate forecasts for the next $n$ steps, but these forecasts would also be conditional on the management action (i.e. harvest quota) selected at each step.
To evaluate two-step-ahead predictions we must consider each possible action under each possible state predicted by the step-ahead forecast, weighted by probability of the state given the model and the probability of the model.
This rapidly expanding set of possibilities is addressed by adaptive management for sequential decision problems [e.g. @Smith1981], which I employ in the next section.

## Decision-Theoretic Approaches

Any adaptive management strategy updates posterior distributions over model uncertainty [@Punt2016; @Ludwig1982].
Unfortunately, any such adaptive updating leads to worse outcomes than the equivalent non-adaptive strategy, in which model uncertainty is held fixed.
I illustrate the application of a passive adaptive management strategy to this simple example, following classic examples for parameter [@Ludwig1982] or structural [@Smith1981] model uncertainty.
Passive adaptive management for a simple sequential decision problem is straight forward to implement over a discrete set of states and actions using dynamic programming with iterative updates [@Smith1981, example code in Appendix].
To demonstrate that the behavior is not driven by failure to explore sufficiently, (which might be addressed by an active adaptive management), I will assign initial probability that model 2 is true at 1%.
After a single iteration of learning, these probabilities are completely reversed, with the manager deciding that model 2 is almost certainly correct  [Fig 3A].
I compare these results to 
As before, this results in a management practice with much worse ecological and economic outcomes than would have been realized by a manager who stubbornly clung to model 1 without updating, which achieves a net present value that only 31% that expected under management using model 1 alone [Fig 2]. 

```{r figure3, fig.cap="Adaptive management under model uncertainty.  Solid lines trace the trajectories of the state (fish stock, circles) and action (harvest quota, triangles), under adaptive management (learning).  Dotted lines trace the corresponding trajectories if iterative learning is omitted, leaving the prior belief fixed throughout the simulation (planning).   Color indicates the belief that model 1 is correct (blue), with an intial prior belief of 99%.  Panel A: Management over the two candidate models, Model 1 and Model 2. Within a single iteration of adaptive management, the belief over models switches from a prior belief that heavily favored model 1 to a posterior that favors model 2 with near certainty.  Future iterations reinfornce the belief in model 2, resulting in both depressed harvests and low stock sizes (solid lines).  If no iterative learning updates are performed, stock sizes and realized harvests (and thus economic profit) are both higher. Panel B: given 42 candidate models over a broad range of parameter values, adaptive management quickly reduces the probability on model 1, and substantially underperforms management without learning (dotted lines). While outcomes improve marginally relative to the two-model case (figure A) they remain significantly worse than had no iterative learning been included."}
am <- read_csv("../data/am.csv")
fig3a <- am %>%  filter(time <=20) %>% 
  pivot_longer(c(stock, quota), values_to="stock", names_to="key") %>%
  mutate(key = factor(key, levels = c("stock", "quota"))) %>%
  ggplot(aes(time, stock, shape = key, group = interaction(method,key))) + 
  geom_line(aes(lty=method), col = "gray40", show.legend = FALSE) + 
  geom_point(aes(col = belief), size=2, show.legend = FALSE) +
  scale_colour_gradient(limits = c(0,1), low = pal[4], high = pal[1]) +
  ylab("fish stock") + labs(subtitle="A.")

am_multi <- read_csv("../data/am_multi.csv")
fig3b <- am_multi %>% 
  pivot_longer(c(stock, quota), values_to="stock", names_to="key") %>%
  mutate(key = factor(key, levels = c("stock", "quota"))) %>%
  ggplot(aes(time, stock, shape = key, group = interaction(method,key))) + 
    geom_line(aes(lty=method), col = "gray40") + 
    geom_point(aes(col = belief), size=2) +
    scale_colour_gradient(limits = c(0,1), low = pal[4], high = pal[1]) +
    ylab("fish stock") + labs(subtitle="B.")


fig3a + fig3b
```


Increasing the space of possible models to cover a whole plausible range of parameters $r$ and $K$ does little to resolve this problem [Fig 3B]. 
Iterative updates again quickly dismiss the parameter values assumed by model 1, though with more options to choose from, this probability is spead over a range of seemingly plausible candidate models instead of a single alternative model (see Appendix).
While the adaptive management of additional actions and observations slowly narrow this subset of plausible models, decisions based on this uncertainty prevent fish stocks from recovering fully, and realize lower harvests as a result.
Note that learning under either adaptive management approach (using two models or 42), the decision-maker becomes ever-increasingly convinced that they are using the right model or models.
Future stock sizes fall consistently in the range predicted by the model(s), and consistently outside the range predicted by model 1.
Consequently, each iteration the managers are only more firmly convinced that they are maintaining the fish stock near the biomass that supports maximum sustainable yield, when in fact they are sustaining a harvest regime that is preventing recovery of the stock to the much higher productivity regime which would have been achieved under model 1.


# Discussion


<!-- quick summary of what was demonstrated -->
Given this simple decision problem in which one of the two models leads to effectively optimal ecological and economic outcomes, current approaches invariably choose the other.
Moreover, a decision maker employing an adaptive management or iterative forecasting assessment such as those considered here would have no way of realizing that the outcomes they experienced were in fact sub-optimal.
In both approaches, the manager quickly concludes that model 1 is entirely implausible, while finding that model 2 is remarkably accurate at predicting future values.
This problem is not addressed by re-estimating parameters (equivalently, considering a larger suite of models spanning a wide range of possible $K$ and $r$ values): which improves only marginally over the outcome of considering only two models [Fig 3B].
Both iterative forecast evaluation and the adaptive management approaches to reducing model uncertainty lead to management becoming trapped in a self-fulfilling prophecy:
holding the fish population at an unproductively low biomass suggested by model 2 while becoming ever more convinced that model is suitable. 
Without access to the true model, none of our methods for model choice or reducing model uncertainty can break this trap.
This should not be mistaken as a critique of adaptive management or iterative forecasting specifically -- other model choice approaches such as goodness-of-fit, information criteria or cross-validation would all prefer model 2 as well. 


<!-- Intuition, Figure 4 -->
The reason for model 1's seemingly contradictory ability to make good decisions but bad forecasts becomes obvious once we compare both curves to that of the underlying model, model 3.
Plotting the growth rate functions of each model, [Fig 4A], it is hardly surprising that no method exists which would not prefer the closely overlapping model 2 to the no-where-close model 1 as the better approximation of model 3.
Nevertheless, decisions based on model 1 are nearly indistinguishable from those based on the true model [Fig 4B], while model 2 leads to over-harvesting.
The explanation comes from noticing that the stock size corresponding to the maximum growth rate under model 1 (the peak of the curve) falls at almost exactly the same stock size as that of the peak growth rate for model 3.
Meanwhile, the peak of model 2 occurs at a substantially lower stock size.
While the optimal control solution appears to depend only on a step-ahead forecast accuracy (indeed, the SDP solution method used here takes only step-ahead forecast probabilities as input, [@Marescot2013]), mathematical analysis showed long ago [@Reed1979] that the optimal solution for this problem depends only on keeping the biomass at the value responsible for the maximum growth rate.
This realization is quite general: for most decision problems, simple models will exist under which the optimal decision is the same as it would be for the true model, even when that simple model is wrong in most other ways.

<!-- Intuition for it's not just this example -->
This phenomenon is not unique to sequential decision problems or optimal control solutions.
For example, Management Strategy Evaluation [MSE; @Punt2016] in fisheries seeks to evaluate pre-specified strategies rather solve for the best possible strategy through optimal control.
That approach is well justified -- optimal control techniques such as stochastic dynamic programming illustrated here quickly become intractable for more complex models [@Marescot2013] typically used in fisheries stock assessments.
Constraining the search to only strategies that impose a constant mortality (defined as harvest per unit biomass, $F = H/B$) means we only have to evaluate those $N$ strategies each iteration, not solve a sequential decision problem.
Doing so, we would find the best constant-mortality solution under model 1 performs much worse than the best constant-mortality solution under model 2.
Does this mean MSE is not susceptible to this issue?
No, it does not.
It is just as easy to construct an alternative model 1 with the same properties of leading to nearly optimal decisions while being rejected by any method of iterative learning, such as forecasting or adaptive management.
Just as the optimal control policy for a Gordon-Schaefer (logistic) model depends only on parameter $K$ [@Reed1979], the optimum constant escapement policy depends only on parameter $r$ [@Schaefer1954].
If model 1 has a value of $r$ such that it happens to match the best possible constant-mortality solution for the (unobserved) true model, then $K$ can be set arbitrarily high, ensuring the any model selection, forecasting, or adaptive management approach would lead away from model 1 and towards worse-performing models once again.
Many optimization problems share this feature in which the optimal policy depends only on a subset or ratio of model parameters, such that it is usually easy to find a similar model 1.
This example also illustrates the importance of decision constraints -- a model that gives very good outcomes when we are free to vary the havest quota each year (the optimal control problem here) may give very poor results under the constraint of constant mortality, and vice versa. 



```{r figure4,  fig.cap="Panel A: Population growth curves of each model. The positive equilibrium of each model occurs where the curve crosses the horizontal axis. Note that while Model 2 is a better approximation to the truth (Model 3), Model 1 better approximates the stock size which leads to maximum growth.  Panel B: The optimal control policy under Model 1 is nearly identical to that under the true Model 3, while the optimal policy under Model 2 supresses stock to a much lower escapement level."}
# Fig 4
model_curves <- read_csv("../data/model_curves.csv") %>% mutate(model = as.character(model))
policies <- read_csv("../data/policies.csv") %>% mutate(model = as.character(model))

plot_models <- 
  model_curves %>% 
  ggplot(aes(state, value, col=model, lty=model)) +
  geom_hline(aes(yintercept = 0), lwd=1) + 
  geom_line(lwd=2, show.legend = FALSE) + 
  coord_cartesian(ylim = c(-5, 8), xlim = c(0,16)) +
  ylab(bquote(f(x) - x)) + xlab("x")

plot_policies <- policies %>%
  ggplot(aes(states, escapement, col=model, lty=model)) + 
  geom_line(lwd=2) + xlab("state")


plot_models + labs(subtitle="A") +  plot_policies +  labs(subtitle="B")
```


<!-- Not just overfitting -->
Note that the mechanism shown here has nothing to do with the much more familiar issue of over-fitting, in which a better-fitting model will also lead to worse outcomes [@Burnham1998].
In fact, an model which has been over-fit will gradually be rejected by either the iterative forecasting or adaptive management approaches shown here, as these approaches continually confront the models with new data to which they had not been previously fit.
The ability to avoid over-fitting is one of the greatest appeals of any iterative management strategy.
In the example here, both Model 1 and Model 2 have the same structural complexity.
The better decision performance of Model 1 is a consequence of capturing the key attribute needed for a good decision, which is perhaps a surprisingly an all-together different criteria than fit.
Note that other methods of model selection not considered here, including goodness-of-fit metrics such as $r^2$  or information criteria [@Burnham1998], will all prefer Model 2 over Model 1 for the same reason.

<!-- Not just "get more models" -->
This case cannot be dismissed merely as being dealt a bad hand in having to pick only between model 1 and model 2.
Model 1 approximates the key feature, giving nearly optimal outcomes.
Previous literature has underscored the importance of Knightian "unknown unknowns" two alternative models examined here fail to fully reflect our uncertainty in the underlying dynamics, and the consequences of underestimating model uncertainty are well understood [@Polasky2011; @Wintle2010].
Had we included the true model in the set of possibilities, the techniques illustrated would have had little difficulty in distinguishing it from models 1 and 2 after sufficient iterations.
In practice, we never have the true model.
We have also seen that including a much larger suite of possible values of $r$, $K$ [Fig 3B] still leaves us far from the optimal performance of model 1 alone.
Would a more comprehensive grid of $r$ and $K$ values, or the consideration common alternative models with different structural forms, such as Beverton-Holt [@Beverton1957], Ricker [@Ricker1954], or Shepherd [@Shepherd1980], succeed in closing that gap?
No it would not, as the comparison to the true model makes clear [Fig 4].
Regardless of parameter value, all of these models still have symmetric growth rates with a peak growth rate at half their steady-state population size.
By contrast, the true model has a peak growth rate much closer to the steady-state size.
Because the decision in this problem is driven by the location of the peak growth rate [@Reed1979], any symmetric model which places its peak at the correct stock size also overestimates the carrying capacity, as is the case with our Model 1.
If such additional structurally different models are not enough, then perhaps more complex models would serve?
Models will always be simpler than reality.
To insist that this issue can and will be avoided by always including more and better models of the process misses the point. 


The potential for long-term deterioration in management outcomes due to adaptive learning has not been widely acknowledged before.
Certainly the limitations of any modeling approach to improve management outcomes due to the human context of competing interests, imperfect enforcement, etc., is readily appreciated [@Ludwig1993].
The problem here goes deeper, to the limits of models and learning from data in even in a toy example free from other real world challenges.
Previous work has acknowledged that iterative learning may not always be feasible and that it may not always be beneficial [@Polasky2011], but has failed to recognize the possibility that it can potentially be detrimental.
For example, active adaptive management is premised on the observation that future observations may be too uninformative to distinguish between alternatives [@Walters1978].
Others have also noted that even big improvements in predictive accuracy could have negligible improvement on the decision outcomes, which motivates quantifying the Value of Information (VOI) [@Katz1987].
But in both cases, the worse outcome is wasted effort.
In the scenario considered here, the VOI is negative: any learning over the proposed model uncertainty leads to lower expected net utility than not learning.


Worse, absent the counterfactual of non-adaptive management to compare to, this detioriation in outcomes is invisible. 
All the information available to our manager reinforces the conviction that this deteriorated ecological state is in fact the natural, sustainably managed equilibrium of our fish population.
By responding in any way to future observations to reduce model uncertainty, our manager is caught in a trap of self-fulfilling prophecy:
believing ever more firmly in models that make more accurate predictions while driving decisions that prevent the fish stock from recovering.
The only way to avoid this trap is to _forego learning_ with each new observation, to avoid iterative updates.
It may seem that the adaptive management approach fails because posterior probabilities are updated according to Bayes rule.
Like iterative forecasting, this favors models which better predict the data, despite the fact that under the adaptive management approach, the overall optimization is conditioned on actual utility and not model fit.
However, this issue is not easily avoided.
For example, so-called greedy optimization techniques that favor actions with higher immediate reward do even worse in this context, since such an algorithm would obviously maximize the immediate harvest and therefore collapse the stock.
Only by comparing the net utility derived from management under model 1 for many iterations to the net utility derived under model 2 after many iterations can we deterime that model 1 leads to better outcomes.
This raises several questions.
Do such methods of learning and reducing model uncertainty ever lead us towards worse long-term outcomes in the real world? If so, How would we know, and what should we do about it?

<!-- **Is it just a unicorn?** -->
Do models which give nearly optimal performance while at the same time making wildly wrong predictions really exist?
It is tempting to argue that model 1 is a kind of unicorn, a mythical creature existing only in theory. But that is a difficult assertion to prove.
Examples of serious forecasts that widely fail to predict future observations abound wherever forecasts are common, from elections to economics to environmental change [e.g. @Tetlock2015].
If there is no shortage of bad forecasts, then could any of them be useful?  The premise that model need not perfectly capture reality to be useful is at the very heart of modeling. 
Model 1 successfully captures the one key feature driving decisions in optimal harvest control problems: the stock size at which the maximum growth rate occurs [Fig 4A].
Capturing only the essential aspects as simply as possible is the goal of any model building exercise.  Thus it should be no surprise that a model can drive good decisions while making poor predictions.
It is also important to note that in many cases, any models being used in decision-making are not necessarily being subjected to the rigorous evaluation and updating steps proposed by adaptive management [@Walters1978] or iterative forecasting [@Dietze2018].
This makes it more likely that such models could persist in practice until now. While iterations that revise this models will no doubt improve decision outcomes in many case, it is worth bearing in mind from the example here that such a connection is not guaranteed.

<!-- So what? -->
If these unicorns do exist in real world management, then what do we do about them?
I believe this is an open question, but that our first step must be to recognize it as such.
We have seen that the problem cannot be resolved by more data, and is not the result of overfitting.
Nor is "creating more models" the answer: when we have a model that is good enough to get optimal results, we cannot always insist on more models.
We have also seen how not updating our uncertainty estimates, or doing so less frequently, can reveal a unicorn model before it is discounted by further iterations.
Approaches such as iterative forecasting or adaptive management that can reduce model uncertainty over time remain promising and important techniques, but because value of information can be negative, we must be careful not to update models without a keen understanding of what aspects of a model drive a decision [Fig 4].
We have seen that  the intuition offered by decision theory [Fig 4] can help us better understand what features of a model are essential to decision outcomes and what are not.
Conversely, models which appear to lead to accurate predictions (like Model 2) can result in outcomes that are far from optimal.
Building on such understanding, it may possible to identify strategies for learning that are more agnostic to the details of the model that are not important, or perhaps not reliant on model-based predictions at all.
How that is best done in general is an open question.


## Acknowledgements

The author acknowledges support from NSF CAREER Award #1942280 and helpful discussions with Melissa Chapman and Jeremy Fox.

\pagebreak 


# References