---
title: 'Appendix for: "Bad Forecast, Good Decision"'
date: "`r Sys.Date()`"
author: Carl Boettiger
journal: Appendix
layout: 3p
bibliography: refs.bib
#output:
#  tufte::tufte_handout:
#    dev: cairo_pdf
#    latex_engine: xelatex
output: 
  hrbrthemes::ipsum_pdf:
    dev: cairo_pdf
    latex_engine: xelatex

---



This appendix provides a fully reproducible walk-through of the implementation of the analyses considered in this paper.  None of the methods applied here: Stochastic Dynamic Programming, Iterative Forecasts assessed with Proper Scoring Rules, or Adaptive Management, are novel 

This appendix includes computer code in the R language, which can also be found in the corresponding R-Markdown document in the paper's GitHub repository, <https://github.com/cboettig/bad-forecast-good-decision>.  While computer code can be more verbose and difficult to read than mathematical formulae presented here, it is also less ambiguous.  


The question of optimal harvest given a population growth model can be posed as a Markov Decision Process (MDP). @Marescot2013 provides an accessible overview of MDP problems and their solutions in conservation context.  @Reed1979 provides a formal proof of optimal harvest strategy for stochastic dynamics under the assumption of any concave growth function. The problem can be summarized as follows:

A manager seeks to maximize the sum of the utility derived from such a harvest
and such a state, $U(X_t,H_t)$, over all time, subject to discount rate $\delta$:

\begin{equation}
\sum_{t=0}^{t=\infty} U(X_t, H_t) \delta^t
\end{equation}

While in principle the utility could reflect many things, including the cost of fishing, market responses to supply and demand, the value of recreational fishing, the intrinsic value fish left in the sea [see @Halpern2013], for simplicity we will assume utility is merely a linear function of the harvest quota set by the manager, i.e. a fixed price $p$ per kilogram of fish harvested: $U(X_t, H_t) = p \min(H_t, X_t)$ (noting that realized harvest cannot exceed the available stock). As units are already specified in dimensionless quantities in this example, so without loss of generality we will set $p=1$. 

To solve this optimization, we must also know the dynamics of $X_t$, describing how future stock $X_{t+1}$ changes from the present state, $X_t$, after to harvest $H_t$:

\begin{equation}
X_{t+1} = f(X_t, H_t)
\end{equation}

This problem is already well studied, and it is worth noting that even under the simple utility function which assigns no value to fish that are not harvested, the optimal strategy still seeks to sustain the fish population indefinitely; as @Clark1973 shows for the deterministic function $f$ and @Reed1979 extended to the stochastic case.  As @Reed1979 proves, the optimal strategy can be characterized by a policy of "constant escapement," in which the manager adjusts the harvest effort each year in an effort to ensure the same number or biomass of fish are left in the sea each year.  

The optimization problem can then be solved by dynamic programming, using the recursive Bellman equation as, 

\begin{equation}
\end{equation}

This technique, known as stochastic dynamic programming (SDP) is widely used in conservation decision problems [e.g. @Mangel1985, @Marescot2013].

It is worth noting that while constant escapement is used in the management of several important fish stocks such as salmon, most marine fisheries are not managed using this optimal solution for a fluctuating population, but instead rely on a 'constant harvest effort' (or constant "yield") policy.  In the case of a deterministic model at equilibrium, these policies are identical.  Though constant-effort policies are technically sub-optimal for the problem as stated above with stochastic growth, in practice they can be more robust, such as when measurements of the current stock size $X_t$ are uncertain.  These issues are discussed in detail in @pomdp-intro.  For our purposes, it suffices to note that constant escapement policy is relevant both from a theoretical standpoint as the optimal solution for the problem under consideration, and an applied standpoint as a policy that guides management of many salmon fisheries.  The optimal control approach using SDP as illustrated here is used in a wide and growing number of conservation problems.  

Bear in mind that in the example presented here, SDP merely serves the role of a convenient and established way for a manager to determine the optimal action, given some model estimation.  Whether we use SDP or some other way to determine the optimal policy, given the model, is immaterial to the conclusion.  








To solve the decision problem using SDP, we define the state space and the action space on a discrete grid of 240 points.  The maximum state is set well above the largest carrying capacity used in the models, which limits the influence of boundary effects introduced by the transformation to a discrete, finite grid.  Available harvest actions match the state space, effectively allowing any harvest level to be possible.  

```{r model_definitions}
states <- seq(0,24, length.out = 240)
actions <- states
obs <- states
```

The utility (reward) of an action is set to the (realized) harvest (e.g.
a fixed price per unit fish, with no cost applied to harvest effort).  Future
utility is discounted by fixed factor of $0.99$. Classic maximum sustainable 
yield models [@Schaefer1954] ignore discounting, while modern economic optimization
models insist on it, so a small discount conforms to the latter while reasonably
approximating the former.  The qualitative conclusion is not sensitive to the 
discounting rate. 

```{r}
reward_fn <- function(x,h) pmin(x,h)
discount <- 0.99
```

Alternate reward functions with varying price and cost structures are possible but do not qualitatively impact the conclusions.  This reward function is a limiting case of any more complex reward, and corresponds both to classic work that does not model utility explicitly [e.g. MSY theory, @Schaefer1954], as well as explicit assumptions typically made in more recent models [@Reed1979]. Moreover, using a simple reward function makes it clear that model that performs best is not doing so merely because of particular features baked into the a carefully chosen reward rule.  


# Ecological Models

The manager chooses between two different logistic growth models, each of which
can be thought of as an approximation to the underlying "true" population model:

\begin{equation}
f_i(Y) = Y + Y r_i \left( 1 - \frac{Y}{K_i} \right) \xi_i(t)
\end{equation}

where $\xi_i(t)$ is a multiplicative log-normal noise term with mean 1 and log-standar-deviation $\sigma_i$.  

Model 1 has $r_1 = 2$, $K = 16$, and $\sigma_1 = 0.05$.
Model 2 has $r_2 = 0.5, K_2 = 10$, and $\sigma_2 = 0.075$.

Meanwhile, the true population growth rate is simulated using a function with
non-linear per-capita growth:

\begin{equation}
f_i(Y) = Y + Y^4 r_i \left( 1 - \frac{Y}{K_i} \right)
\end{equation}

with $r_3 = 0.002$, $K_3 = 10$ and $\sigma_3 = 0.05$.



Here we provide annotated code necessary to completely reproduce all of the analysis presented in the main paper. This analysis is run in R [@R] uses `MDPtoolbox` [@MDPtoolbox] for solving Markov Decision Processes (MDP) using stochastic dynamic programming functionality, `expm` for matrix exponentials [@expm], and a few custom MDP functions provided by our package, `mdplearning` [@mdplearning].  We will also use `tidyverse`  packages for basic manipulation and plotting [@tidyverse]. This file is also available as an RMarkdown document [@rmarkdown] at  <https://github.com/cboettig/bad-forecast-good-decision>.


```{r graphics_setup, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"

knitr::opts_chunk$set(cache=FALSE, tidy = "styler", message = FALSE, warning = FALSE, echo = TRUE)

```

```{r setup, message=FALSE}
library(tidyverse)
library(MDPtoolbox)
library(expm)
# remotes::install_github("boettiger-lab/mdplearning")
library(mdplearning)
```




```{r}
# K is at twice max of f3; 8 * K_3 / 5
f1 <- function(x, h = 0, r = 2, K = 10 * 8 / 5){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
}
f2 <- function(x, h = 0, r = 0.5, K = 10){
  s <- pmax(x - h, 0)
  s + s * (r * (1 - s / K) )
}

# max is at 4 * K / 5 
f3  <- function(x, h = 0, r = .002, K = 10){
  s <- pmax(x - h, 0)
  s + s ^ 4 * r * (1 - s / K)
}

## gather models together, indicate true model
sigma_g <- 0.05
models <- list("1" = f1, "2" = f2, "3" = f3)
model_sigmas <- c(sigma_g, 1.5 * sigma_g, sigma_g)
true_model <- "3"
```


On a discrete grid of possible states and actions, we can define the growth rate of a given state $X_t$ subject to harvest $H_t$, $f(X_t,H_t)$ as set of matrices.  Each matrix $i$ gives the transition probabilities for any current state to any future state, 
given that action $i$ is taken.  



```{r transition_matrices}
transition_matrices <- function(f, states, actions, sigma_g){
  n_s <- length(states)
  n_a <- length(actions)
  transition <- array(0, dim = c(n_s, n_s, n_a))
  for (k in 1:n_s) {
    for (i in 1:n_a) {
      nextpop <- f(states[k], actions[i])
      if(nextpop <= 0){
        transition[k, , i] <- c(1, rep(0, n_s - 1))
      } else if(sigma_g > 0){
        x <- dlnorm(states, log(nextpop), sdlog = sigma_g)
        if(sum(x) == 0){ ## nextpop is computationally zero
          transition[k, , i] <- c(1, rep(0, n_s - 1))
        } else {
          x <- x / sum(x) # normalize evenly
          transition[k, , i] <- x
        }
      }
    }
  }
  transition
}
```

This follows the standard setup for standard stochastic dynamic programming, see @Marescot2013. Having defined a function to compute the transition matrix, we can use it to create matrices corresponding to each of the three models:

```{r}
transitions <- lapply(seq_along(models), 
                      function(i) transition_matrices(models[[i]], 
                                                      states, 
                                                      actions, 
                                                      model_sigmas[[i]]))
names(transitions) <- c("1", "2", "3")

```


Likewise, a corresponding matrix defining the rewards associated with each state $X$ and each harvest action $H$ can also be defined.

````{r}
## Compute reward matrix (shared across all models)
n_s <- length(states)
n_a <- length(actions)
reward <- array(0, dim = c(n_s, n_a))
for (k in 1:n_s) {
  for (i in 1:n_a) {
    reward[k, i] <- reward_fn(states[k], actions[i])
  }
}
```


# Optimal control solutions


We use value iteration to solve the stochastic dynamic program [@Marescot2013; @MDPtoolbox] for each model. This determines the optimal harvest policy for each possible state, given each model. Because this step is the most computationally intensive routine, we cache the results using memosization conditioned on the transition matrices [@memoise].  Running this code with alternate transition matrices automatically invalidates that cache, reducing the risk of loading spurious results. 


```{r sdp, results="hide"}
mdp <- memoise::memoise(mdp_value_iteration,
                        cache = memoise::cache_filesystem("cache/"))

policies <- 
  map_dfr(transitions, 
          function(P){
    soln <- mdp(P, reward, discount = discount, 
                epsilon = 0.01, max_iter = 2000, V0 = rep(0,dim(P)[[1]]))
    escapement <- states - actions[soln$policy]
    tibble(states, policy = soln$policy, escapement)
    }, 
  .id = "model")

write_csv(policies, "../data/policies.csv")
```



# Simulations and step-ahead forecasts

We simulate fishing dynamics under the optimal policy for each model, using a simple helper function from the `mdplearning` package. Because growth dynamics are stochastic, we perform 100 simulations of each model from identical starting condition to ensure results are not the result of chance alone.


```{r simulations}
library(mdplearning)
Tmax <- 100
x0 <- which.min(abs(states - 6))
reps <- 100
set.seed(12345)

## Simulate each policy reps times, with `3` as the true model:
simulate_policy <- function(i, policy){
  mdp_planning(transitions[[true_model]], reward, discount,
           policy = policy, x0 = x0, Tmax = Tmax) %>%
    select(value, state_index = state, time, action_index = action)  %>% 
    mutate(state = states[state_index])
}

sims <- 
  map_dfr(names(transitions), 
          function(m){
            policy <- policies %>% filter(model == m) %>% pull(policy)
            map_dfr(1:reps, simulate_policy, policy = policy, .id = "reps")
          },
          .id = "model"
         )

write_csv(sims, "../data/sims.csv")
```

Using the transition matrices directly, we can examine what each model would 
have forecast the future stock size to be in the following year when no fishing
occurs (note that for each model, we use the transition matrix that corresponds
to 'no fishing', `model[[state_index, ,1]]`) (Fig 1a, main text).  

The transition matrices give the full (discretized) probability distribution,
from which we can easily calculate both the expected value and the 95% 
confidence interval.  


```{r stepahead_unfished}
stepahead_unfished <- sims
stepahead_unfished$state_index <- rep(sims$state_index[sims$model == "1"],3)

stepahead_unfished <- stepahead_unfished  %>% 
  filter(model != "3") %>% 
  mutate(next_state = dplyr::lead(state_index), model = as.integer(model)) %>%
  rowwise() %>%
  mutate(expected = transitions[[model]][state_index, , 1]  %*% states,
         var = transitions[[model]][state_index, , 1]  %*% states ^ 2 - expected ^ 2,
         low = states[max(which(cumsum(transitions[[model]][state_index,,1]) < 0.025)) ],
         high = states[min(which(cumsum(transitions[[model]][state_index,,1]) > 0.975)) ],
         true = states[next_state ]) 
```



We also look at the forecast each model makes when implementing the corresponding
optimal harvest:


```{r stepahead_fished}
stepahead_fished <- sims %>% 
  filter(model != "3") %>%
  mutate(next_state = dplyr::lead(state_index), model = as.integer(model)) %>%
  rowwise() %>%
  mutate(prob =  transitions[[model]][state_index, next_state, action_index],
         expected = transitions[[model]][state_index, , action_index]  %*% states,
         var = transitions[[model]][state_index, , action_index]  %*% states ^ 2 - expected ^ 2,
         low = states[max(which(cumsum(transitions[[model]][state_index,,action_index]) < 0.025)) ],
         high = states[min(which(cumsum(transitions[[model]][state_index,,action_index]) > 0.975)) ],
         true = states[next_state]) %>%
 select(time, model, true, expected, low, high, var, prob, reps)
```


# Proper scores

It is straight forward to apply the proper scoring formula of @Gneiting2007 
based on the first two moments of the distribution to score the respective
forecasts under both the unfished and actively managed scenario sfor each model:

```{r proper_scores, message = FALSE}
# Gneiting & Raferty (2007), eq27
scoring_fn <- function(x, mu, sigma){ -(mu - x )^2 / sigma^2  - log(sigma)}

stepahead_unfished <- stepahead_unfished %>%
  mutate(sd = sqrt(var),
         score = scoring_fn(expected, true, sd))

stepahead_fished <- stepahead_fished %>%
  mutate(sd = sqrt(var),
         score = scoring_fn(expected, true, sd))
```

```{r}
predictions <- 
  stepahead_unfished %>% 
  select(time, model, reps, expected, low, high, true, score) %>% 
  mutate(scenario = "A_unfished") %>% 
  bind_rows(stepahead_fished  %>% 
          select(time, model, reps, expected, low, high, true, score) %>% 
          mutate(scenario = "B_fished")
  ) %>% 
  mutate(model = as.character(model))


write_csv(predictions, "../data/predictions.csv")
```






# Adaptive Management

Passive adaptive management using a Bayesian learning scheme still learns the wrong model. 

```{r}
adaptive_management <- memoise::memoise(mdp_learning, cache = memoise::cache_filesystem("cache/"))
x0 <- which.min(abs(states - 6))

am1 <- adaptive_management(transitions[1:2], reward, discount, 
                           model_prior = c(0.99, 0.01), x0 = x0, 
                           Tmax = 50, true_transition = transitions[[3]], 
                            epsilon = 0.001, max_iter = 2000)
```


```{r}
am <- am1$df %>% mutate(belief = am1$posterior$V1,
                        stock = states[state],
                        quota = actions[action])
write_csv(am, "../data/am.csv")

```





### Additional calculations for plots


To plot the economic value over time, we must sum up the discounted values
at each time step, and then average over replicate simulations of each model:

```{r plot_npv}
##  Net Present Value accumulates over time
npv_df <- sims %>% 
  group_by(model, reps) %>%
  mutate(npv = cumsum(value * discount ^ time)) %>%
  group_by(time, model)  %>% 
  summarise(mean_npv = mean(npv), .groups="drop") %>% 
  arrange(model, time)

write_csv(npv_df, "../data/npv_df.csv")
```






```{r}
# tabular comparisons
npv <- npv_df %>% group_by(model) %>% summarize(npv = max(mean_npv))

am_npv <- sum(am$value * discount ^ am$time)
am_economics_percent <- round(am_npv / npv[[1,"npv"]] * 100)

mean_state <- sims %>% group_by(model) %>% summarize(state = mean(state))
am_ecology_percent <- round(mean(states[am$state])/mean_state[[1, "state"]]*100)
```

Under adaptive management, the manager realizes only `r am_economics_percent`%
of the economic value that would be achieved under Model 1 alone, and only 
`r am_ecology_percent`% of the spawning stock biomass that would have been 
achieved under Model 1 alone. 


To plot growth curves of individual models (Fig 4a), we evaluate 

$$\Delta x = x_{t+1} - x_t = f(x_t) - x_t$$

for each model for all possible states $x_t$.

```{r plot_models}
model_curves <- 
  map_dfc(models, function(f) f(states) - states) %>%
  mutate(state = states) %>%  pivot_longer(names(models), "model")

write_csv(model_curves, "../data/model_curves.csv")

```




# References
