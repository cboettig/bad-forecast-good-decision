---
title: 'Appendix for: the Forecasting Trap'
author: Carl Boettiger
journal: TBD
layout: 3p
bibliography: refs.bib
csl: csl/ecology-letters.csl
#output:
#  tufte::tufte_handout:
#    dev: cairo_pdf
#    latex_engine: xelatex
output: 
  hrbrthemes::ipsum_pdf:
    dev: cairo_pdf
    latex_engine: xelatex

header-includes:
  - \usepackage[left]{lineno}
  - \linenumbers

---


Because the methods applied here: Stochastic Dynamic Programming, Iterative Forecasts assessed with Proper Scoring Rules, or Adaptive Management, are not themselves novel, the reader is referred to previous reviews and text books which can provide a thorough introduction to methods, in particular, @Marescot2013 and @Smith1981.  However, implementation of these approaches generally depends on numerical methods, for which even precise mathematical formulas or pseudo-code descriptions can be inadequate to specify unambiguously or to properly facilitate replication [@Barnes2010].  As such, this appendix includes computer code in the R language, which can also be found in the corresponding R-Markdown document in the paper's GitHub repository, <https://github.com/cboettig/bad-forecast-good-decision>.  While computer code can be more verbose and difficult to read than mathematical formulas presented here, it is also less ambiguous. Though R is already widely used among ecologists, the implementations concern mostly matrix algebra that can be translated to other languages, and the `MDPtoolbox` package used here is also implemented in MatLab [@Marescot2013; @MDPtoolbox].  


Below, I provide carefully annotated code necessary to completely reproduce all of the analysis presented in the main paper. This analysis is run in R [@R] uses `MDPtoolbox` [@MDPtoolbox] for solving Markov Decision Processes (MDP) using stochastic dynamic programming functionality, `expm` for matrix exponentials [@expm], and a few custom MDP functions provided by our package, `mdplearning` [@mdplearning].  We will also use `tidyverse`  packages for basic manipulation and plotting [@tidyverse]. This file is also available as an RMarkdown document [@rmarkdown] at  <https://github.com/cboettig/bad-forecast-good-decision>.  The code below generates the data tables required for each plot shown in the main paper.  Data are stored as `.csv` tables to be accessible to any relevant software program. Plotting commands used in the main paper are embedded in the paper's RMarkdown file at <https://github.com/cboettig/bad-forecast-good-decision>.

```{r setup, message=FALSE}
library(tidyverse)
library(MDPtoolbox)
library(expm)
# remotes::install_github("boettiger-lab/mdplearning")
library(mdplearning)
```



# Optimal quotas as a Markov Decision Process (MDP) problem


## Solving MDP via SDP

```{r model_definitions}
states <- seq(0,30, length=100)
actions <- seq(0,30, length.out = 100)
obs <- states
```


```{r}

benefit <- 1
cost <- 1 # quadratic costs
reward_fn <- function(x,h) benefit * x - (h ^ cost)
discount <- 0.999
```




```{r graphics_setup, message=FALSE, warning=FALSE, include = FALSE}
## Plotting themes, colors, and fonts 
## aesthetics choices only, all can be omitted without consequence
library(ggthemes)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(patchwork)
library(styler)
extrafont::loadfonts(quiet = TRUE)
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

scale_colour_discrete <- function(...) ggthemes::scale_colour_solarized()
scale_fill_discrete <- function(...) ggthemes::scale_fill_solarized()
pal <- ggthemes::solarized_pal()(8)
txtcolor <- "#586e75"

knitr::opts_chunk$set(cache=FALSE, tidy = "styler", message = FALSE, warning = FALSE, echo = TRUE)

```


```{r}

p <- list(r = 2, K = 25, q = 4, b = 2, a = 7) # higher-state peak is optimal


# Matches the optimal policy under these settings (depends on sigma, among others)
f1 <- 
  function(x, h = 0){
    y = x+h
    pmax(
      y + y * (p$r - .65) * (1 - y / p$K),  
      0)
  }



f2  <- function(x, h = 0, r = .095, K = 20){
  s <- pmax(x - h, 0)
  s + s ^ 2 * r * (1 - s / K)
}




f3 <- 
  function(x, h = 0){ # May
    y <- x +  h
    pmax(
      ## controlling h is controlling the bifurcation point directly...
      y + y * p$r * (1 - y / p$K)  - p$a * y ^ p$q / (y ^ p$q + p$b ^ p$q),  
      0)
  }




## gather models together, indicate true model
sigma_g <- 0.05
models <- list("1" = f1, "2" = f2, "3" = f3)
model_sigmas <- c(sigma_g, sigma_g, sigma_g)
true_model <- "3"
```


On a discrete grid of possible states and actions, we can define the growth rate of a given state $X_t$ subject to harvest $H_t$, $f(X_t,H_t)$ as set of matrices.
Each matrix $i$ gives the transition probabilities for any current state to any future state, given that action $i$ is taken.  



```{r transition_matrices}
transition_matrices <- function(f, states, actions, sigma_g){
  n_s <- length(states)
  n_a <- length(actions)
  transition <- array(0, dim = c(n_s, n_s, n_a))
  for (k in 1:n_s) {
    for (i in 1:n_a) {
      nextpop <- f(states[k], actions[i])
      if(nextpop <= 0){
        transition[k, , i] <- c(1, rep(0, n_s - 1))
      } else if(sigma_g > 0){
        x <- dlnorm(states, log(nextpop), sdlog = sigma_g)
        if(sum(x) == 0){ ## nextpop is computationally zero
          transition[k, , i] <- c(1, rep(0, n_s - 1))
        } else {
          x <- x / sum(x) # normalize evenly
          transition[k, , i] <- x
        }
      }
    }
  }
  transition
}
```

This follows the standard setup for standard stochastic dynamic programming, see @Marescot2013. Having defined a function to compute the transition matrix, we can use it to create matrices corresponding to each of the three models:

```{r}
transitions <- lapply(seq_along(models), function(i) 
  transition_matrices(models[[i]], states, actions, model_sigmas[[i]]))
names(transitions) <- c("1", "2", "3")

```


Likewise, a corresponding matrix defining the rewards associated with each state $X$ and each harvest action $H$ can also be defined.

````{r}
## Compute reward matrix (shared across all models)
n_s <- length(states)
n_a <- length(actions)
reward <- array(0, dim = c(n_s, n_a))
for (k in 1:n_s) {
  for (i in 1:n_a) {
    reward[k, i] <- reward_fn(states[k], actions[i])
  }
}
```


## Optimal control solutions


We use value iteration to solve the stochastic dynamic program [@Marescot2013; @MDPtoolbox] for each model. This determines the optimal harvest policy for each possible state, given each model. Because this step is the most computationally intensive routine, we cache the results using memosization conditioned on the transition matrices [@memoise].  Running this code with alternate transition matrices automatically invalidates that cache, reducing the risk of loading spurious results. 


```{r sdp, results="hide"}
mdp <- memoise::memoise(mdp_policy_iteration,
                        cache = memoise::cache_filesystem("cache/"))

policies <-
  map_dfr(transitions, 
          function(P){
    soln <- mdp(P, reward, discount = discount, max_iter = 5000, policy0 = rep(1, length(states)))
    tibble(states, policy = soln$policy, actions = actions[soln$policy])
    }, 
  .id = "model") 
```



# Simulations and step-ahead forecasts (Fig 1, 2A)

We simulate fishing dynamics under the optimal policy for each model, using a simple helper function from the `mdplearning` package. Because growth dynamics are stochastic, we perform 100 simulations of each model from identical starting condition to ensure results are not the result of chance alone.


```{r simulations}
library(mdplearning)
Tmax <- 100
x0 <- which.min(abs(states - 1.5))
reps <- 100
set.seed(12345)

## Simulate each policy reps times, with `3` as the true model:
simulate_policy <- function(i, policy){
  mdp_planning(transitions[[true_model]], reward, discount,
           policy = policy, x0 = x0, Tmax = Tmax) %>%
    select(value, state_index = state, time, action_index = action)  %>% 
    mutate(state = states[state_index])
}

sims <- 
  map_dfr(names(transitions), 
          function(m){
            policy <- policies %>% filter(model == m) %>% pull(policy)
            map_dfr(1:reps, simulate_policy, policy = policy, .id = "reps")
          },
          .id = "model"
         )
```


```{r}
## add predictions from each model.  
do_nothing <- rep(1, length(states))
unmanaged <- mdp_planning(transitions[[true_model]], reward, discount,
           policy = do_nothing, x0 = x0, Tmax = Tmax) %>%
    select(value, state_index = state, time, action_index = action)  %>% 
    mutate(state = states[state_index])


```

Using the transition matrices directly, we can examine what each model would 
have forecast the future stock size to be in the following year when no fishing
occurs (note that for each model, we use the transition matrix that corresponds
to 'no fishing', `model[[state_index, ,1]]`) (Fig 1a, main text).  

The transition matrices give the full (discretized) probability distribution,
from which we can easily calculate both the expected value and the 95% 
confidence interval.  

We also look at the forecast each model makes when implementing the corresponding
optimal harvest:


```{r stepahead_fished}
stepahead_fished <- sims %>% 
  filter(model != "3") %>%
  mutate(next_state = dplyr::lead(state_index), model = as.integer(model)) %>%
  rowwise() %>%
  mutate(prob =  transitions[[model]][state_index, next_state, action_index],
         expected = transitions[[model]][state_index, , action_index]  %*% states,
         var = transitions[[model]][state_index, , action_index]  %*% states ^ 2 - expected ^ 2,
         low = states[max(which(cumsum(transitions[[model]][state_index,,action_index]) < 0.025)) ],
         high = states[min(which(cumsum(transitions[[model]][state_index,,action_index]) > 0.975)) ],
         true = states[next_state]) %>%
 select(time, model, true, expected, low, high, var, prob, reps)
```


# Proper scores (Fig 1)

It is straight forward to apply the proper scoring formula of @Gneiting2007 
based on the first two moments of the distribution to score the respective
forecasts under both the un-fished and actively managed scenario for each model:

```{r proper_scores, message = FALSE}
# Gneiting & Raferty (2007), eq27
scoring_fn <- function(x, mu, sigma){ -(mu - x )^2 / sigma^2  - log(sigma)}

stepahead_fished <- stepahead_fished %>%
  mutate(sd = sqrt(var),
         score = scoring_fn(expected, true, sd))
```


We store the replicate predictions under each model at each timestep, along with the scores, in a `predictions.csv` table for plotting.

```{r}
predictions <- 
  stepahead_fished  %>% 
  select(time, model, reps, expected, low, high, true, score)  %>% 
  mutate(model = as.character(model))

```

```{r}

fig1cd <- predictions %>%
  ggplot(aes(x = score, group = model, fill = model)) +
  geom_histogram(binwidth = 2, show.legend = FALSE) +
  coord_cartesian(xlim = c(-100, 1), ylim = c(0,4000)) +
  xlab("Proper score") +
  facet_wrap(~scenario, ncol = 1, labeller =as_labeller(c(
    A_unfished = "C.",
    B_fished = "D."))) + 
  scale_x_continuous(breaks = c(-100, 0)) +
  theme(axis.text.y = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )



fig1ab <- predictions %>% 
  filter(reps == "2", time < 10) %>%
  ggplot(aes(time, col = model, fill = model)) + 
  geom_point(aes(y = expected)) + 
  geom_errorbar(aes(ymin = low, ymax = high)) +
  geom_point(aes(y = true), pch = "*", size = 12, 
             alpha = 0.6) +
  ylab("stock size")

fig1ab + fig1cd + 
  plot_layout(widths = c(3,1), guides = 'collect') + 
  theme(plot.margin = margin(0, 0, .1, .1, "cm"))
```


# Economic value tabulation (Fig 2B)


To plot the economic value over time, we must sum up the discounted values
at each time step, and then average over replicate simulations of each model:

```{r plot_npv}
##  Net Present Value accumulates over time
npv_df <- sims %>% 
  group_by(model, reps) %>%
  mutate(npv = cumsum(value * discount ^ time)) %>%
  group_by(time, model)  %>% 
  summarise(mean_npv = mean(npv), .groups="drop") %>% 
  arrange(model, time)
```

```{r figure2, message = FALSE, fig.cap = "Ecological and economic performance of each forecast."}
plot_ecology <- 
  sims %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  group_by(model, time) %>%
  summarise(mean_state = mean(state), sd = sd(state), .groups = "drop") %>%
  filter(time < 25, model != "3") %>%
  ggplot(aes(time, mean_state)) + 
  geom_line(aes(col = model), lwd=1.5, show.legend = FALSE) +
  geom_ribbon(aes(ymin = mean_state - 2*sd, 
                    ymax = mean_state + 2*sd,
                    fill = model),
                alpha = 0.2, show.legend = FALSE) +
  ylab("state") + 
  labs(subtitle = "A. Ecological outcomes")

optimal <- select(filter(npv_df, model == "3"), time, mean_npv)

plot_econ <- 
npv_df %>%
  mutate(model = as.character(model), resp = as.character(reps)) %>%
  filter(model != "3", time %in% seq(1,100, by = 5)) %>%
  ggplot(aes(time, mean_npv)) +
  geom_line(data = optimal, lwd = 1.5, col = "grey20") +
  geom_point(aes(col=model), size = 4, alpha = 0.8) + 
  ylab("Net present value") + xlab("time") + 
  labs(subtitle = "B. Economic outcomes")


plot_ecology + plot_econ
```


# Adaptive Management

Passive adaptive management using a Bayesian learning scheme still learns the wrong model.
Adaptive management for MDP problems is particularly computationally intensive, and not directly supported by the `MDPtoolbox` implementation.
Nevertheless, the principle involves only a relatively straightforward, if not particularly efficient extension to the task of solving and MDP problem for a single model shown above.
Given a set of transition probability matrices $P_i$ and a belief probability $b_i$ that the $i$th model is correct, it simple to define the transition matrices over possible models by the laws of conditional probability, $P = \sum_i P_i b_i$, given $\sum_i b_i = 1$.
After each subsequent action and then new observation x_j, the $b_i$ probabilities for each model are updated according to Bayes law: if $b_i = P(M_i)$ is the prior probability that the model $M_i$ is correct, then the posterior probability that model $M_i$ is correct given observation $x_j$ is:

$$b_i' = P(M_i | x_j) = P(M_i) P(x_j | M_i) = b_i P(x_j | M_i) $$

Note that because the transition matrices are now altered after every action and observation, it is necessary to repeat the complete SDP calculation to determine the optimal action given the updated uncertainty.
While straight forward, this is what makes adaptive management for MDP problems particularly computationally intensive and thus poorly suited for handling a large space of candidate models.
Nevertheless, our example of two models is most feasible, and even the larger example of 36 models is not unreasonable.
Here I make use of the `mdplearning` R package, which provides a straight-forward implementation of this algorithm.  




```{r}
adaptive_management <- memoise::memoise(mdp_learning, cache = memoise::cache_filesystem("cache/"))
am1 <- adaptive_management(transitions[1:2], reward, discount, 
                           model_prior = c(0.99, 0.01), x0 = x0, 
                           Tmax = 50, true_transition = transitions[[3]], 
                            epsilon = 0.001, max_iter = 2000)
```



I also compare the adaptive management solution to an approach which still integrates uncertainty over both models, but treats the resulting policy as fixed for the duration of management, rather than adaptive _learning_ by updating probabilities.
This approach is sometimes referred to as uncertainty "planning," as it accounts for the same initial uncertainty over models but does not attempt to learn.
For consistency, I use the same model prior as in the learning case, which heavily favors model 1 to begin with.
Not surprisingly, the resulting policy is nearly identical to that under model 1 alone.


```{r}
# Compute policy using SDP over both models, given fixed prior beliefs
policy_planning <- memoise::memoise(mdp_compute_policy, cache = memoise::cache_filesystem("cache/"))
policy <- policy_planning(transitions[1:2],  
                          reward = reward, 
                          discount = discount,
                          model_prior = c(0.99, 0.01),
                          Tmax = 50, 
                          epsilon = 0.001, 
                          max_iter = 2000)
```



```{r}
# Simulate under true model using fixed planning policy
non_am <- mdp_planning(transitions[[3]],  
                       reward = reward, 
                       discount = discount,
                       model_prior = c(0.99, 0.01),
                       x0 = x0, 
                       a0 = 1,
                       policy = policy$policy,
                       Tmax = 50)  %>% 
  mutate(belief = 0.99, # belief in model 1 is fixed
         method = "planning")
```



We combine results of the two methods and store the resulting `data.frame` of stock size, quota, and belief in Model 1 over time as `am.csv` for later plotting. 

```{r}
am <- am1$df %>%
  mutate(belief = am1$posterior$V1,
         method = "learning") %>%
  bind_rows(non_am) %>%
  mutate(stock = states[state],
         quota = actions[action]) %>%
  select(time, stock, quota, belief, method) 
```


We can also compare these results to the net economic value achieved under Model 1 alone:

```{r}
# tabular comparisons
npv <- npv_df %>% group_by(model) %>% summarize(npv = max(mean_npv))

am_npv <- sum(am$value * discount ^ am$time)
am_economics_percent <- round(am_npv / npv[[1,"npv"]] * 100)

mean_state <- sims %>% group_by(model) %>% summarize(state = mean(state))
am_ecology_percent <- round(mean(states[am$state])/mean_state[[1, "state"]]*100)
```

Under adaptive management, the manager realizes only `r am_economics_percent`%
of the economic value that would be achieved under Model 1 alone, and only 
`r am_ecology_percent`% of the spawning stock biomass that would have been 
achieved under Model 1 alone. 



# Growth models 

To plot growth curves of individual models (Fig 4a), we evaluate 

$$\Delta x = x_{t+1} - x_t = f(x_t) - x_t$$

for each model for all possible states $x_t$.

```{r plot_models}
model_curves <- 
  map_dfc(models, function(f) f(states) - states) %>%
  mutate(state = states) %>%  pivot_longer(names(models), "model")

```




```{r}
plot_models <- 
  model_curves %>% 
  ggplot(aes(state, value, col=model, lty=model)) +
  geom_hline(aes(yintercept = 0)) + 
  geom_line(show.legend = FALSE) + 
  coord_cartesian(ylim = c(-5, 8)) +
  ylab(bquote(f(x) - x)) + xlab("x")

plot_policies <- policies %>%
  ggplot(aes(states, actions, col=model, lty=model)) + 
  geom_line() + xlab("state")


plot_models + labs(subtitle="A") + plot_policies + labs(subtitle="B")
```


```{r}
```


\pagebreak


# References
