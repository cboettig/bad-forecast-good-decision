---
name: Carl Boettiger
title: Assistant Professor
email: cboettig@berkeley.edu
homepage: https://carlboettiger.info
address: "Science Magazine"
opening: "Dear Editors:"
closing: "Sincerely,"
campus: ucb
dept: espm
fontsize: 10pt

## Leave this as is, make sure template.tex is in working directory
#output:
#  pdf_document:
#    template: template.tex

## NB: Add an image of your signature called 'signature.png'
---


Dear Editors,

Recent events, from the current pandemic to election predictions, have underpinned the central role of model-based forecasts in decision-making. Both scientific journals and general news sources cite accuracy or inaccuracy of forecasts as a crucial judgment on which models should be preferred in making decisions.  The rapid expansion in available data and computational power has now paved the way for a new generation of forecasts based on more black-box approaches such as machine learning which could dramatically improve the accuracy and precision of such forecasts.  While such advances could open exciting new possibilities, they may also lead to worse decisions if we allow a previous generation of more mechanistic models to be replaced on the grounds of forecast skill alone.  

In the enclosed manuscript, I illustrate both how this conundrum arises in which the more accurate model leads a standard decision algorithm to worse outcomes, and also how it can be understood and avoided. A decision-maker's notion of utility will almost always be distinct from the statistician's notion of forecasting skill. In managing a dynamical system such as the spread of a virus or the recovery of a fishery, the optimal policy often hinges on certain details while being relatively insensitive to other details.  Using the classic ecological management problem of setting a fishing quota, I illustrate how easily one can choose the wrong model for decision-making by focusing on forecasting skill, and how a model with much less accurate predictions can lead to much better outcomes by capturing only certain essential features.  Today, decision-makers are tearing up old models that did not give accurate forecasts of elections, of COVID infections, of fire danger.  Many of these will be replaced by more complex models and machine learning algorithms that can give better forecasts. Sometimes this will lead to better outcomes.  But we also risk being stuck with flawed models, like model 2 in my example, which consistently gives good predictions while also dangerously over-harvesting, resulting in a poor economic yield and degraded ecosystem.  I believe this example can be a guide both to the risks of an over-reliance on formal metrics of forecasting skill, while also offering insight into a better way to assess predictive models used for decision-making rooted in principles of decision theory and adaptive management.  


I would recommend any of the following as potential reviewers:

- Simon Levin <slevin@princeton.edu>
- Michael Dietze <dietze@bu.edu>
- Anthony Ives <arives@wisc.edu>
- Michael Neubert <mneubert@whoi.edu>
- Carl Walters <c.walters@oceans.ubc.ca>


Sincerely,

Carl Boettiger
Assistant Professor
UC Berkeley

