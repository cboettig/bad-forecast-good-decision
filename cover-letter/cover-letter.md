---
name: Carl Boettiger
title: Assistant Professor
email: cboettig@berkeley.edu
homepage: https://carlboettiger.info
address: "Science Magazine"
opening: "Dear Editors:"
closing: "Sincerely,"
campus: ucb
dept: espm
fontsize: 10pt

## Leave this as is, make sure template.tex is in working directory
#output:
#  pdf_document:
#    template: template.tex

## NB: Add an image of your signature called 'signature.png'
---


Dear Editors,

Following my pre-submission discussion with Deputy Editor Sacha Vignieri, I am pleased to submit the enclosed manuscript, "Bad forecast, good decision: predictive accuracy is not everything" for your consideration for publication.  

Recent events, from the current pandemic to election predictions, have underpinned the central role of model-based forecasts in decision-making. Both scientific journals and general news sources cite accuracy or inaccuracy of forecasts as a crucial judgment on which models should be preferred in making decisions.  An article in _Science_ by Jim Clark and colleagues really launched an ever-growing emphasis on forecasting in ecology as a way of evaluating decision-relevant models two decades ago, and recent trends as well as advances in data collection have made this a growing focus in our field.  Despite the importance of this work, I am concerned that the seemingly obvious assertion that good decisions require good forecasts is now being adopted uncritically in both popular and scientific audiences.  None of my colleagues would quibble if I argued "we should pick this model for decision-making because it gives the most accurate forecasts."  Therefore, they have all been surprised to see how simple it is to construct a counter-example: where the model with most accurate predictions still leads to decisively worse decisions.

The lure of ever-more-powerful methods of prediction, including approaches rooted in machine learning, has given measures of statistical accuracy of a forecast primacy over common-sense objectives.  We risk finding ourselves in a world where we can suddenly make accurate forecasts about previously difficult-to-predict areas like ecological dynamics, while at the same time making worse decisions about them.  The example I present uses the classic problem of setting a sustainable fishing quota, and shows how and why the model that gives the best forecasts of future stock sizes nevertheless gives much worse ecological and economic outcomes.  Though it sounds counter-intuitive, even to me, it is really a very simple example; an argument that could be made on a napkin or two. While I write, decision-makers are tearing up old models that did not give accurate forecasts of elections, of COVID infections, of fire danger.  Many of these will be replaced by more complex models and machine learning algorithms that can give better forecasts, and often this will lead to better outcomes.  But we also risk being stuck with flawed models, like model 2 in my example, which consistently gives good predictions while also dangerously over-harvesting, resulting in a poor economic yield and degraded ecosystem.  

I realize such theoretical work is not fashionable, it can easily be critiqued that anything without empirical evidence would only be of interest to specialists. But as the pages of _Science_ have often demonstrated, theory has an ability to generalize and guide intuition as well. I hope that a single, concise example can more easily be a memorable and comprehensible example that your readers might remember the next time a model is critiqued on the accuracy of its forecasts alone.  The field of decision theory should not be forgotten in the application of statistics.  Models used in decisions can and must be assessed in terms of the decision outcomes to which they lead. 

I would recommend any of the following as potential reviewers:

- Simon Levin <slevin@princeton.edu>
- Michael Dietze <dietze@bu.edu>
- Anthony Ives <arives@wisc.edu>
- Michael Neubert <mneubert@whoi.edu>
- Carl Walters <c.walters@oceans.ubc.ca>


Sincerely,

Carl Boettiger
Assistant Professor
UC Berkeley

